---
layout: post
title: "FastAPI + LangChainìœ¼ë¡œ êµ¬ì¶•í•˜ëŠ” LLM MSA ì„œë¹„ìŠ¤"
date: 2025-09-06 10:00:00 +0900
categories: [FastAPI, LangChain, LLM, Microservices, AI]
tags: [FastAPI, LangChain, OpenAI, GPT, Microservices, Python, AI, Machine Learning, Vector Database, RAG]
---

FastAPIì™€ LangChainì„ ê²°í•©í•˜ì—¬ í™•ì¥ ê°€ëŠ¥í•œ LLM(Large Language Model) ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ë¥¼ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ í™œìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ êµ¬í˜„ íŒ¨í„´ê³¼ ìµœì í™” ê¸°ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ê°œìš”

### MSA êµ¬ì¡° ì„¤ê³„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Gateway   â”‚    â”‚   Auth Service  â”‚    â”‚ Vector Database â”‚
â”‚    (FastAPI)    â”‚â”€â”€â”€â”€â”‚    (FastAPI)    â”‚    â”‚   (Chroma/Pinecone)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
         â–¼                        â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chat Service   â”‚    â”‚ Embedding       â”‚    â”‚ Document        â”‚
â”‚  (LangChain)    â”‚â”€â”€â”€â”€â”‚ Service         â”‚â”€â”€â”€â”€â”‚ Processing      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
         â–¼                        â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM Provider  â”‚    â”‚ Monitoring      â”‚    â”‚  Message Queue  â”‚
â”‚ (OpenAI/Hugging)â”‚    â”‚  & Logging      â”‚    â”‚    (Redis)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ 1. ê¸°ë³¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
llm-msa-service/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ api-gateway/
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”‚   â””â”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ chat-service/
â”‚   â”œâ”€â”€ embedding-service/
â”‚   â”œâ”€â”€ auth-service/
â”‚   â””â”€â”€ document-service/
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ utils/
â”‚   â””â”€â”€ config/
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ k8s/
```

### ê³µí†µ ì˜ì¡´ì„± ì„¤ì •

```python
# shared/requirements.txt
fastapi==0.104.1
langchain==0.0.335
langchain-openai==0.0.2
langchain-community==0.0.8
uvicorn[standard]==0.24.0
pydantic==2.5.0
redis==5.0.1
chromadb==0.4.18
python-multipart==0.0.6
python-jose[cryptography]==3.3.0
prometheus-fastapi-instrumentator==6.1.0
structlog==23.2.0
httpx==0.25.2
```

## ğŸ”§ 2. API Gateway êµ¬í˜„

### ë©”ì¸ ê²Œì´íŠ¸ì›¨ì´ ì„œë¹„ìŠ¤

```python
# services/api-gateway/app/main.py
from fastapi import FastAPI, Depends, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from prometheus_fastapi_instrumentator import Instrumentator
import structlog
import httpx
import time
from typing import Dict, Any

from .routers import chat, documents, health
from .middleware.auth import AuthMiddleware
from .middleware.rate_limit import RateLimitMiddleware
from .config.settings import get_settings

# ë¡œê±° ì„¤ì •
logger = structlog.get_logger()

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="LLM MSA API Gateway",
    description="Microservices Architecture for LLM Applications",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(TrustedHostMiddleware, allowed_hosts=["*"])
app.add_middleware(AuthMiddleware)
app.add_middleware(RateLimitMiddleware)

# ë©”íŠ¸ë¦­ ìˆ˜ì§‘
instrumentator = Instrumentator()
instrumentator.instrument(app).expose(app)

# ì„œë¹„ìŠ¤ ë””ìŠ¤ì»¤ë²„ë¦¬ ì„¤ì •
SERVICES = {
    "chat": "http://chat-service:8001",
    "embedding": "http://embedding-service:8002",
    "document": "http://document-service:8003",
    "auth": "http://auth-service:8004"
}

class ServiceProxy:
    def __init__(self):
        self.client = httpx.AsyncClient(timeout=30.0)
    
    async def forward_request(
        self, 
        service: str, 
        path: str, 
        method: str = "GET",
        data: Dict[Any, Any] = None,
        headers: Dict[str, str] = None
    ):
        """ì„œë¹„ìŠ¤ë¡œ ìš”ì²­ ì „ë‹¬"""
        if service not in SERVICES:
            raise HTTPException(status_code=404, detail=f"Service {service} not found")
        
        url = f"{SERVICES[service]}{path}"
        
        try:
            start_time = time.time()
            
            response = await self.client.request(
                method=method,
                url=url,
                json=data,
                headers=headers
            )
            
            latency = time.time() - start_time
            logger.info(
                "service_request",
                service=service,
                path=path,
                status_code=response.status_code,
                latency=latency
            )
            
            return response.json() if response.headers.get("content-type") == "application/json" else response.text
            
        except httpx.RequestError as e:
            logger.error("service_request_failed", service=service, error=str(e))
            raise HTTPException(status_code=503, detail=f"Service {service} unavailable")

proxy = ServiceProxy()

# ë¼ìš°í„° ë“±ë¡
app.include_router(chat.router, prefix="/api/v1/chat", tags=["chat"])
app.include_router(documents.router, prefix="/api/v1/documents", tags=["documents"])
app.include_router(health.router, prefix="/health", tags=["health"])

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "service": "LLM MSA API Gateway",
        "version": "1.0.0",
        "status": "healthy"
    }

@app.middleware("http")
async def log_requests(request: Request, call_next):
    """ìš”ì²­ ë¡œê¹… ë¯¸ë“¤ì›¨ì–´"""
    start_time = time.time()
    
    response = await call_next(request)
    
    process_time = time.time() - start_time
    logger.info(
        "request_processed",
        method=request.method,
        url=str(request.url),
        status_code=response.status_code,
        process_time=process_time
    )
    
    return response
```

### ì±„íŒ… ë¼ìš°í„° êµ¬í˜„

```python
# services/api-gateway/app/routers/chat.py
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import uuid
from datetime import datetime

from ..main import proxy
from ..middleware.auth import get_current_user

router = APIRouter()

class ChatMessage(BaseModel):
    role: str = Field(..., description="ë©”ì‹œì§€ ì—­í•  (user, assistant, system)")
    content: str = Field(..., description="ë©”ì‹œì§€ ë‚´ìš©")
    timestamp: Optional[datetime] = None

class ChatRequest(BaseModel):
    message: str = Field(..., description="ì‚¬ìš©ì ë©”ì‹œì§€")
    conversation_id: Optional[str] = None
    model: str = Field(default="gpt-3.5-turbo", description="ì‚¬ìš©í•  LLM ëª¨ë¸")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0)
    max_tokens: int = Field(default=1000, ge=1, le=4000)
    use_rag: bool = Field(default=False, description="RAG ê¸°ëŠ¥ ì‚¬ìš© ì—¬ë¶€")
    context_docs: Optional[List[str]] = Field(default=None, description="ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ ID ëª©ë¡")

class ChatResponse(BaseModel):
    conversation_id: str
    message: ChatMessage
    response: ChatMessage
    model_used: str
    tokens_used: int
    response_time: float
    sources: Optional[List[Dict[str, Any]]] = None

@router.post("/", response_model=ChatResponse)
async def chat(
    request: ChatRequest,
    background_tasks: BackgroundTasks,
    user = Depends(get_current_user)
):
    """
    ì±„íŒ… ë©”ì‹œì§€ ì²˜ë¦¬
    
    - ì¼ë°˜ ì±„íŒ… ë˜ëŠ” RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì§€ì›
    - ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€
    - í† í° ì‚¬ìš©ëŸ‰ ë° ì‘ë‹µ ì‹œê°„ ì¶”ì 
    """
    try:
        # ëŒ€í™” ID ìƒì„± ë˜ëŠ” ê¸°ì¡´ ID ì‚¬ìš©
        conversation_id = request.conversation_id or str(uuid.uuid4())
        
        # ì±„íŒ… ì„œë¹„ìŠ¤ë¡œ ìš”ì²­ ì „ë‹¬
        chat_data = {
            "user_id": user["user_id"],
            "conversation_id": conversation_id,
            "message": request.message,
            "model": request.model,
            "temperature": request.temperature,
            "max_tokens": request.max_tokens,
            "use_rag": request.use_rag,
            "context_docs": request.context_docs
        }
        
        response = await proxy.forward_request(
            service="chat",
            path="/generate",
            method="POST",
            data=chat_data
        )
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‚¬ìš©ëŸ‰ ë¡œê¹…
        background_tasks.add_task(
            log_usage, 
            user["user_id"], 
            response.get("tokens_used", 0),
            response.get("model_used", request.model)
        )
        
        return ChatResponse(**response)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chat processing failed: {str(e)}")

@router.get("/conversations/{conversation_id}/history")
async def get_conversation_history(
    conversation_id: str,
    limit: int = 50,
    user = Depends(get_current_user)
):
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    try:
        response = await proxy.forward_request(
            service="chat",
            path=f"/conversations/{conversation_id}/history?limit={limit}",
            method="GET"
        )
        
        return response
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get conversation history: {str(e)}")

@router.delete("/conversations/{conversation_id}")
async def delete_conversation(
    conversation_id: str,
    user = Depends(get_current_user)
):
    """ëŒ€í™” ì‚­ì œ"""
    try:
        await proxy.forward_request(
            service="chat",
            path=f"/conversations/{conversation_id}",
            method="DELETE"
        )
        
        return {"message": "Conversation deleted successfully"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete conversation: {str(e)}")

async def log_usage(user_id: str, tokens_used: int, model: str):
    """ì‚¬ìš©ëŸ‰ ë¡œê¹… (ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬)"""
    try:
        await proxy.forward_request(
            service="auth",
            path="/usage/log",
            method="POST",
            data={
                "user_id": user_id,
                "tokens_used": tokens_used,
                "model": model,
                "timestamp": datetime.utcnow().isoformat()
            }
        )
    except Exception as e:
        # ë¡œê¹… ì‹¤íŒ¨ëŠ” ë©”ì¸ ê¸°ëŠ¥ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ
        print(f"Usage logging failed: {e}")
```

## ğŸ¤– 3. ì±„íŒ… ì„œë¹„ìŠ¤ êµ¬í˜„

### LangChain ê¸°ë°˜ ì±„íŒ… ì—”ì§„

```python
# services/chat-service/app/main.py
from fastapi import FastAPI, HTTPException
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferWindowMemory
from langchain.schema import HumanMessage, AIMessage, SystemMessage
from langchain.callbacks.base import BaseCallbackHandler
from langchain.chains import ConversationalRetrievalChain
from langchain.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
import redis
import json
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel

app = FastAPI(title="Chat Service", version="1.0.0")

# Redis ì—°ê²° (ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ìš©)
redis_client = redis.Redis(host="redis", port=6379, db=0, decode_responses=True)

# Vector Database ì—°ê²° (RAGìš©)
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(
    persist_directory="/app/data/chroma",
    embedding_function=embeddings
)

class TokenCounterCallback(BaseCallbackHandler):
    """í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  ì½œë°±"""
    def __init__(self):
        self.total_tokens = 0
        self.prompt_tokens = 0
        self.completion_tokens = 0
    
    def on_llm_end(self, response, **kwargs):
        if hasattr(response, 'llm_output') and response.llm_output:
            token_usage = response.llm_output.get('token_usage', {})
            self.total_tokens = token_usage.get('total_tokens', 0)
            self.prompt_tokens = token_usage.get('prompt_tokens', 0)
            self.completion_tokens = token_usage.get('completion_tokens', 0)

class ChatEngine:
    def __init__(self):
        self.models = {
            "gpt-3.5-turbo": ChatOpenAI(model="gpt-3.5-turbo"),
            "gpt-4": ChatOpenAI(model="gpt-4"),
            "gpt-4-turbo-preview": ChatOpenAI(model="gpt-4-turbo-preview")
        }
        self.retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    
    async def generate_response(
        self,
        user_id: str,
        conversation_id: str,
        message: str,
        model: str = "gpt-3.5-turbo",
        temperature: float = 0.7,
        max_tokens: int = 1000,
        use_rag: bool = False,
        context_docs: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """ë©”ì‹œì§€ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        
        start_time = time.time()
        token_callback = TokenCounterCallback()
        
        try:
            # LLM ëª¨ë¸ ì„¤ì •
            llm = self.models.get(model, self.models["gpt-3.5-turbo"])
            llm.temperature = temperature
            llm.max_tokens = max_tokens
            llm.callbacks = [token_callback]
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ
            memory = self._load_conversation_memory(conversation_id)
            
            if use_rag:
                # RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„±
                response_text = await self._generate_rag_response(
                    message, llm, memory, context_docs
                )
            else:
                # ì¼ë°˜ ëŒ€í™” ì‘ë‹µ ìƒì„±
                response_text = await self._generate_normal_response(
                    message, llm, memory
                )
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥
            await self._save_conversation(
                conversation_id, user_id, message, response_text
            )
            
            response_time = time.time() - start_time
            
            return {
                "conversation_id": conversation_id,
                "message": {
                    "role": "user",
                    "content": message,
                    "timestamp": time.time()
                },
                "response": {
                    "role": "assistant",
                    "content": response_text,
                    "timestamp": time.time()
                },
                "model_used": model,
                "tokens_used": token_callback.total_tokens,
                "response_time": response_time
            }
            
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Response generation failed: {str(e)}")
    
    async def _generate_rag_response(
        self,
        message: str,
        llm: ChatOpenAI,
        memory: ConversationBufferWindowMemory,
        context_docs: Optional[List[str]]
    ) -> str:
        """RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
        relevant_docs = self.retriever.get_relevant_documents(message)
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([doc.page_content for doc in relevant_docs])
        
        # RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        rag_prompt = f"""
ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {message}

ë‹µë³€ì€ ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•˜ë˜, ëª…í™•í•˜ê³  ë„ì›€ì´ ë˜ë„ë¡ ì‘ì„±í•˜ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, ê·¸ë ‡ë‹¤ê³  ë§ì”€í•˜ì„¸ìš”.
"""
        
        # ConversationalRetrievalChain ì‚¬ìš©
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=self.retriever,
            memory=memory,
            return_source_documents=True
        )
        
        result = qa_chain({"question": message})
        return result["answer"]
    
    async def _generate_normal_response(
        self,
        message: str,
        llm: ChatOpenAI,
        memory: ConversationBufferWindowMemory
    ) -> str:
        """ì¼ë°˜ ëŒ€í™” ì‘ë‹µ ìƒì„±"""
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì •
        system_message = SystemMessage(content="""
ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ê³  ì¹œê·¼í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
ëª¨ë¥´ëŠ” ê²ƒì´ ìˆë‹¤ë©´ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”.
""")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        chat_history = memory.chat_memory.messages
        
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [system_message] + chat_history + [HumanMessage(content=message)]
        
        # ì‘ë‹µ ìƒì„±
        response = llm(messages)
        
        # ë©”ëª¨ë¦¬ì— ëŒ€í™” ì¶”ê°€
        memory.chat_memory.add_user_message(message)
        memory.chat_memory.add_ai_message(response.content)
        
        return response.content
    
    def _load_conversation_memory(self, conversation_id: str) -> ConversationBufferWindowMemory:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ"""
        memory = ConversationBufferWindowMemory(
            k=10,  # ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€
            return_messages=True
        )
        
        try:
            # Redisì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ
            history_key = f"conversation:{conversation_id}"
            messages = redis_client.lrange(history_key, -20, -1)  # ìµœê·¼ 20ê°œ ë©”ì‹œì§€
            
            for msg_json in messages:
                msg = json.loads(msg_json)
                if msg["role"] == "user":
                    memory.chat_memory.add_user_message(msg["content"])
                elif msg["role"] == "assistant":
                    memory.chat_memory.add_ai_message(msg["content"])
        
        except Exception as e:
            print(f"Failed to load conversation memory: {e}")
        
        return memory
    
    async def _save_conversation(
        self, 
        conversation_id: str, 
        user_id: str, 
        user_message: str, 
        ai_response: str
    ):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥"""
        try:
            history_key = f"conversation:{conversation_id}"
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
            user_msg = {
                "role": "user",
                "content": user_message,
                "timestamp": time.time(),
                "user_id": user_id
            }
            redis_client.rpush(history_key, json.dumps(user_msg))
            
            # AI ì‘ë‹µ ì €ì¥
            ai_msg = {
                "role": "assistant",
                "content": ai_response,
                "timestamp": time.time()
            }
            redis_client.rpush(history_key, json.dumps(ai_msg))
            
            # TTL ì„¤ì • (7ì¼)
            redis_client.expire(history_key, 604800)
            
        except Exception as e:
            print(f"Failed to save conversation: {e}")

# ì±„íŒ… ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
chat_engine = ChatEngine()

# API ì—”ë“œí¬ì¸íŠ¸
class ChatRequest(BaseModel):
    user_id: str
    conversation_id: str
    message: str
    model: str = "gpt-3.5-turbo"
    temperature: float = 0.7
    max_tokens: int = 1000
    use_rag: bool = False
    context_docs: Optional[List[str]] = None

@app.post("/generate")
async def generate_chat_response(request: ChatRequest):
    """ì±„íŒ… ì‘ë‹µ ìƒì„±"""
    return await chat_engine.generate_response(
        user_id=request.user_id,
        conversation_id=request.conversation_id,
        message=request.message,
        model=request.model,
        temperature=request.temperature,
        max_tokens=request.max_tokens,
        use_rag=request.use_rag,
        context_docs=request.context_docs
    )

@app.get("/conversations/{conversation_id}/history")
async def get_conversation_history(conversation_id: str, limit: int = 50):
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    try:
        history_key = f"conversation:{conversation_id}"
        messages = redis_client.lrange(history_key, -limit, -1)
        
        history = []
        for msg_json in messages:
            history.append(json.loads(msg_json))
        
        return {"conversation_id": conversation_id, "messages": history}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get history: {str(e)}")

@app.delete("/conversations/{conversation_id}")
async def delete_conversation(conversation_id: str):
    """ëŒ€í™” ì‚­ì œ"""
    try:
        history_key = f"conversation:{conversation_id}"
        redis_client.delete(history_key)
        return {"message": "Conversation deleted successfully"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete conversation: {str(e)}")
```

## ğŸ“„ 4. ë¬¸ì„œ ì²˜ë¦¬ ì„œë¹„ìŠ¤

### ì„ë² ë”© ë° ë²¡í„° ì €ì¥

```python
# services/document-service/app/main.py
from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks
from langchain.document_loaders import PyPDFLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
import tempfile
import os
import uuid
from typing import List, Dict, Any
from pydantic import BaseModel

app = FastAPI(title="Document Processing Service", version="1.0.0")

# ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ì„¤ì •
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(
    persist_directory="/app/data/chroma",
    embedding_function=embeddings
)

class DocumentProcessor:
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
    
    async def process_document(
        self, 
        file: UploadFile, 
        user_id: str,
        collection_name: str = "default"
    ) -> Dict[str, Any]:
        """ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„°í™”"""
        
        doc_id = str(uuid.uuid4())
        
        try:
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file.filename.split('.')[-1]}") as tmp_file:
                content = await file.read()
                tmp_file.write(content)
                tmp_file_path = tmp_file.name
            
            # ë¬¸ì„œ ë¡œë“œ
            if file.filename.endswith('.pdf'):
                loader = PyPDFLoader(tmp_file_path)
            elif file.filename.endswith('.txt'):
                loader = TextLoader(tmp_file_path, encoding='utf-8')
            else:
                raise HTTPException(status_code=400, detail="Unsupported file format")
            
            documents = loader.load()
            
            # í…ìŠ¤íŠ¸ ë¶„í• 
            texts = self.text_splitter.split_documents(documents)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for i, text in enumerate(texts):
                text.metadata.update({
                    "document_id": doc_id,
                    "user_id": user_id,
                    "filename": file.filename,
                    "chunk_index": i,
                    "collection": collection_name
                })
            
            # ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€
            vectorstore.add_documents(texts)
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            os.unlink(tmp_file_path)
            
            return {
                "document_id": doc_id,
                "filename": file.filename,
                "chunks_created": len(texts),
                "collection": collection_name,
                "status": "processed"
            }
            
        except Exception as e:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if 'tmp_file_path' in locals():
                try:
                    os.unlink(tmp_file_path)
                except:
                    pass
            
            raise HTTPException(status_code=500, detail=f"Document processing failed: {str(e)}")
    
    async def search_documents(
        self, 
        query: str, 
        user_id: str = None,
        collection_name: str = "default",
        k: int = 5
    ) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ê²€ìƒ‰"""
        
        try:
            # ê²€ìƒ‰ í•„í„° ì„¤ì •
            search_kwargs = {"k": k}
            if user_id:
                search_kwargs["filter"] = {
                    "user_id": user_id,
                    "collection": collection_name
                }
            
            # ìœ ì‚¬ë„ ê²€ìƒ‰
            docs = vectorstore.similarity_search_with_score(query, **search_kwargs)
            
            results = []
            for doc, score in docs:
                results.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "similarity_score": float(score)
                })
            
            return results
            
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Document search failed: {str(e)}")

# ë¬¸ì„œ ì²˜ë¦¬ê¸° ì¸ìŠ¤í„´ìŠ¤
doc_processor = DocumentProcessor()

# API ì—”ë“œí¬ì¸íŠ¸
class SearchRequest(BaseModel):
    query: str
    user_id: str = None
    collection_name: str = "default"
    k: int = 5

@app.post("/upload")
async def upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    user_id: str = "anonymous",
    collection_name: str = "default"
):
    """ë¬¸ì„œ ì—…ë¡œë“œ ë° ì²˜ë¦¬"""
    
    # íŒŒì¼ í¬ê¸° ì œí•œ (10MB)
    if file.size > 10 * 1024 * 1024:
        raise HTTPException(status_code=413, detail="File too large")
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¬¸ì„œ ì²˜ë¦¬
    result = await doc_processor.process_document(file, user_id, collection_name)
    
    return result

@app.post("/search")
async def search_documents(request: SearchRequest):
    """ë¬¸ì„œ ê²€ìƒ‰"""
    return await doc_processor.search_documents(
        query=request.query,
        user_id=request.user_id,
        collection_name=request.collection_name,
        k=request.k
    )

@app.get("/collections/{collection_name}/documents")
async def list_documents(collection_name: str, user_id: str = None):
    """ì»¬ë ‰ì…˜ì˜ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    try:
        # ë²¡í„° ì €ì¥ì†Œì—ì„œ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰
        all_docs = vectorstore.get()
        
        documents = {}
        for i, metadata in enumerate(all_docs['metadatas']):
            if metadata.get('collection') == collection_name:
                if user_id is None or metadata.get('user_id') == user_id:
                    doc_id = metadata.get('document_id')
                    if doc_id not in documents:
                        documents[doc_id] = {
                            "document_id": doc_id,
                            "filename": metadata.get('filename'),
                            "user_id": metadata.get('user_id'),
                            "chunks": 0
                        }
                    documents[doc_id]["chunks"] += 1
        
        return {"documents": list(documents.values())}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list documents: {str(e)}")

@app.delete("/documents/{document_id}")
async def delete_document(document_id: str, user_id: str):
    """ë¬¸ì„œ ì‚­ì œ"""
    try:
        # í•´ë‹¹ ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ ì‚­ì œ
        vectorstore.delete(where={"document_id": document_id, "user_id": user_id})
        
        return {"message": "Document deleted successfully"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete document: {str(e)}")
```

## ğŸ” 5. ì¸ì¦ ì„œë¹„ìŠ¤

### JWT ê¸°ë°˜ ì¸ì¦ ì‹œìŠ¤í…œ

```python
# services/auth-service/app/main.py
from fastapi import FastAPI, HTTPException, Depends, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel
from passlib.context import CryptContext
from jose import JWTError, jwt
from datetime import datetime, timedelta
import redis
import json
from typing import Optional, Dict, Any

app = FastAPI(title="Authentication Service", version="1.0.0")

# ì„¤ì •
SECRET_KEY = "your-secret-key-here"  # í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•  ê²ƒ
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30
REFRESH_TOKEN_EXPIRE_DAYS = 7

# Redis ì—°ê²° (í† í° ê´€ë¦¬ìš©)
redis_client = redis.Redis(host="redis", port=6379, db=1, decode_responses=True)

# ë¹„ë°€ë²ˆí˜¸ í•´ì‹±
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
security = HTTPBearer()

class User(BaseModel):
    user_id: str
    username: str
    email: str
    is_active: bool = True
    token_limit: int = 100000  # ì›”ê°„ í† í° ì œí•œ

class UserCreate(BaseModel):
    username: str
    email: str
    password: str

class UserLogin(BaseModel):
    username: str
    password: str

class TokenResponse(BaseModel):
    access_token: str
    refresh_token: str
    token_type: str = "bearer"
    expires_in: int

class AuthService:
    def __init__(self):
        # ì„ì‹œ ì‚¬ìš©ì ì €ì¥ì†Œ (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©)
        self.users_db = {}
    
    def verify_password(self, plain_password: str, hashed_password: str) -> bool:
        """ë¹„ë°€ë²ˆí˜¸ ê²€ì¦"""
        return pwd_context.verify(plain_password, hashed_password)
    
    def get_password_hash(self, password: str) -> str:
        """ë¹„ë°€ë²ˆí˜¸ í•´ì‹±"""
        return pwd_context.hash(password)
    
    def create_access_token(self, data: dict, expires_delta: Optional[timedelta] = None):
        """ì•¡ì„¸ìŠ¤ í† í° ìƒì„±"""
        to_encode = data.copy()
        if expires_delta:
            expire = datetime.utcnow() + expires_delta
        else:
            expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
        
        to_encode.update({"exp": expire})
        encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
        return encoded_jwt
    
    def create_refresh_token(self, user_id: str) -> str:
        """ë¦¬í”„ë ˆì‹œ í† í° ìƒì„±"""
        data = {"sub": user_id, "type": "refresh"}
        expire = datetime.utcnow() + timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS)
        data.update({"exp": expire})
        
        refresh_token = jwt.encode(data, SECRET_KEY, algorithm=ALGORITHM)
        
        # Redisì— ë¦¬í”„ë ˆì‹œ í† í° ì €ì¥
        redis_client.setex(
            f"refresh_token:{user_id}",
            timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS),
            refresh_token
        )
        
        return refresh_token
    
    def verify_token(self, token: str) -> Dict[str, Any]:
        """í† í° ê²€ì¦"""
        try:
            payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
            user_id: str = payload.get("sub")
            if user_id is None:
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Invalid token"
                )
            return payload
        except JWTError:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid token"
            )
    
    async def register_user(self, user_data: UserCreate) -> User:
        """ì‚¬ìš©ì ë“±ë¡"""
        # ì¤‘ë³µ í™•ì¸
        if user_data.username in self.users_db:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Username already registered"
            )
        
        # ì‚¬ìš©ì ìƒì„±
        user_id = f"user_{len(self.users_db) + 1}"
        hashed_password = self.get_password_hash(user_data.password)
        
        user = User(
            user_id=user_id,
            username=user_data.username,
            email=user_data.email
        )
        
        # ì €ì¥ (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥)
        self.users_db[user_data.username] = {
            "user_id": user_id,
            "username": user_data.username,
            "email": user_data.email,
            "hashed_password": hashed_password,
            "is_active": True,
            "token_limit": 100000,
            "created_at": datetime.utcnow().isoformat()
        }
        
        return user
    
    async def authenticate_user(self, username: str, password: str) -> Optional[User]:
        """ì‚¬ìš©ì ì¸ì¦"""
        user_data = self.users_db.get(username)
        if not user_data:
            return None
        
        if not self.verify_password(password, user_data["hashed_password"]):
            return None
        
        return User(**user_data)
    
    async def get_user(self, user_id: str) -> Optional[User]:
        """ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ"""
        for user_data in self.users_db.values():
            if user_data["user_id"] == user_id:
                return User(**user_data)
        return None

# ì¸ì¦ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
auth_service = AuthService()

async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)) -> User:
    """í˜„ì¬ ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    payload = auth_service.verify_token(credentials.credentials)
    user = await auth_service.get_user(payload.get("sub"))
    
    if user is None:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="User not found"
        )
    
    return user

# API ì—”ë“œí¬ì¸íŠ¸
@app.post("/register", response_model=User)
async def register(user_data: UserCreate):
    """ì‚¬ìš©ì ë“±ë¡"""
    return await auth_service.register_user(user_data)

@app.post("/login", response_model=TokenResponse)
async def login(user_data: UserLogin):
    """ë¡œê·¸ì¸"""
    user = await auth_service.authenticate_user(user_data.username, user_data.password)
    
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password"
        )
    
    # í† í° ìƒì„±
    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = auth_service.create_access_token(
        data={"sub": user.user_id, "username": user.username},
        expires_delta=access_token_expires
    )
    
    refresh_token = auth_service.create_refresh_token(user.user_id)
    
    return TokenResponse(
        access_token=access_token,
        refresh_token=refresh_token,
        expires_in=ACCESS_TOKEN_EXPIRE_MINUTES * 60
    )

@app.post("/refresh", response_model=TokenResponse)
async def refresh_token(refresh_token: str):
    """í† í° ê°±ì‹ """
    try:
        payload = jwt.decode(refresh_token, SECRET_KEY, algorithms=[ALGORITHM])
        user_id = payload.get("sub")
        token_type = payload.get("type")
        
        if token_type != "refresh":
            raise HTTPException(status_code=401, detail="Invalid token type")
        
        # Redisì—ì„œ ë¦¬í”„ë ˆì‹œ í† í° í™•ì¸
        stored_token = redis_client.get(f"refresh_token:{user_id}")
        if stored_token != refresh_token:
            raise HTTPException(status_code=401, detail="Invalid refresh token")
        
        # ìƒˆ ì•¡ì„¸ìŠ¤ í† í° ìƒì„±
        user = await auth_service.get_user(user_id)
        if not user:
            raise HTTPException(status_code=401, detail="User not found")
        
        access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
        access_token = auth_service.create_access_token(
            data={"sub": user.user_id, "username": user.username},
            expires_delta=access_token_expires
        )
        
        return TokenResponse(
            access_token=access_token,
            refresh_token=refresh_token,
            expires_in=ACCESS_TOKEN_EXPIRE_MINUTES * 60
        )
        
    except JWTError:
        raise HTTPException(status_code=401, detail="Invalid refresh token")

@app.get("/me", response_model=User)
async def get_me(current_user: User = Depends(get_current_user)):
    """í˜„ì¬ ì‚¬ìš©ì ì •ë³´"""
    return current_user

@app.post("/usage/log")
async def log_usage(usage_data: dict):
    """ì‚¬ìš©ëŸ‰ ë¡œê¹…"""
    try:
        user_id = usage_data["user_id"]
        tokens_used = usage_data["tokens_used"]
        
        # ì›”ê°„ ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
        month_key = f"usage:{user_id}:{datetime.now().strftime('%Y-%m')}"
        redis_client.incrby(month_key, tokens_used)
        redis_client.expire(month_key, timedelta(days=32))
        
        return {"message": "Usage logged successfully"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to log usage: {str(e)}")

@app.get("/usage/{user_id}")
async def get_usage(user_id: str, month: str = None):
    """ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
    if not month:
        month = datetime.now().strftime('%Y-%m')
    
    month_key = f"usage:{user_id}:{month}"
    usage = redis_client.get(month_key) or 0
    
    return {
        "user_id": user_id,
        "month": month,
        "tokens_used": int(usage)
    }
```

## ğŸ³ 6. Docker ë° ë°°í¬ ì„¤ì •

### Docker Compose êµ¬ì„±

```yaml
# docker-compose.yml
version: '3.8'

services:
  # Redis
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes

  # API Gateway
  api-gateway:
    build: ./services/api-gateway
    ports:
      - "8000:8000"
    environment:
      - REDIS_URL=redis://redis:6379
    depends_on:
      - redis
      - chat-service
      - auth-service
      - document-service
    volumes:
      - ./logs:/app/logs

  # Chat Service
  chat-service:
    build: ./services/chat-service
    ports:
      - "8001:8001"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REDIS_URL=redis://redis:6379
    depends_on:
      - redis
    volumes:
      - chroma_data:/app/data/chroma

  # Auth Service
  auth-service:
    build: ./services/auth-service
    ports:
      - "8004:8004"
    environment:
      - REDIS_URL=redis://redis:6379
      - SECRET_KEY=${SECRET_KEY}
    depends_on:
      - redis

  # Document Service
  document-service:
    build: ./services/document-service
    ports:
      - "8003:8003"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - chroma_data:/app/data/chroma
      - ./uploads:/app/uploads

  # Monitoring
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana

volumes:
  redis_data:
  chroma_data:
  prometheus_data:
  grafana_data:
```

### Kubernetes ë°°í¬ ì„¤ì •

```yaml
# k8s/api-gateway-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-gateway
  labels:
    app: api-gateway
spec:
  replicas: 3
  selector:
    matchLabels:
      app: api-gateway
  template:
    metadata:
      labels:
        app: api-gateway
    spec:
      containers:
      - name: api-gateway
        image: llm-msa/api-gateway:latest
        ports:
        - containerPort: 8000
        env:
        - name: REDIS_URL
          value: "redis://redis:6379"
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: api-gateway-service
spec:
  selector:
    app: api-gateway
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8000
  type: LoadBalancer
```

## ğŸ‰ ê²°ë¡ 

FastAPIì™€ LangChainì„ í™œìš©í•œ LLM MSA ì„œë¹„ìŠ¤ êµ¬ì¶•ì˜ í•µì‹¬ ìš”ì†Œë“¤ì„ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤.

### âœ… ì£¼ìš” ë‹¬ì„± ì‚¬í•­

1. **í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜**: ê° ì„œë¹„ìŠ¤ë³„ ë…ë¦½ì  ìŠ¤ì¼€ì¼ë§
2. **ì‹¤ì‹œê°„ ì²˜ë¦¬**: ìŠ¤íŠ¸ë¦¬ë° ê¸°ë°˜ ì±„íŒ… ë° ë¬¸ì„œ ì²˜ë¦¬
3. **RAG ì‹œìŠ¤í…œ**: ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ êµ¬í˜„
4. **ì™„ì „í•œ ì¸ì¦**: JWT ê¸°ë°˜ ë³´ì•ˆ ì‹œìŠ¤í…œ
5. **ëª¨ë‹ˆí„°ë§**: Prometheus/Grafana ì—°ë™
6. **ë°°í¬ ìë™í™”**: Docker/Kubernetes ì§€ì›

### ğŸ› ï¸ ì‹¤ë¬´ ì ìš© ê°€ì´ë“œ

- **ë‹¨ê³„ì  ë„ì…**: ëª¨ë†€ë¦¬ìŠ¤ì—ì„œ MSAë¡œ ì ì§„ì  ì „í™˜
- **ì„±ëŠ¥ ìµœì í™”**: ìºì‹±ê³¼ ë¡œë“œ ë°¸ëŸ°ì‹± ì ìš©
- **ë¹„ìš© ê´€ë¦¬**: í† í° ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë° ì œí•œ
- **ë³´ì•ˆ ê°•í™”**: API ê²Œì´íŠ¸ì›¨ì´ë¥¼ í†µí•œ ì¤‘ì•™ì§‘ì¤‘ì‹ ë³´ì•ˆ ê´€ë¦¬

ì´ ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ì—…ìš© AI ì„œë¹„ìŠ¤, ê°œì¸ ì–´ì‹œìŠ¤í„´íŠ¸, ë¬¸ì„œ ë¶„ì„ ì‹œìŠ¤í…œ ë“± ë‹¤ì–‘í•œ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ RAG ì‹œìŠ¤í…œì„ í†µí•´ ë„ë©”ì¸ íŠ¹í™” ì§€ì‹ì„ í™œìš©í•œ ê³ í’ˆì§ˆ ì‘ë‹µ ìƒì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
