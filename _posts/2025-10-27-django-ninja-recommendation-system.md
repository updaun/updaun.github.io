---
layout: post
title: "Django Ninjaë¡œ êµ¬ì¶•í•˜ëŠ” ì§€ëŠ¥í˜• ì¶”ì²œì‹œìŠ¤í…œ - í˜‘ì—… í•„í„°ë§ë¶€í„° ë”¥ëŸ¬ë‹ê¹Œì§€"
subtitle: "ì‹¤ì‹œê°„ ê°œì¸í™” ì¶”ì²œ ì—”ì§„ ì™„ë²½ êµ¬í˜„ ê°€ì´ë“œ"
date: 2025-10-27 10:00:00 +0900
background: '/img/posts/django-ninja-recommendation-bg.jpg'
categories: [Django, MachineLearning, AI]
tags: [django-ninja, recommendation-system, collaborative-filtering, machine-learning, personalization, fastapi]
---

# ğŸ¤– Django Ninjaë¡œ êµ¬ì¶•í•˜ëŠ” ì§€ëŠ¥í˜• ì¶”ì²œì‹œìŠ¤í…œ

í˜„ëŒ€ ì„œë¹„ìŠ¤ì—ì„œ **ê°œì¸í™” ì¶”ì²œ**ì€ ì‚¬ìš©ì ê²½í—˜ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ê³¼ë¥¼ ê²°ì •í•˜ëŠ” í•µì‹¬ ìš”ì†Œì…ë‹ˆë‹¤. Netflixì˜ ì˜í™” ì¶”ì²œ, Amazonì˜ ìƒí’ˆ ì¶”ì²œ, Spotifyì˜ ìŒì•… ì¶”ì²œ ë“± ëª¨ë“  ì„±ê³µí•œ í”Œë«í¼ì€ ê°•ë ¥í•œ ì¶”ì²œ ì—”ì§„ì„ ë³´ìœ í•˜ê³  ìˆì£ .

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” **Django Ninja**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì¶”ì²œì‹œìŠ¤í…œ**ì„ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤. ê¸°ë³¸ì ì¸ í˜‘ì—… í•„í„°ë§ë¶€í„° ìµœì‹  ë”¥ëŸ¬ë‹ ê¸°ë²•ê¹Œì§€ ë‹¨ê³„ë³„ë¡œ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.

## ğŸ¯ êµ¬í˜„í•  ì¶”ì²œì‹œìŠ¤í…œ ê¸°ëŠ¥

- **í˜‘ì—… í•„í„°ë§** (User-Based, Item-Based)
- **ì½˜í…ì¸  ê¸°ë°˜ í•„í„°ë§** (Content-Based Filtering)
- **í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ** (Multiple Algorithms)
- **ì‹¤ì‹œê°„ ì¶”ì²œ** (Real-time Recommendations)
- **A/B í…ŒìŠ¤íŠ¸** ì§€ì›
- **ì¶”ì²œ ì„±ê³¼ ë¶„ì„** (CTR, Conversion Rate)
- **ì½œë“œ ìŠ¤íƒ€íŠ¸ ë¬¸ì œ** í•´ê²°

---

## ğŸ“Š 1. ì¶”ì²œì‹œìŠ¤í…œ ê¸°ì´ˆ ì„¤ê³„

ë¨¼ì € ì¶”ì²œì‹œìŠ¤í…œì˜ í•µì‹¬ ê°œë…ê³¼ ì•„í‚¤í…ì²˜ë¥¼ ì´í•´í•˜ê³  í”„ë¡œì íŠ¸ êµ¬ì¡°ë¥¼ ì„¤ê³„í•´ë³´ê² ìŠµë‹ˆë‹¤.

### 1.1 ì¶”ì²œì‹œìŠ¤í…œ ìœ í˜•ë³„ íŠ¹ì§•

```python
# docs/recommendation_types.py
"""
ì¶”ì²œì‹œìŠ¤í…œ ì£¼ìš” ìœ í˜•ê³¼ íŠ¹ì§•

1. í˜‘ì—… í•„í„°ë§ (Collaborative Filtering)
   - User-Based CF: ë¹„ìŠ·í•œ ì‚¬ìš©ìë“¤ì´ ì¢‹ì•„í•œ ì•„ì´í…œ ì¶”ì²œ
   - Item-Based CF: ì‚¬ìš©ìê°€ ì¢‹ì•„í•œ ì•„ì´í…œê³¼ ìœ ì‚¬í•œ ì•„ì´í…œ ì¶”ì²œ
   
   ì¥ì : ë„ë©”ì¸ ì§€ì‹ ë¶ˆí•„ìš”, ì˜ì™¸ì„±(Serendipity) ì œê³µ
   ë‹¨ì : ì½œë“œ ìŠ¤íƒ€íŠ¸ ë¬¸ì œ, í¬ì†Œì„± ë¬¸ì œ, í™•ì¥ì„± ì´ìŠˆ

2. ì½˜í…ì¸  ê¸°ë°˜ í•„í„°ë§ (Content-Based Filtering)
   - ì•„ì´í…œì˜ ì†ì„±/íŠ¹ì§•ì„ ë¶„ì„í•˜ì—¬ ìœ ì‚¬ ì•„ì´í…œ ì¶”ì²œ
   
   ì¥ì : ì½œë“œ ìŠ¤íƒ€íŠ¸ ë¬¸ì œ í•´ê²°, íˆ¬ëª…í•œ ì¶”ì²œ ì´ìœ 
   ë‹¨ì : ê³¼ì í•©, ë‹¤ì–‘ì„± ë¶€ì¡±, ë„ë©”ì¸ ì§€ì‹ í•„ìš”

3. í•˜ì´ë¸Œë¦¬ë“œ (Hybrid)
   - ì—¬ëŸ¬ ë°©ë²•ì„ ì¡°í•©í•˜ì—¬ ê°ê°ì˜ ë‹¨ì  ë³´ì™„
   
   ë°©ì‹: Weighted, Switching, Cascade, Mixed, Feature Combination
"""

class RecommendationStrategy:
    """ì¶”ì²œ ì „ëµ ì¸í„°í˜ì´ìŠ¤"""
    
    def recommend(self, user_id: int, num_recommendations: int = 10) -> List[Dict]:
        raise NotImplementedError
    
    def explain(self, user_id: int, item_id: int) -> Dict:
        """ì¶”ì²œ ì´ìœ  ì„¤ëª…"""
        raise NotImplementedError
    
    def get_performance_metrics(self) -> Dict:
        """ì¶”ì²œ ì„±ê³¼ ì§€í‘œ"""
        raise NotImplementedError
```

### 1.2 í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ì •

```bash
# requirements.txt
django==4.2.7
django-ninja==1.0.1
numpy==1.24.3
pandas==2.0.3
scikit-learn==1.3.0
scipy==1.11.1
redis==5.0.1
celery==5.3.4

# ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ìš©
surprise==1.1.3
implicit==0.7.2
lightfm==1.17

# ë”¥ëŸ¬ë‹ (ì„ íƒì‚¬í•­)
torch==2.0.1
sentence-transformers==2.2.2

# ë°ì´í„° ì²˜ë¦¬
sqlalchemy==2.0.19
psycopg2-binary==2.9.9

# ëª¨ë‹ˆí„°ë§
prometheus-client==0.17.1
```

```python
# config/settings/base.py
from pathlib import Path
from decouple import config

# ì¶”ì²œì‹œìŠ¤í…œ ì„¤ì •
RECOMMENDATION_SETTINGS = {
    # ê¸°ë³¸ ì„¤ì •
    'DEFAULT_ALGORITHM': 'collaborative_filtering',
    'DEFAULT_NUM_RECOMMENDATIONS': 10,
    'MAX_RECOMMENDATIONS': 50,
    
    # ì„±ëŠ¥ ì„¤ì •
    'BATCH_SIZE': 1000,
    'CACHE_TTL': 3600,  # 1ì‹œê°„
    'MODEL_UPDATE_INTERVAL': 86400,  # 24ì‹œê°„
    
    # ì•Œê³ ë¦¬ì¦˜ë³„ íŒŒë¼ë¯¸í„°
    'COLLABORATIVE_FILTERING': {
        'similarity_metric': 'cosine',
        'min_interactions': 5,
        'n_neighbors': 50,
    },
    
    'CONTENT_BASED': {
        'feature_weights': {
            'category': 0.3,
            'brand': 0.2,
            'price_range': 0.1,
            'description': 0.4,
        }
    },
    
    # A/B í…ŒìŠ¤íŠ¸ ì„¤ì •
    'AB_TEST': {
        'enabled': True,
        'test_ratio': 0.1,  # 10% ì‚¬ìš©ìê°€ í…ŒìŠ¤íŠ¸ ê·¸ë£¹
    }
}

# Redis ì„¤ì • (ìºì‹±ìš©)
REDIS_URL = config('REDIS_URL', default='redis://localhost:6379/0')
CACHES = {
    'default': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': REDIS_URL,
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
        }
    }
}

INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    
    # Third party
    'ninja',
    'django_extensions',
    'django_redis',
    
    # Local apps
    'apps.accounts',
    'apps.products',
    'apps.recommendations',  # ìƒˆë¡œ ìƒì„±
    'apps.analytics',        # ìƒˆë¡œ ìƒì„±
]
```

### 1.3 í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
recommendation_system/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ recommendations/
â”‚   â”‚   â”œâ”€â”€ models.py              # ì¶”ì²œ ê´€ë ¨ ëª¨ë¸
â”‚   â”‚   â”œâ”€â”€ algorithms/            # ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ë“¤
â”‚   â”‚   â”‚   â”œâ”€â”€ collaborative.py   # í˜‘ì—… í•„í„°ë§
â”‚   â”‚   â”‚   â”œâ”€â”€ content_based.py   # ì½˜í…ì¸  ê¸°ë°˜
â”‚   â”‚   â”‚   â”œâ”€â”€ hybrid.py          # í•˜ì´ë¸Œë¦¬ë“œ
â”‚   â”‚   â”‚   â””â”€â”€ deep_learning.py   # ë”¥ëŸ¬ë‹ ê¸°ë°˜
â”‚   â”‚   â”œâ”€â”€ services.py            # ì¶”ì²œ ì„œë¹„ìŠ¤ ë¡œì§
â”‚   â”‚   â”œâ”€â”€ api.py                 # Django Ninja API
â”‚   â”‚   â”œâ”€â”€ tasks.py               # Celery ë¹„ë™ê¸° ì‘ì—…
â”‚   â”‚   â”œâ”€â”€ schemas.py             # Pydantic ìŠ¤í‚¤ë§ˆ
â”‚   â”‚   â””â”€â”€ utils/                 # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
â”‚   â”‚       â”œâ”€â”€ metrics.py         # ì„±ê³¼ ì¸¡ì •
â”‚   â”‚       â”œâ”€â”€ data_processor.py  # ë°ì´í„° ì „ì²˜ë¦¬
â”‚   â”‚       â””â”€â”€ cache_manager.py   # ìºì‹œ ê´€ë¦¬
â”‚   â”œâ”€â”€ products/
â”‚   â”‚   â””â”€â”€ models.py              # ìƒí’ˆ ëª¨ë¸
â”‚   â”œâ”€â”€ accounts/
â”‚   â”‚   â””â”€â”€ models.py              # ì‚¬ìš©ì ëª¨ë¸
â”‚   â””â”€â”€ analytics/
â”‚       â”œâ”€â”€ models.py              # ë¶„ì„ ë°ì´í„° ëª¨ë¸
â”‚       â””â”€â”€ services.py            # ë¶„ì„ ì„œë¹„ìŠ¤
â”œâ”€â”€ ml_models/                     # í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥
â”œâ”€â”€ data/                          # ë°ì´í„° íŒŒì¼ë“¤
â””â”€â”€ notebooks/                     # ì£¼í”¼í„° ë…¸íŠ¸ë¶ (ì‹¤í—˜ìš©)
```

---

## ğŸ—ï¸ 2. ë°ì´í„° ëª¨ë¸ ë° ìŠ¤í‚¤ë§ˆ ì„¤ê³„

ì¶”ì²œì‹œìŠ¤í…œì˜ í•µì‹¬ì´ ë˜ëŠ” ë°ì´í„° ëª¨ë¸ë“¤ì„ ì„¤ê³„í•´ë³´ê² ìŠµë‹ˆë‹¤.

### 2.1 ê¸°ë³¸ ì—”í‹°í‹° ëª¨ë¸

```python
# apps/products/models.py
from django.db import models
from django.contrib.auth.models import User
from django.contrib.postgres.fields import ArrayField
from model_utils.models import TimeStampedModel

class Category(TimeStampedModel):
    """ìƒí’ˆ ì¹´í…Œê³ ë¦¬"""
    name = models.CharField(max_length=100, unique=True)
    parent = models.ForeignKey('self', on_delete=models.CASCADE, null=True, blank=True)
    level = models.IntegerField(default=0)
    
    def __str__(self):
        return self.name

class Brand(TimeStampedModel):
    """ë¸Œëœë“œ"""
    name = models.CharField(max_length=100, unique=True)
    description = models.TextField(blank=True)
    
    def __str__(self):
        return self.name

class Product(TimeStampedModel):
    """ìƒí’ˆ"""
    name = models.CharField(max_length=200)
    description = models.TextField()
    category = models.ForeignKey(Category, on_delete=models.PROTECT, related_name='products')
    brand = models.ForeignKey(Brand, on_delete=models.PROTECT, related_name='products')
    
    # ê°€ê²© ì •ë³´
    price = models.DecimalField(max_digits=10, decimal_places=2)
    original_price = models.DecimalField(max_digits=10, decimal_places=2, null=True, blank=True)
    
    # ì¶”ì²œì‹œìŠ¤í…œìš© ë©”íƒ€ë°ì´í„°
    tags = ArrayField(models.CharField(max_length=50), default=list, blank=True)
    attributes = models.JSONField(default=dict, blank=True)  # ìƒ‰ìƒ, ì‚¬ì´ì¦ˆ, ì†Œì¬ ë“±
    
    # í†µê³„ ì •ë³´
    view_count = models.IntegerField(default=0)
    purchase_count = models.IntegerField(default=0)
    rating_sum = models.IntegerField(default=0)
    rating_count = models.IntegerField(default=0)
    
    # ìƒí’ˆ ìƒíƒœ
    is_active = models.BooleanField(default=True)
    stock_quantity = models.IntegerField(default=0)
    
    class Meta:
        indexes = [
            models.Index(fields=['category', 'is_active']),
            models.Index(fields=['brand', 'is_active']),
            models.Index(fields=['price']),
            models.Index(fields=['view_count']),
            models.Index(fields=['purchase_count']),
        ]
    
    def __str__(self):
        return self.name
    
    @property
    def average_rating(self):
        if self.rating_count > 0:
            return self.rating_sum / self.rating_count
        return 0
    
    @property
    def discount_rate(self):
        if self.original_price:
            return (self.original_price - self.price) / self.original_price * 100
        return 0

# apps/accounts/models.py
class UserProfile(TimeStampedModel):
    """ì‚¬ìš©ì í”„ë¡œí•„ í™•ì¥"""
    user = models.OneToOneField(User, on_delete=models.CASCADE, related_name='profile')
    
    # ì¸êµ¬í†µê³„í•™ì  ì •ë³´
    age_range = models.CharField(max_length=20, blank=True)  # 20-29, 30-39 ë“±
    gender = models.CharField(max_length=10, blank=True)
    location = models.CharField(max_length=100, blank=True)
    
    # ì„ í˜¸ë„ ì •ë³´
    preferred_categories = models.ManyToManyField(Category, blank=True)
    preferred_brands = models.ManyToManyField(Brand, blank=True)
    preferred_price_range = models.JSONField(default=dict, blank=True)  # min, max
    
    # í–‰ë™ íŒ¨í„´
    shopping_frequency = models.CharField(max_length=20, blank=True)  # weekly, monthly ë“±
    preferred_shopping_time = models.CharField(max_length=20, blank=True)  # morning, evening ë“±
    
    # ê°œì¸í™” ì„¤ì •
    recommendation_enabled = models.BooleanField(default=True)
    email_recommendations = models.BooleanField(default=True)
    
    def __str__(self):
        return f"{self.user.username} Profile"
```

### 2.2 ìƒí˜¸ì‘ìš© ë° ì¶”ì²œ ëª¨ë¸

```python
# apps/recommendations/models.py
from django.db import models
from django.contrib.auth.models import User
from django.contrib.contenttypes.fields import GenericForeignKey
from django.contrib.contenttypes.models import ContentType
from model_utils.models import TimeStampedModel
from model_utils import Choices

class InteractionType(models.TextChoices):
    """ìƒí˜¸ì‘ìš© ìœ í˜•"""
    VIEW = 'view', 'ì¡°íšŒ'
    LIKE = 'like', 'ì¢‹ì•„ìš”'
    CART = 'cart', 'ì¥ë°”êµ¬ë‹ˆ ì¶”ê°€'
    PURCHASE = 'purchase', 'êµ¬ë§¤'
    RATING = 'rating', 'í‰ì '
    REVIEW = 'review', 'ë¦¬ë·°'
    SHARE = 'share', 'ê³µìœ '
    BOOKMARK = 'bookmark', 'ë¶ë§ˆí¬'

class UserInteraction(TimeStampedModel):
    """ì‚¬ìš©ì-ì•„ì´í…œ ìƒí˜¸ì‘ìš©"""
    user = models.ForeignKey(User, on_delete=models.CASCADE, related_name='interactions')
    product = models.ForeignKey('products.Product', on_delete=models.CASCADE, related_name='interactions')
    
    interaction_type = models.CharField(max_length=20, choices=InteractionType.choices)
    value = models.FloatField(null=True, blank=True)  # í‰ì , êµ¬ë§¤ ìˆ˜ëŸ‰ ë“±
    
    # ì»¨í…ìŠ¤íŠ¸ ì •ë³´
    session_id = models.CharField(max_length=100, blank=True)
    device_type = models.CharField(max_length=20, blank=True)  # mobile, desktop, tablet
    source = models.CharField(max_length=50, blank=True)  # search, recommendation, category_browse
    
    # ìœ„ì¹˜ ì •ë³´
    ip_address = models.GenericIPAddressField(null=True, blank=True)
    user_agent = models.TextField(blank=True)
    
    class Meta:
        indexes = [
            models.Index(fields=['user', 'interaction_type', 'created']),
            models.Index(fields=['product', 'interaction_type', 'created']),
            models.Index(fields=['user', 'product', 'interaction_type']),
            models.Index(fields=['session_id']),
            models.Index(fields=['created']),
        ]
        unique_together = [
            ('user', 'product', 'interaction_type', 'created'),
        ]
    
    def __str__(self):
        return f"{self.user.username} - {self.product.name} - {self.interaction_type}"

class RecommendationAlgorithm(TimeStampedModel):
    """ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ ì •ë³´"""
    name = models.CharField(max_length=100, unique=True)
    version = models.CharField(max_length=20)
    description = models.TextField()
    
    # ì„¤ì • ë° íŒŒë¼ë¯¸í„°
    parameters = models.JSONField(default=dict)
    is_active = models.BooleanField(default=True)
    
    # ì„±ëŠ¥ ì§€í‘œ
    precision = models.FloatField(null=True, blank=True)
    recall = models.FloatField(null=True, blank=True)
    f1_score = models.FloatField(null=True, blank=True)
    coverage = models.FloatField(null=True, blank=True)
    
    def __str__(self):
        return f"{self.name} v{self.version}"

class RecommendationRequest(TimeStampedModel):
    """ì¶”ì²œ ìš”ì²­ ë¡œê·¸"""
    user = models.ForeignKey(User, on_delete=models.CASCADE, related_name='recommendation_requests')
    algorithm = models.ForeignKey(RecommendationAlgorithm, on_delete=models.CASCADE)
    
    # ìš”ì²­ ì •ë³´
    num_requested = models.IntegerField(default=10)
    context = models.JSONField(default=dict)  # ì¶”ì²œ ì»¨í…ìŠ¤íŠ¸ (í˜ì´ì§€, ì‹œê°„ ë“±)
    
    # ì‘ë‹µ ì •ë³´
    num_returned = models.IntegerField(default=0)
    response_time_ms = models.IntegerField(default=0)
    
    # ì„±ê³¼ ì¶”ì 
    impressions = models.IntegerField(default=0)  # ë…¸ì¶œ ìˆ˜
    clicks = models.IntegerField(default=0)       # í´ë¦­ ìˆ˜
    conversions = models.IntegerField(default=0)   # ì „í™˜ ìˆ˜
    
    class Meta:
        indexes = [
            models.Index(fields=['user', 'created']),
            models.Index(fields=['algorithm', 'created']),
        ]

class RecommendationResult(TimeStampedModel):
    """ì¶”ì²œ ê²°ê³¼"""
    request = models.ForeignKey(RecommendationRequest, on_delete=models.CASCADE, related_name='results')
    product = models.ForeignKey('products.Product', on_delete=models.CASCADE)
    
    # ì¶”ì²œ ì •ë³´
    rank = models.IntegerField()  # ì¶”ì²œ ìˆœìœ„
    score = models.FloatField()   # ì¶”ì²œ ì ìˆ˜
    explanation = models.JSONField(default=dict)  # ì¶”ì²œ ì´ìœ 
    
    # ì„±ê³¼ ì¶”ì 
    was_viewed = models.BooleanField(default=False)
    was_clicked = models.BooleanField(default=False)
    was_purchased = models.BooleanField(default=False)
    
    # í”¼ë“œë°±
    user_feedback = models.CharField(max_length=20, blank=True)  # like, dislike, not_interested
    
    class Meta:
        indexes = [
            models.Index(fields=['request', 'rank']),
            models.Index(fields=['product']),
        ]
        unique_together = [('request', 'product')]
    
    def __str__(self):
        return f"Recommendation #{self.rank}: {self.product.name}"

class UserSimilarity(TimeStampedModel):
    """ì‚¬ìš©ì ê°„ ìœ ì‚¬ë„ (í˜‘ì—… í•„í„°ë§ìš©)"""
    user1 = models.ForeignKey(User, on_delete=models.CASCADE, related_name='similarities_as_user1')
    user2 = models.ForeignKey(User, on_delete=models.CASCADE, related_name='similarities_as_user2')
    
    similarity_score = models.FloatField()
    algorithm = models.CharField(max_length=50)  # cosine, pearson, jaccard ë“±
    
    # ê³„ì‚° ë©”íƒ€ë°ì´í„°
    common_items_count = models.IntegerField(default=0)
    last_calculated = models.DateTimeField(auto_now=True)
    
    class Meta:
        indexes = [
            models.Index(fields=['user1', 'similarity_score']),
            models.Index(fields=['user2', 'similarity_score']),
        ]
        unique_together = [('user1', 'user2', 'algorithm')]
    
    def __str__(self):
        return f"{self.user1.username} - {self.user2.username}: {self.similarity_score:.3f}"

class ItemSimilarity(TimeStampedModel):
    """ì•„ì´í…œ ê°„ ìœ ì‚¬ë„"""
    item1 = models.ForeignKey('products.Product', on_delete=models.CASCADE, related_name='similarities_as_item1')
    item2 = models.ForeignKey('products.Product', on_delete=models.CASCADE, related_name='similarities_as_item2')
    
    similarity_score = models.FloatField()
    algorithm = models.CharField(max_length=50)  # content, collaborative, hybrid
    
    # ìœ ì‚¬ë„ êµ¬ì„± ìš”ì†Œë³„ ì ìˆ˜
    category_similarity = models.FloatField(default=0)
    brand_similarity = models.FloatField(default=0)
    price_similarity = models.FloatField(default=0)
    feature_similarity = models.FloatField(default=0)
    
    last_calculated = models.DateTimeField(auto_now=True)
    
    class Meta:
        indexes = [
            models.Index(fields=['item1', 'similarity_score']),
            models.Index(fields=['item2', 'similarity_score']),
        ]
        unique_together = [('item1', 'item2', 'algorithm')]

class RecommendationCache(TimeStampedModel):
    """ì¶”ì²œ ê²°ê³¼ ìºì‹œ"""
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    algorithm = models.ForeignKey(RecommendationAlgorithm, on_delete=models.CASCADE)
    
    # ìºì‹œ í‚¤ì™€ ë°ì´í„°
    cache_key = models.CharField(max_length=200, unique=True)
    recommendations = models.JSONField()  # ì¶”ì²œ ê²°ê³¼ JSON
    
    # ìºì‹œ ê´€ë¦¬
    expires_at = models.DateTimeField()
    hit_count = models.IntegerField(default=0)
    
    class Meta:
        indexes = [
            models.Index(fields=['user', 'algorithm']),
            models.Index(fields=['expires_at']),
        ]
```

### 2.3 Pydantic ìŠ¤í‚¤ë§ˆ ì •ì˜

```python
# apps/recommendations/schemas.py
from ninja import Schema, Field
from typing import List, Optional, Dict, Any
from datetime import datetime
from pydantic import validator

class InteractionCreateSchema(Schema):
    """ìƒí˜¸ì‘ìš© ìƒì„± ìŠ¤í‚¤ë§ˆ"""
    product_id: int
    interaction_type: str = Field(..., description="ìƒí˜¸ì‘ìš© ìœ í˜• (view, like, cart, purchase ë“±)")
    value: Optional[float] = Field(None, description="ìƒí˜¸ì‘ìš© ê°’ (í‰ì , ìˆ˜ëŸ‰ ë“±)")
    context: Optional[Dict[str, Any]] = Field(None, description="ì»¨í…ìŠ¤íŠ¸ ì •ë³´")
    
    @validator('interaction_type')
    def validate_interaction_type(cls, v):
        allowed_types = ['view', 'like', 'cart', 'purchase', 'rating', 'review', 'share', 'bookmark']
        if v not in allowed_types:
            raise ValueError(f'Invalid interaction type. Must be one of: {allowed_types}')
        return v

class ProductBasicSchema(Schema):
    """ê¸°ë³¸ ìƒí’ˆ ì •ë³´ ìŠ¤í‚¤ë§ˆ"""
    id: int
    name: str
    price: float
    image_url: Optional[str] = None
    category_name: str
    brand_name: str
    average_rating: float
    view_count: int

class RecommendationItemSchema(Schema):
    """ì¶”ì²œ ì•„ì´í…œ ìŠ¤í‚¤ë§ˆ"""
    product: ProductBasicSchema
    score: float = Field(..., description="ì¶”ì²œ ì ìˆ˜")
    rank: int = Field(..., description="ì¶”ì²œ ìˆœìœ„")
    explanation: Optional[Dict[str, Any]] = Field(None, description="ì¶”ì²œ ì´ìœ ")
    
    class Config:
        from_attributes = True

class RecommendationRequestSchema(Schema):
    """ì¶”ì²œ ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    algorithm: Optional[str] = Field('hybrid', description="ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜")
    num_recommendations: int = Field(10, ge=1, le=50, description="ì¶”ì²œ ê°œìˆ˜")
    context: Optional[Dict[str, Any]] = Field(None, description="ì¶”ì²œ ì»¨í…ìŠ¤íŠ¸")
    include_explanation: bool = Field(False, description="ì¶”ì²œ ì´ìœ  í¬í•¨ ì—¬ë¶€")
    
    @validator('algorithm')
    def validate_algorithm(cls, v):
        allowed_algorithms = ['collaborative', 'content_based', 'hybrid', 'popular', 'random']
        if v not in allowed_algorithms:
            raise ValueError(f'Invalid algorithm. Must be one of: {allowed_algorithms}')
        return v

class RecommendationResponseSchema(Schema):
    """ì¶”ì²œ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    recommendations: List[RecommendationItemSchema]
    algorithm_used: str
    total_count: int
    response_time_ms: int
    request_id: str = Field(..., description="ìš”ì²­ ID (ì¶”ì ìš©)")
    
    # ë©”íƒ€ ì •ë³´
    user_profile: Optional[Dict[str, Any]] = None
    diversification_applied: bool = False
    cache_hit: bool = False

class SimilarItemsSchema(Schema):
    """ìœ ì‚¬ ì•„ì´í…œ ìŠ¤í‚¤ë§ˆ"""
    target_product_id: int
    similar_items: List[RecommendationItemSchema]
    similarity_type: str = Field(..., description="ìœ ì‚¬ë„ ê³„ì‚° ë°©ë²•")

class UserFeedbackSchema(Schema):
    """ì‚¬ìš©ì í”¼ë“œë°± ìŠ¤í‚¤ë§ˆ"""
    recommendation_id: int
    feedback_type: str = Field(..., description="í”¼ë“œë°± ìœ í˜• (like, dislike, not_interested)")
    reason: Optional[str] = Field(None, description="í”¼ë“œë°± ì´ìœ ")
    
    @validator('feedback_type')
    def validate_feedback_type(cls, v):
        allowed_types = ['like', 'dislike', 'not_interested', 'inappropriate']
        if v not in allowed_types:
            raise ValueError(f'Invalid feedback type. Must be one of: {allowed_types}')
        return v

class RecommendationAnalyticsSchema(Schema):
    """ì¶”ì²œ ë¶„ì„ ìŠ¤í‚¤ë§ˆ"""
    algorithm: str
    period: str  # daily, weekly, monthly
    metrics: Dict[str, float] = Field(..., description="ì„±ê³¼ ì§€í‘œ")
    
    # ì£¼ìš” ë©”íŠ¸ë¦­
    click_through_rate: float
    conversion_rate: float
    coverage: float
    diversity: float
    novelty: float

class PersonalizationSettingsSchema(Schema):
    """ê°œì¸í™” ì„¤ì • ìŠ¤í‚¤ë§ˆ"""
    recommendation_enabled: bool = True
    preferred_categories: List[int] = []
    preferred_brands: List[int] = []
    price_range: Optional[Dict[str, float]] = None
    exclude_categories: List[int] = []
    diversity_preference: float = Field(0.5, ge=0, le=1, description="ë‹¤ì–‘ì„± ì„ í˜¸ë„ (0=ì•ˆì „, 1=ë‹¤ì–‘)")

class ExplainableRecommendationSchema(Schema):
    """ì„¤ëª… ê°€ëŠ¥í•œ ì¶”ì²œ ìŠ¤í‚¤ë§ˆ"""
    product: ProductBasicSchema
    score: float
    explanation: Dict[str, Any] = Field(..., description="ìƒì„¸ ì¶”ì²œ ì´ìœ ")
    
    # ì„¤ëª… êµ¬ì„± ìš”ì†Œ
    primary_reason: str = Field(..., description="ì£¼ìš” ì¶”ì²œ ì´ìœ ")
    secondary_reasons: List[str] = Field([], description="ë¶€ì°¨ì  ì¶”ì²œ ì´ìœ ë“¤")
    confidence_score: float = Field(..., ge=0, le=1, description="ì¶”ì²œ ì‹ ë¢°ë„")
```

---

## ğŸ¤ 3. í˜‘ì—… í•„í„°ë§ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„

ì‚¬ìš©ì í–‰ë™ íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í˜‘ì—… í•„í„°ë§ì„ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤. User-Basedì™€ Item-Based ë‘ ê°€ì§€ ë°©ì‹ì„ ëª¨ë‘ êµ¬í˜„í•©ë‹ˆë‹¤.

### 3.1 ë°ì´í„° ì „ì²˜ë¦¬ ë° ìœ í‹¸ë¦¬í‹°

```python
# apps/recommendations/utils/data_processor.py
import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix
from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances
from sklearn.preprocessing import StandardScaler
from django.core.cache import cache
from django.db.models import Count, Avg, F
from apps.recommendations.models import UserInteraction
from apps.products.models import Product
from typing import Dict, List, Tuple, Optional
import logging

logger = logging.getLogger(__name__)

class InteractionMatrixBuilder:
    """ì‚¬ìš©ì-ì•„ì´í…œ ìƒí˜¸ì‘ìš© ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
    
    def __init__(self):
        self.interaction_weights = {
            'view': 1.0,
            'like': 2.0,
            'cart': 3.0,
            'purchase': 5.0,
            'rating': 4.0,
            'review': 3.0,
            'share': 2.0,
            'bookmark': 2.5,
        }
    
    def build_matrix(self, min_interactions: int = 5, time_decay: bool = True) -> Tuple[csr_matrix, Dict, Dict]:
        """ìƒí˜¸ì‘ìš© ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
        logger.info("Building user-item interaction matrix...")
        
        # ìƒí˜¸ì‘ìš© ë°ì´í„° ì¡°íšŒ
        interactions_qs = UserInteraction.objects.select_related('user', 'product').filter(
            product__is_active=True
        ).values(
            'user_id', 'product_id', 'interaction_type', 'created', 'value'
        )
        
        # íŒë‹¤ìŠ¤ DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(interactions_qs)
        
        if df.empty:
            logger.warning("No interactions found")
            return csr_matrix((0, 0)), {}, {}
        
        # ê°€ì¤‘ì¹˜ ì ìš©
        df['weight'] = df['interaction_type'].map(self.interaction_weights).fillna(1.0)
        
        # í‰ì ì˜ ê²½ìš° ì‹¤ì œ ê°’ ì‚¬ìš©
        rating_mask = df['interaction_type'] == 'rating'
        df.loc[rating_mask, 'weight'] = df.loc[rating_mask, 'value'].fillna(3.0)
        
        # ì‹œê°„ ê°€ì¤‘ì¹˜ ì ìš© (ìµœê·¼ ìƒí˜¸ì‘ìš©ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜)
        if time_decay:
            df['days_ago'] = (pd.Timestamp.now() - pd.to_datetime(df['created'])).dt.days
            df['time_weight'] = np.exp(-df['days_ago'] / 30)  # 30ì¼ ê¸°ì¤€ ì§€ìˆ˜ ê°ì†Œ
            df['weight'] *= df['time_weight']
        
        # ì‚¬ìš©ì/ì•„ì´í…œë³„ ìƒí˜¸ì‘ìš© ìˆ˜ ê³„ì‚°
        user_interactions = df.groupby('user_id').size()
        item_interactions = df.groupby('product_id').size()
        
        # ìµœì†Œ ìƒí˜¸ì‘ìš© ìˆ˜ í•„í„°ë§
        valid_users = user_interactions[user_interactions >= min_interactions].index
        valid_items = item_interactions[item_interactions >= min_interactions].index
        
        df_filtered = df[
            df['user_id'].isin(valid_users) & 
            df['product_id'].isin(valid_items)
        ]
        
        if df_filtered.empty:
            logger.warning("No valid interactions after filtering")
            return csr_matrix((0, 0)), {}, {}
        
        # ì‚¬ìš©ì/ì•„ì´í…œ ì§‘ê³„ (ì¤‘ë³µ ìƒí˜¸ì‘ìš© í•©ê³„)
        interaction_matrix_df = df_filtered.groupby(['user_id', 'product_id'])['weight'].sum().reset_index()
        
        # ì¸ë±ìŠ¤ ë§¤í•‘ ìƒì„±
        unique_users = sorted(interaction_matrix_df['user_id'].unique())
        unique_items = sorted(interaction_matrix_df['product_id'].unique())
        
        user_to_idx = {user_id: idx for idx, user_id in enumerate(unique_users)}
        item_to_idx = {item_id: idx for idx, item_id in enumerate(unique_items)}
        
        # í¬ì†Œ í–‰ë ¬ ìƒì„±
        row_indices = [user_to_idx[user_id] for user_id in interaction_matrix_df['user_id']]
        col_indices = [item_to_idx[item_id] for item_id in interaction_matrix_df['product_id']]
        data = interaction_matrix_df['weight'].values
        
        matrix = csr_matrix(
            (data, (row_indices, col_indices)), 
            shape=(len(unique_users), len(unique_items))
        )
        
        logger.info(f"Built matrix: {matrix.shape[0]} users x {matrix.shape[1]} items, density: {matrix.nnz / (matrix.shape[0] * matrix.shape[1]):.4f}")
        
        return matrix, user_to_idx, item_to_idx
    
    def normalize_matrix(self, matrix: csr_matrix, method: str = 'user') -> csr_matrix:
        """ë§¤íŠ¸ë¦­ìŠ¤ ì •ê·œí™”"""
        if method == 'user':
            # ì‚¬ìš©ìë³„ ì •ê·œí™” (ê° ì‚¬ìš©ìì˜ í‰ê·  ì„ í˜¸ë„ ì œê±°)
            user_means = np.array(matrix.mean(axis=1)).flatten()
            user_means[user_means == 0] = 0  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            
            # í–‰ë ¬ì—ì„œ ì‚¬ìš©ì í‰ê·  ì°¨ê°
            normalized = matrix.copy().astype(float)
            for i in range(matrix.shape[0]):
                if user_means[i] > 0:
                    normalized.data[normalized.indptr[i]:normalized.indptr[i+1]] -= user_means[i]
            
            return normalized
        
        elif method == 'item':
            # ì•„ì´í…œë³„ ì •ê·œí™”
            item_means = np.array(matrix.mean(axis=0)).flatten()
            normalized = matrix.copy().astype(float)
            
            for j in range(matrix.shape[1]):
                if item_means[j] > 0:
                    col_data = normalized.getcol(j).data
                    col_data -= item_means[j]
            
            return normalized
        
        return matrix

class SimilarityCalculator:
    """ìœ ì‚¬ë„ ê³„ì‚° í´ë˜ìŠ¤"""
    
    @staticmethod
    def cosine_similarity_sparse(matrix: csr_matrix, top_k: int = 50) -> csr_matrix:
        """í¬ì†Œ í–‰ë ¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        # ì •ê·œí™”
        matrix_norm = matrix.copy()
        matrix_norm.data = matrix_norm.data / np.sqrt(np.array(matrix_norm.power(2).sum(axis=1)).flatten())
        matrix_norm.data = np.nan_to_num(matrix_norm.data)
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarity = matrix_norm @ matrix_norm.T
        
        # Top-Kë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        if top_k < similarity.shape[0]:
            similarity = SimilarityCalculator._keep_top_k(similarity, top_k)
        
        return similarity
    
    @staticmethod
    def _keep_top_k(similarity_matrix: csr_matrix, k: int) -> csr_matrix:
        """ê° í–‰ì—ì„œ ìƒìœ„ Kê°œ ìœ ì‚¬ë„ë§Œ ìœ ì§€"""
        for i in range(similarity_matrix.shape[0]):
            row_start = similarity_matrix.indptr[i]
            row_end = similarity_matrix.indptr[i + 1]
            
            if row_end - row_start > k:
                # í˜„ì¬ í–‰ì˜ ë°ì´í„°
                row_data = similarity_matrix.data[row_start:row_end]
                row_indices = similarity_matrix.indices[row_start:row_end]
                
                # ìƒìœ„ kê°œ ì¸ë±ìŠ¤ ì°¾ê¸°
                top_k_idx = np.argpartition(row_data, -k)[-k:]
                
                # ìƒìœ„ kê°œë§Œ ìœ ì§€
                similarity_matrix.data[row_start:row_end] = 0
                similarity_matrix.indices[row_start:row_end] = 0
                
                for idx in top_k_idx:
                    similarity_matrix.data[row_start + idx] = row_data[idx]
                    similarity_matrix.indices[row_start + idx] = row_indices[idx]
        
        similarity_matrix.eliminate_zeros()
        return similarity_matrix
    
    @staticmethod
    def pearson_correlation(matrix: csr_matrix) -> np.ndarray:
        """í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê³„ì‚°"""
        # í¬ì†Œ í–‰ë ¬ì„ ë°€ì§‘ í–‰ë ¬ë¡œ ë³€í™˜ (ì‘ì€ ë°ì´í„°ì…‹ìš©)
        dense_matrix = matrix.toarray()
        
        # ê° ì‚¬ìš©ìì˜ í‰ê·  ê³„ì‚° (0ì´ ì•„ë‹Œ ê°’ë“¤ë§Œ)
        user_means = []
        for i in range(dense_matrix.shape[0]):
            non_zero_mask = dense_matrix[i] != 0
            if np.sum(non_zero_mask) > 0:
                user_means.append(np.mean(dense_matrix[i][non_zero_mask]))
            else:
                user_means.append(0)
        
        user_means = np.array(user_means)
        
        # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        correlation_matrix = np.zeros((matrix.shape[0], matrix.shape[0]))
        
        for i in range(matrix.shape[0]):
            for j in range(i, matrix.shape[0]):
                # ê³µí†µ ì•„ì´í…œ ì°¾ê¸°
                common_items = (dense_matrix[i] != 0) & (dense_matrix[j] != 0)
                
                if np.sum(common_items) > 1:  # ìµœì†Œ 2ê°œ ê³µí†µ ì•„ì´í…œ í•„ìš”
                    user_i_ratings = dense_matrix[i][common_items] - user_means[i]
                    user_j_ratings = dense_matrix[j][common_items] - user_means[j]
                    
                    numerator = np.sum(user_i_ratings * user_j_ratings)
                    denominator = np.sqrt(np.sum(user_i_ratings**2) * np.sum(user_j_ratings**2))
                    
                    if denominator > 0:
                        correlation = numerator / denominator
                        correlation_matrix[i, j] = correlation_matrix[j, i] = correlation
        
        return correlation_matrix
```

### 3.2 User-Based í˜‘ì—… í•„í„°ë§

```python
# apps/recommendations/algorithms/collaborative.py
import numpy as np
from scipy.sparse import csr_matrix
from django.core.cache import cache
from django.contrib.auth.models import User
from apps.products.models import Product
from apps.recommendations.utils.data_processor import InteractionMatrixBuilder, SimilarityCalculator
from apps.recommendations.models import UserSimilarity, RecommendationAlgorithm
from typing import List, Dict, Tuple, Optional
import logging

logger = logging.getLogger(__name__)

class UserBasedCollaborativeFiltering:
    """ì‚¬ìš©ì ê¸°ë°˜ í˜‘ì—… í•„í„°ë§"""
    
    def __init__(self, min_interactions: int = 5, similarity_threshold: float = 0.1, n_neighbors: int = 50):
        self.min_interactions = min_interactions
        self.similarity_threshold = similarity_threshold
        self.n_neighbors = n_neighbors
        self.matrix_builder = InteractionMatrixBuilder()
        
        # ìºì‹œ í‚¤
        self.matrix_cache_key = "cf_user_matrix"
        self.similarity_cache_key = "cf_user_similarity"
    
    def train(self, force_rebuild: bool = False) -> bool:
        """ëª¨ë¸ í›ˆë ¨ (ì‚¬ìš©ì ìœ ì‚¬ë„ ê³„ì‚°)"""
        logger.info("Starting User-Based CF training...")
        
        # ìºì‹œ í™•ì¸
        if not force_rebuild and cache.get(self.matrix_cache_key):
            logger.info("Using cached interaction matrix")
            return True
        
        try:
            # ìƒí˜¸ì‘ìš© ë§¤íŠ¸ë¦­ìŠ¤ êµ¬ì„±
            matrix, user_to_idx, item_to_idx = self.matrix_builder.build_matrix(
                min_interactions=self.min_interactions
            )
            
            if matrix.shape[0] == 0:
                logger.error("Empty interaction matrix")
                return False
            
            # ë§¤íŠ¸ë¦­ìŠ¤ ì •ê·œí™”
            normalized_matrix = self.matrix_builder.normalize_matrix(matrix, method='user')
            
            # ì‚¬ìš©ì ê°„ ìœ ì‚¬ë„ ê³„ì‚°
            user_similarity = SimilarityCalculator.cosine_similarity_sparse(
                normalized_matrix, top_k=self.n_neighbors
            )
            
            # ìºì‹œì— ì €ì¥
            cache_data = {
                'matrix': matrix,
                'normalized_matrix': normalized_matrix,
                'user_similarity': user_similarity,
                'user_to_idx': user_to_idx,
                'item_to_idx': item_to_idx
            }
            
            cache.set(self.matrix_cache_key, cache_data, timeout=3600*24)  # 24ì‹œê°„
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ìœ ì‚¬ë„ ì €ì¥ (ìƒìœ„ ìœ ì‚¬ ì‚¬ìš©ìë“¤ë§Œ)
            self._save_similarities_to_db(user_similarity, user_to_idx)
            
            logger.info(f"User-Based CF training completed. Matrix shape: {matrix.shape}")
            return True
            
        except Exception as e:
            logger.error(f"Error in User-Based CF training: {str(e)}")
            return False
    
    def _save_similarities_to_db(self, similarity_matrix: csr_matrix, user_to_idx: Dict[int, int]):
        """ìœ ì‚¬ë„ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        idx_to_user = {idx: user_id for user_id, idx in user_to_idx.items()}
        
        # ê¸°ì¡´ ìœ ì‚¬ë„ ì‚­ì œ
        UserSimilarity.objects.filter(algorithm='user_based_cf').delete()
        
        similarities_to_create = []
        
        for i in range(similarity_matrix.shape[0]):
            user_id = idx_to_user[i]
            
            # í˜„ì¬ ì‚¬ìš©ìì™€ ìœ ì‚¬í•œ ì‚¬ìš©ìë“¤ ì°¾ê¸°
            row_start = similarity_matrix.indptr[i]
            row_end = similarity_matrix.indptr[i + 1]
            
            similar_indices = similarity_matrix.indices[row_start:row_end]
            similarity_scores = similarity_matrix.data[row_start:row_end]
            
            for j, score in zip(similar_indices, similarity_scores):
                if i != j and score > self.similarity_threshold:
                    similar_user_id = idx_to_user[j]
                    
                    similarities_to_create.append(
                        UserSimilarity(
                            user1_id=user_id,
                            user2_id=similar_user_id,
                            similarity_score=float(score),
                            algorithm='user_based_cf'
                        )
                    )
        
        # ë²Œí¬ ìƒì„± (ë°°ì¹˜ í¬ê¸°ë¡œ ë¶„í• )
        batch_size = 1000
        for i in range(0, len(similarities_to_create), batch_size):
            batch = similarities_to_create[i:i + batch_size]
            UserSimilarity.objects.bulk_create(batch, ignore_conflicts=True)
        
        logger.info(f"Saved {len(similarities_to_create)} user similarities to database")
    
    def recommend(self, user_id: int, num_recommendations: int = 10, exclude_purchased: bool = True) -> List[Dict]:
        """ì‚¬ìš©ìì—ê²Œ ì¶”ì²œ"""
        cache_data = cache.get(self.matrix_cache_key)
        if not cache_data:
            logger.warning("No cached matrix found, training required")
            if not self.train():
                return []
            cache_data = cache.get(self.matrix_cache_key)
        
        matrix = cache_data['matrix']
        user_similarity = cache_data['user_similarity']
        user_to_idx = cache_data['user_to_idx']
        item_to_idx = cache_data['item_to_idx']
        
        # ì‚¬ìš©ì ì¸ë±ìŠ¤ í™•ì¸
        if user_id not in user_to_idx:
            logger.warning(f"User {user_id} not found in training data")
            return self._get_fallback_recommendations(user_id, num_recommendations)
        
        user_idx = user_to_idx[user_id]
        
        # ì‚¬ìš©ìì˜ ê¸°ì¡´ ìƒí˜¸ì‘ìš© ê°€ì ¸ì˜¤ê¸°
        user_interactions = matrix[user_idx].toarray().flatten()
        
        # ìœ ì‚¬í•œ ì‚¬ìš©ìë“¤ ì°¾ê¸°
        user_similarities = user_similarity[user_idx].toarray().flatten()
        similar_users = np.argsort(user_similarities)[::-1]
        
        # ì¶”ì²œ ì ìˆ˜ ê³„ì‚°
        recommendation_scores = np.zeros(matrix.shape[1])
        total_similarity = 0
        
        for similar_user_idx in similar_users[:self.n_neighbors]:
            if user_similarities[similar_user_idx] <= 0:
                break
            
            similar_user_interactions = matrix[similar_user_idx].toarray().flatten()
            similarity_score = user_similarities[similar_user_idx]
            
            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì ìˆ˜ ê³„ì‚°
            recommendation_scores += similarity_score * similar_user_interactions
            total_similarity += similarity_score
        
        if total_similarity > 0:
            recommendation_scores /= total_similarity
        
        # ì´ë¯¸ ìƒí˜¸ì‘ìš©í•œ ì•„ì´í…œ ì œì™¸
        recommendation_scores[user_interactions > 0] = 0
        
        # ìƒìœ„ ì¶”ì²œ ì•„ì´í…œ ì„ íƒ
        top_items = np.argsort(recommendation_scores)[::-1][:num_recommendations * 2]
        
        # ì‹¤ì œ ì¶”ì²œ ê²°ê³¼ ìƒì„±
        idx_to_item = {idx: item_id for item_id, idx in item_to_idx.items()}
        recommendations = []
        
        for item_idx in top_items:
            if len(recommendations) >= num_recommendations:
                break
            
            score = recommendation_scores[item_idx]
            if score <= 0:
                continue
            
            item_id = idx_to_item[item_idx]
            
            # ìƒí’ˆ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            try:
                product = Product.objects.get(id=item_id, is_active=True)
                recommendations.append({
                    'product_id': item_id,
                    'product': product,
                    'score': float(score),
                    'algorithm': 'user_based_cf',
                    'explanation': self._generate_explanation(user_id, item_id, user_similarities, similar_users)
                })
            except Product.DoesNotExist:
                continue
        
        return recommendations
    
    def _generate_explanation(self, user_id: int, item_id: int, user_similarities: np.ndarray, similar_users: np.ndarray) -> Dict:
        """ì¶”ì²œ ì´ìœ  ìƒì„±"""
        # ì´ ì•„ì´í…œì„ ì¢‹ì•„í•œ ìœ ì‚¬ ì‚¬ìš©ìë“¤ ì°¾ê¸°
        cache_data = cache.get(self.matrix_cache_key)
        matrix = cache_data['matrix']
        user_to_idx = cache_data['user_to_idx']
        item_to_idx = cache_data['item_to_idx']
        
        item_idx = item_to_idx.get(item_id, -1)
        if item_idx == -1:
            return {'reason': 'Similar users liked this item'}
        
        # ì´ ì•„ì´í…œì— ìƒí˜¸ì‘ìš©í•œ ìœ ì‚¬ ì‚¬ìš©ìë“¤
        similar_users_who_liked = []
        
        for similar_user_idx in similar_users[:10]:  # ìƒìœ„ 10ëª…ë§Œ í™•ì¸
            similarity_score = user_similarities[similar_user_idx]
            if similarity_score > 0 and matrix[similar_user_idx, item_idx] > 0:
                similar_users_who_liked.append(similarity_score)
        
        return {
            'reason': 'Similar users liked this item',
            'similar_users_count': len(similar_users_who_liked),
            'avg_similarity': float(np.mean(similar_users_who_liked)) if similar_users_who_liked else 0,
            'confidence': min(len(similar_users_who_liked) / 10.0, 1.0)
        }
    
    def _get_fallback_recommendations(self, user_id: int, num_recommendations: int) -> List[Dict]:
        """í´ë°± ì¶”ì²œ (ì¸ê¸°ë„ ê¸°ë°˜)"""
        popular_products = Product.objects.filter(
            is_active=True
        ).order_by('-purchase_count', '-view_count')[:num_recommendations]
        
        recommendations = []
        for i, product in enumerate(popular_products):
            recommendations.append({
                'product_id': product.id,
                'product': product,
                'score': 1.0 - (i * 0.1),  # ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜
                'algorithm': 'popularity_fallback',
                'explanation': {'reason': 'Popular item (fallback)'}
            })
        
        return recommendations
    
    def get_similar_users(self, user_id: int, num_users: int = 10) -> List[Dict]:
        """ìœ ì‚¬í•œ ì‚¬ìš©ì ì¡°íšŒ"""
        similarities = UserSimilarity.objects.filter(
            user1_id=user_id,
            algorithm='user_based_cf'
        ).order_by('-similarity_score')[:num_users]
        
        similar_users = []
        for sim in similarities:
            similar_users.append({
                'user_id': sim.user2_id,
                'user': sim.user2,
                'similarity_score': sim.similarity_score,
                'common_items': sim.common_items_count
            })
        
        return similar_users
```

### 3.3 Item-Based í˜‘ì—… í•„í„°ë§

```python
# apps/recommendations/algorithms/collaborative.pyì— ì¶”ê°€

class ItemBasedCollaborativeFiltering:
    """ì•„ì´í…œ ê¸°ë°˜ í˜‘ì—… í•„í„°ë§"""
    
    def __init__(self, min_interactions: int = 5, similarity_threshold: float = 0.1, n_neighbors: int = 20):
        self.min_interactions = min_interactions
        self.similarity_threshold = similarity_threshold
        self.n_neighbors = n_neighbors
        self.matrix_builder = InteractionMatrixBuilder()
        
        self.matrix_cache_key = "cf_item_matrix"
    
    def train(self, force_rebuild: bool = False) -> bool:
        """ëª¨ë¸ í›ˆë ¨ (ì•„ì´í…œ ìœ ì‚¬ë„ ê³„ì‚°)"""
        logger.info("Starting Item-Based CF training...")
        
        try:
            # ìƒí˜¸ì‘ìš© ë§¤íŠ¸ë¦­ìŠ¤ êµ¬ì„±
            matrix, user_to_idx, item_to_idx = self.matrix_builder.build_matrix(
                min_interactions=self.min_interactions
            )
            
            if matrix.shape[1] == 0:
                logger.error("Empty item matrix")
                return False
            
            # ì•„ì´í…œ-ì‚¬ìš©ì ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ì „ì¹˜
            item_matrix = matrix.T
            
            # ì•„ì´í…œë³„ ì •ê·œí™”
            normalized_matrix = self.matrix_builder.normalize_matrix(item_matrix, method='user')
            
            # ì•„ì´í…œ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
            item_similarity = SimilarityCalculator.cosine_similarity_sparse(
                normalized_matrix, top_k=self.n_neighbors
            )
            
            # ìºì‹œì— ì €ì¥
            cache_data = {
                'user_matrix': matrix,
                'item_matrix': item_matrix,
                'item_similarity': item_similarity,
                'user_to_idx': user_to_idx,
                'item_to_idx': item_to_idx
            }
            
            cache.set(self.matrix_cache_key, cache_data, timeout=3600*24)
            
            # ì•„ì´í…œ ìœ ì‚¬ë„ DB ì €ì¥
            self._save_item_similarities_to_db(item_similarity, item_to_idx)
            
            logger.info(f"Item-Based CF training completed. Items: {item_matrix.shape[0]}")
            return True
            
        except Exception as e:
            logger.error(f"Error in Item-Based CF training: {str(e)}")
            return False
    
    def recommend(self, user_id: int, num_recommendations: int = 10) -> List[Dict]:
        """ì•„ì´í…œ ê¸°ë°˜ ì¶”ì²œ"""
        cache_data = cache.get(self.matrix_cache_key)
        if not cache_data:
            if not self.train():
                return []
            cache_data = cache.get(self.matrix_cache_key)
        
        user_matrix = cache_data['user_matrix']
        item_similarity = cache_data['item_similarity']
        user_to_idx = cache_data['user_to_idx']
        item_to_idx = cache_data['item_to_idx']
        
        if user_id not in user_to_idx:
            return self._get_fallback_recommendations(user_id, num_recommendations)
        
        user_idx = user_to_idx[user_id]
        user_interactions = user_matrix[user_idx].toarray().flatten()
        
        # ì‚¬ìš©ìê°€ ìƒí˜¸ì‘ìš©í•œ ì•„ì´í…œë“¤
        interacted_items = np.where(user_interactions > 0)[0]
        
        if len(interacted_items) == 0:
            return self._get_fallback_recommendations(user_id, num_recommendations)
        
        # ì¶”ì²œ ì ìˆ˜ ê³„ì‚°
        recommendation_scores = np.zeros(len(item_to_idx))
        
        for item_idx in interacted_items:
            user_rating = user_interactions[item_idx]
            
            # ì´ ì•„ì´í…œê³¼ ìœ ì‚¬í•œ ì•„ì´í…œë“¤ ì°¾ê¸°
            similar_items_row = item_similarity[item_idx]
            similar_indices = similar_items_row.indices
            similarity_scores = similar_items_row.data
            
            for similar_item_idx, similarity_score in zip(similar_indices, similarity_scores):
                if similar_item_idx != item_idx:  # ìê¸° ìì‹  ì œì™¸
                    recommendation_scores[similar_item_idx] += user_rating * similarity_score
        
        # ì´ë¯¸ ìƒí˜¸ì‘ìš©í•œ ì•„ì´í…œ ì œì™¸
        recommendation_scores[interacted_items] = 0
        
        # ìƒìœ„ ì¶”ì²œ ì„ íƒ
        top_items = np.argsort(recommendation_scores)[::-1][:num_recommendations]
        
        # ê²°ê³¼ ìƒì„±
        idx_to_item = {idx: item_id for item_id, idx in item_to_idx.items()}
        recommendations = []
        
        for item_idx in top_items:
            score = recommendation_scores[item_idx]
            if score <= 0:
                continue
            
            item_id = idx_to_item[item_idx]
            
            try:
                product = Product.objects.get(id=item_id, is_active=True)
                recommendations.append({
                    'product_id': item_id,
                    'product': product,
                    'score': float(score),
                    'algorithm': 'item_based_cf',
                    'explanation': self._generate_item_explanation(user_id, item_id, interacted_items, item_similarity, item_to_idx)
                })
            except Product.DoesNotExist:
                continue
        
        return recommendations
    
    def get_similar_items(self, item_id: int, num_items: int = 10) -> List[Dict]:
        """ìœ ì‚¬ ì•„ì´í…œ ì¡°íšŒ"""
        from apps.recommendations.models import ItemSimilarity
        
        similarities = ItemSimilarity.objects.filter(
            item1_id=item_id,
            algorithm='item_based_cf'
        ).select_related('item2').order_by('-similarity_score')[:num_items]
        
        similar_items = []
        for sim in similarities:
            similar_items.append({
                'product_id': sim.item2_id,
                'product': sim.item2,
                'similarity_score': sim.similarity_score,
                'explanation': {
                    'category_similarity': sim.category_similarity,
                    'brand_similarity': sim.brand_similarity,
                    'feature_similarity': sim.feature_similarity
                }
            })
        
        return similar_items
    
    def _save_item_similarities_to_db(self, similarity_matrix: csr_matrix, item_to_idx: Dict[int, int]):
        """ì•„ì´í…œ ìœ ì‚¬ë„ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        from apps.recommendations.models import ItemSimilarity
        
        idx_to_item = {idx: item_id for item_id, idx in item_to_idx.items()}
        
        # ê¸°ì¡´ ìœ ì‚¬ë„ ì‚­ì œ
        ItemSimilarity.objects.filter(algorithm='item_based_cf').delete()
        
        similarities_to_create = []
        
        for i in range(similarity_matrix.shape[0]):
            item_id = idx_to_item[i]
            
            row_start = similarity_matrix.indptr[i]
            row_end = similarity_matrix.indptr[i + 1]
            
            similar_indices = similarity_matrix.indices[row_start:row_end]
            similarity_scores = similarity_matrix.data[row_start:row_end]
            
            for j, score in zip(similar_indices, similarity_scores):
                if i != j and score > self.similarity_threshold:
                    similar_item_id = idx_to_item[j]
                    
                    similarities_to_create.append(
                        ItemSimilarity(
                            item1_id=item_id,
                            item2_id=similar_item_id,
                            similarity_score=float(score),
                            algorithm='item_based_cf'
                        )
                    )
        
        # ë²Œí¬ ìƒì„±
        batch_size = 1000
        for i in range(0, len(similarities_to_create), batch_size):
            batch = similarities_to_create[i:i + batch_size]
            ItemSimilarity.objects.bulk_create(batch, ignore_conflicts=True)
    
    def _generate_item_explanation(self, user_id: int, recommended_item_id: int, 
                                 interacted_items: np.ndarray, item_similarity: csr_matrix, 
                                 item_to_idx: Dict[int, int]) -> Dict:
        """ì•„ì´í…œ ê¸°ë°˜ ì¶”ì²œ ì´ìœ  ìƒì„±"""
        # ì¶”ì²œ ì•„ì´í…œê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ì•„ì´í…œ ì°¾ê¸°
        recommended_idx = item_to_idx.get(recommended_item_id, -1)
        if recommended_idx == -1:
            return {'reason': 'Similar to items you liked'}
        
        similar_interacted_items = []
        
        for interacted_idx in interacted_items:
            # ìœ ì‚¬ë„ í–‰ë ¬ì—ì„œ ìœ ì‚¬ë„ í™•ì¸
            similarity_row = item_similarity[interacted_idx]
            
            # ì¶”ì²œ ì•„ì´í…œê³¼ì˜ ìœ ì‚¬ë„ ì°¾ê¸°
            item_position = np.where(similarity_row.indices == recommended_idx)[0]
            if len(item_position) > 0:
                similarity_score = similarity_row.data[item_position[0]]
                idx_to_item = {idx: item_id for item_id, idx in item_to_idx.items()}
                interacted_item_id = idx_to_item[interacted_idx]
                
                similar_interacted_items.append({
                    'item_id': interacted_item_id,
                    'similarity': similarity_score
                })
        
        # ê°€ì¥ ìœ ì‚¬í•œ ì•„ì´í…œë“¤ ì •ë ¬
        similar_interacted_items.sort(key=lambda x: x['similarity'], reverse=True)
        
        return {
            'reason': 'Similar to items you liked',
            'based_on_items': similar_interacted_items[:3],  # ìƒìœ„ 3ê°œ
            'similarity_count': len(similar_interacted_items)
        }
    
    def _get_fallback_recommendations(self, user_id: int, num_recommendations: int) -> List[Dict]:
        """í´ë°± ì¶”ì²œ"""
        popular_products = Product.objects.filter(
            is_active=True
        ).order_by('-purchase_count')[:num_recommendations]
        
        return [{
            'product_id': product.id,
            'product': product,
            'score': 1.0,
            'algorithm': 'popularity_fallback',
            'explanation': {'reason': 'Popular item'}
        } for product in popular_products]
```

---

## ğŸ“Š 4. ì½˜í…ì¸  ê¸°ë°˜ í•„í„°ë§ êµ¬í˜„

ìƒí’ˆì˜ ì†ì„±ê³¼ íŠ¹ì§•ì„ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ì ì„ í˜¸ë„ì— ë§ëŠ” ìœ ì‚¬í•œ ìƒí’ˆì„ ì¶”ì²œí•˜ëŠ” ì½˜í…ì¸  ê¸°ë°˜ í•„í„°ë§ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

### 4.1 íŠ¹ì§• ì¶”ì¶œ ë° ë²¡í„°í™”

```python
# apps/recommendations/algorithms/content_based.py
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD
from django.core.cache import cache
from django.db.models import Q
from apps.products.models import Product, Category, Brand
from apps.recommendations.models import UserInteraction
from typing import Dict, List, Tuple, Optional, Any
import re
import logging

logger = logging.getLogger(__name__)

class ProductFeatureExtractor:
    """ìƒí’ˆ íŠ¹ì§• ì¶”ì¶œê¸°"""
    
    def __init__(self):
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words='english',
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.8
        )
        self.scaler = StandardScaler()
        self.mlb_tags = MultiLabelBinarizer()
        self.feature_weights = {
            'category': 0.3,
            'brand': 0.2,
            'price': 0.1,
            'description': 0.25,
            'tags': 0.15
        }
    
    def extract_features(self, products_queryset=None) -> Tuple[np.ndarray, Dict[str, Any]]:
        """ìƒí’ˆë“¤ì˜ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ ë²¡í„°í™”"""
        logger.info("Extracting product features...")
        
        if products_queryset is None:
            products_queryset = Product.objects.filter(is_active=True).select_related('category', 'brand')
        
        products = list(products_queryset)
        if not products:
            return np.array([]), {}
        
        # íŠ¹ì§•ë³„ ë°ì´í„° ì¤€ë¹„
        feature_data = {
            'product_ids': [p.id for p in products],
            'categories': [p.category.name if p.category else '' for p in products],
            'brands': [p.brand.name if p.brand else '' for p in products],
            'prices': [float(p.price) for p in products],
            'descriptions': [self._clean_text(p.description) for p in products],
            'names': [self._clean_text(p.name) for p in products],
            'tags': [p.tags for p in products]
        }
        
        # 1. ì¹´í…Œê³ ë¦¬ íŠ¹ì§•
        category_features = self._encode_categories(feature_data['categories'])
        
        # 2. ë¸Œëœë“œ íŠ¹ì§•
        brand_features = self._encode_brands(feature_data['brands'])
        
        # 3. ê°€ê²© íŠ¹ì§•
        price_features = self._encode_prices(feature_data['prices'])
        
        # 4. í…ìŠ¤íŠ¸ íŠ¹ì§• (ì œí’ˆëª… + ì„¤ëª…)
        text_data = [f"{name} {desc}" for name, desc in zip(feature_data['names'], feature_data['descriptions'])]
        text_features = self._encode_text(text_data)
        
        # 5. íƒœê·¸ íŠ¹ì§•
        tag_features = self._encode_tags(feature_data['tags'])
        
        # íŠ¹ì§• ê²°í•©
        all_features = []
        feature_info = {
            'product_ids': feature_data['product_ids'],
            'feature_names': [],
            'feature_ranges': {}
        }
        
        start_idx = 0
        
        # ì¹´í…Œê³ ë¦¬ íŠ¹ì§• ì¶”ê°€
        if category_features.size > 0:
            weighted_category = category_features * self.feature_weights['category']
            all_features.append(weighted_category)
            end_idx = start_idx + category_features.shape[1]
            feature_info['feature_ranges']['category'] = (start_idx, end_idx)
            start_idx = end_idx
        
        # ë¸Œëœë“œ íŠ¹ì§• ì¶”ê°€
        if brand_features.size > 0:
            weighted_brand = brand_features * self.feature_weights['brand']
            all_features.append(weighted_brand)
            end_idx = start_idx + brand_features.shape[1]
            feature_info['feature_ranges']['brand'] = (start_idx, end_idx)
            start_idx = end_idx
        
        # ê°€ê²© íŠ¹ì§• ì¶”ê°€
        if price_features.size > 0:
            weighted_price = price_features * self.feature_weights['price']
            all_features.append(weighted_price)
            end_idx = start_idx + price_features.shape[1]
            feature_info['feature_ranges']['price'] = (start_idx, end_idx)
            start_idx = end_idx
        
        # í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ê°€
        if text_features.size > 0:
            weighted_text = text_features * self.feature_weights['description']
            all_features.append(weighted_text)
            end_idx = start_idx + text_features.shape[1]
            feature_info['feature_ranges']['description'] = (start_idx, end_idx)
            start_idx = end_idx
        
        # íƒœê·¸ íŠ¹ì§• ì¶”ê°€
        if tag_features.size > 0:
            weighted_tags = tag_features * self.feature_weights['tags']
            all_features.append(weighted_tags)
            end_idx = start_idx + tag_features.shape[1]
            feature_info['feature_ranges']['tags'] = (start_idx, end_idx)
            start_idx = end_idx
        
        # ìµœì¢… íŠ¹ì§• ë§¤íŠ¸ë¦­ìŠ¤
        if all_features:
            combined_features = np.hstack(all_features)
        else:
            combined_features = np.array([])
        
        logger.info(f"Feature extraction completed. Shape: {combined_features.shape}")
        return combined_features, feature_info
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        # íŠ¹ìˆ˜ë¬¸ì ì •ê·œí™”
        text = re.sub(r'[^\w\s]', ' ', text)
        # ê³µë°± ì •ê·œí™”
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip().lower()
    
    def _encode_categories(self, categories: List[str]) -> np.ndarray:
        """ì¹´í…Œê³ ë¦¬ ì›í•« ì¸ì½”ë”©"""
        from sklearn.preprocessing import LabelEncoder, OneHotEncoder
        
        if not categories:
            return np.array([])
        
        # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
        categories = [cat if cat else 'unknown' for cat in categories]
        
        le = LabelEncoder()
        encoded_categories = le.fit_transform(categories)
        
        ohe = OneHotEncoder(sparse_output=False)
        onehot_categories = ohe.fit_transform(encoded_categories.reshape(-1, 1))
        
        return onehot_categories
    
    def _encode_brands(self, brands: List[str]) -> np.ndarray:
        """ë¸Œëœë“œ ì›í•« ì¸ì½”ë”©"""
        from sklearn.preprocessing import LabelEncoder, OneHotEncoder
        
        if not brands:
            return np.array([])
        
        brands = [brand if brand else 'unknown' for brand in brands]
        
        le = LabelEncoder()
        encoded_brands = le.fit_transform(brands)
        
        ohe = OneHotEncoder(sparse_output=False)
        onehot_brands = ohe.fit_transform(encoded_brands.reshape(-1, 1))
        
        return onehot_brands
    
    def _encode_prices(self, prices: List[float]) -> np.ndarray:
        """ê°€ê²© íŠ¹ì§• ì¸ì½”ë”©"""
        if not prices:
            return np.array([])
        
        prices_array = np.array(prices).reshape(-1, 1)
        
        # ê°€ê²© êµ¬ê°„ë³„ íŠ¹ì§• ìƒì„±
        price_ranges = [
            (0, 50000),      # ì €ê°€
            (50000, 100000), # ì¤‘ì €ê°€  
            (100000, 200000),# ì¤‘ê°€
            (200000, 500000),# ì¤‘ê³ ê°€
            (500000, float('inf'))  # ê³ ê°€
        ]
        
        price_features = np.zeros((len(prices), len(price_ranges)))
        
        for i, price in enumerate(prices):
            for j, (min_price, max_price) in enumerate(price_ranges):
                if min_price <= price < max_price:
                    price_features[i, j] = 1
                    break
        
        # ì •ê·œí™”ëœ ê°€ê²©ë„ ì¶”ê°€
        normalized_prices = self.scaler.fit_transform(prices_array)
        
        return np.hstack([price_features, normalized_prices])
    
    def _encode_text(self, texts: List[str]) -> np.ndarray:
        """í…ìŠ¤íŠ¸ TF-IDF ì¸ì½”ë”©"""
        if not texts or all(not text for text in texts):
            return np.array([])
        
        # ë¹ˆ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        texts = [text if text else 'no description' for text in texts]
        
        try:
            tfidf_features = self.tfidf_vectorizer.fit_transform(texts)
            
            # ì°¨ì› ì¶•ì†Œ (ì„ íƒì‚¬í•­)
            if tfidf_features.shape[1] > 100:
                svd = TruncatedSVD(n_components=100, random_state=42)
                tfidf_features = svd.fit_transform(tfidf_features)
            else:
                tfidf_features = tfidf_features.toarray()
            
            return tfidf_features
            
        except ValueError as e:
            logger.error(f"Error in text encoding: {str(e)}")
            return np.zeros((len(texts), 1))
    
    def _encode_tags(self, tags_list: List[List[str]]) -> np.ndarray:
        """íƒœê·¸ ë©€í‹°ë ˆì´ë¸” ì¸ì½”ë”©"""
        if not tags_list:
            return np.array([])
        
        # ë¹ˆ íƒœê·¸ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
        tags_list = [tags if tags else [] for tags in tags_list]
        
        try:
            tag_features = self.mlb_tags.fit_transform(tags_list)
            return tag_features
        except ValueError:
            return np.zeros((len(tags_list), 1))

class UserProfileBuilder:
    """ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±ê¸°"""
    
    def __init__(self, feature_extractor: ProductFeatureExtractor):
        self.feature_extractor = feature_extractor
        
        # ìƒí˜¸ì‘ìš© ê°€ì¤‘ì¹˜
        self.interaction_weights = {
            'view': 1.0,
            'like': 3.0,
            'cart': 4.0,
            'purchase': 5.0,
            'rating': 4.0,
            'bookmark': 2.0
        }
    
    def build_user_profile(self, user_id: int, feature_matrix: np.ndarray, 
                          feature_info: Dict[str, Any]) -> Optional[np.ndarray]:
        """ì‚¬ìš©ì ì„ í˜¸ë„ í”„ë¡œí•„ ìƒì„±"""
        # ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ë°ì´í„° ì¡°íšŒ
        interactions = UserInteraction.objects.filter(
            user_id=user_id,
            product__is_active=True
        ).select_related('product').order_by('-created')[:100]  # ìµœê·¼ 100ê°œ
        
        if not interactions:
            return None
        
        # ìƒí˜¸ì‘ìš©í•œ ìƒí’ˆë“¤ì˜ íŠ¹ì§• ê°€ì ¸ì˜¤ê¸°
        product_ids = feature_info['product_ids']
        user_product_indices = []
        weights = []
        
        for interaction in interactions:
            try:
                product_idx = product_ids.index(interaction.product_id)
                user_product_indices.append(product_idx)
                
                # ìƒí˜¸ì‘ìš© ê°€ì¤‘ì¹˜ ê³„ì‚°
                base_weight = self.interaction_weights.get(interaction.interaction_type, 1.0)
                
                # í‰ì ì´ ìˆëŠ” ê²½ìš° ë°˜ì˜
                if interaction.interaction_type == 'rating' and interaction.value:
                    rating_weight = interaction.value / 5.0  # 5ì  ë§Œì ì„ 1ì  ë§Œì ìœ¼ë¡œ ì •ê·œí™”
                    base_weight *= rating_weight
                
                # ì‹œê°„ ê°€ì¤‘ì¹˜ (ìµœê·¼ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜)
                days_ago = (timezone.now() - interaction.created).days
                time_weight = np.exp(-days_ago / 30)  # 30ì¼ ê¸°ì¤€ ì§€ìˆ˜ ê°ì†Œ
                
                final_weight = base_weight * time_weight
                weights.append(final_weight)
                
            except (ValueError, IndexError):
                continue
        
        if not user_product_indices:
            return None
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±
        user_features = feature_matrix[user_product_indices]
        weights_array = np.array(weights).reshape(-1, 1)
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        weighted_features = user_features * weights_array
        user_profile = np.sum(weighted_features, axis=0) / np.sum(weights_array)
        
        return user_profile
    
    def build_user_category_preference(self, user_id: int) -> Dict[str, float]:
        """ì‚¬ìš©ì ì¹´í…Œê³ ë¦¬ ì„ í˜¸ë„ ë¶„ì„"""
        interactions = UserInteraction.objects.filter(
            user_id=user_id,
            product__is_active=True
        ).select_related('product__category').values(
            'product__category__name', 'interaction_type'
        )
        
        category_scores = {}
        
        for interaction in interactions:
            category = interaction['product__category__name'] or 'unknown'
            weight = self.interaction_weights.get(interaction['interaction_type'], 1.0)
            
            if category in category_scores:
                category_scores[category] += weight
            else:
                category_scores[category] = weight
        
        # ì •ê·œí™”
        if category_scores:
            total_score = sum(category_scores.values())
            category_scores = {k: v/total_score for k, v in category_scores.items()}
        
        return category_scores

class ContentBasedRecommender:
    """ì½˜í…ì¸  ê¸°ë°˜ ì¶”ì²œê¸°"""
    
    def __init__(self):
        self.feature_extractor = ProductFeatureExtractor()
        self.user_profile_builder = UserProfileBuilder(self.feature_extractor)
        self.cache_timeout = 3600 * 6  # 6ì‹œê°„
    
    def train(self, force_rebuild: bool = False) -> bool:
        """íŠ¹ì§• ë§¤íŠ¸ë¦­ìŠ¤ êµ¬ì¶•"""
        cache_key = "content_based_features"
        
        if not force_rebuild and cache.get(cache_key):
            logger.info("Using cached feature matrix")
            return True
        
        try:
            logger.info("Building content-based feature matrix...")
            
            # í™œì„± ìƒí’ˆë“¤ì˜ íŠ¹ì§• ì¶”ì¶œ
            products_qs = Product.objects.filter(is_active=True).select_related('category', 'brand')
            feature_matrix, feature_info = self.feature_extractor.extract_features(products_qs)
            
            if feature_matrix.size == 0:
                logger.error("Empty feature matrix")
                return False
            
            # ìƒí’ˆ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
            similarity_matrix = cosine_similarity(feature_matrix)
            
            # ìºì‹œì— ì €ì¥
            cache_data = {
                'feature_matrix': feature_matrix,
                'feature_info': feature_info,
                'similarity_matrix': similarity_matrix
            }
            
            cache.set(cache_key, cache_data, timeout=self.cache_timeout)
            
            logger.info(f"Content-based training completed. Feature matrix shape: {feature_matrix.shape}")
            return True
            
        except Exception as e:
            logger.error(f"Error in content-based training: {str(e)}")
            return False
    
    def recommend(self, user_id: int, num_recommendations: int = 10, 
                 diversity_factor: float = 0.3) -> List[Dict]:
        """ì½˜í…ì¸  ê¸°ë°˜ ì¶”ì²œ"""
        cache_key = "content_based_features"
        cache_data = cache.get(cache_key)
        
        if not cache_data:
            if not self.train():
                return self._get_fallback_recommendations(user_id, num_recommendations)
            cache_data = cache.get(cache_key)
        
        feature_matrix = cache_data['feature_matrix']
        feature_info = cache_data['feature_info']
        similarity_matrix = cache_data['similarity_matrix']
        
        # ì‚¬ìš©ì í”„ë¡œí•„ êµ¬ì¶•
        user_profile = self.user_profile_builder.build_user_profile(
            user_id, feature_matrix, feature_info
        )
        
        if user_profile is None:
            return self._get_fallback_recommendations(user_id, num_recommendations)
        
        # ì‚¬ìš©ì í”„ë¡œí•„ê³¼ ê° ìƒí’ˆ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        product_similarities = cosine_similarity([user_profile], feature_matrix)[0]
        
        # ì‚¬ìš©ìê°€ ì´ë¯¸ ìƒí˜¸ì‘ìš©í•œ ìƒí’ˆë“¤ ì œì™¸
        interacted_products = set(
            UserInteraction.objects.filter(user_id=user_id)
            .values_list('product_id', flat=True)
        )
        
        # ì¶”ì²œ ì ìˆ˜ ê³„ì‚°
        recommendations = []
        product_ids = feature_info['product_ids']
        
        for i, similarity_score in enumerate(product_similarities):
            product_id = product_ids[i]
            
            if product_id in interacted_products:
                continue
            
            try:
                product = Product.objects.get(id=product_id, is_active=True)
                
                # ë‹¤ì–‘ì„± ìš”ì†Œ ì¶”ê°€
                diversified_score = self._apply_diversity(
                    similarity_score, product, recommendations, diversity_factor
                )
                
                recommendations.append({
                    'product_id': product_id,
                    'product': product,
                    'score': float(diversified_score),
                    'similarity_score': float(similarity_score),
                    'algorithm': 'content_based',
                    'explanation': self._generate_explanation(user_id, product, user_profile, feature_matrix[i], feature_info)
                })
                
            except Product.DoesNotExist:
                continue
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ì¶”ì²œ ë°˜í™˜
        recommendations.sort(key=lambda x: x['score'], reverse=True)
        return recommendations[:num_recommendations]
    
    def get_similar_products(self, product_id: int, num_recommendations: int = 10) -> List[Dict]:
        """íŠ¹ì • ìƒí’ˆê³¼ ìœ ì‚¬í•œ ìƒí’ˆë“¤ ì¶”ì²œ"""
        cache_key = "content_based_features"
        cache_data = cache.get(cache_key)
        
        if not cache_data:
            if not self.train():
                return []
            cache_data = cache.get(cache_key)
        
        feature_info = cache_data['feature_info']
        similarity_matrix = cache_data['similarity_matrix']
        
        try:
            product_idx = feature_info['product_ids'].index(product_id)
        except ValueError:
            logger.warning(f"Product {product_id} not found in feature matrix")
            return []
        
        # ìœ ì‚¬ë„ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        similarities = similarity_matrix[product_idx]
        
        # ìê¸° ìì‹  ì œì™¸í•˜ê³  ì •ë ¬
        similar_indices = np.argsort(similarities)[::-1]
        
        recommendations = []
        for idx in similar_indices:
            if len(recommendations) >= num_recommendations:
                break
            
            if idx == product_idx:  # ìê¸° ìì‹  ì œì™¸
                continue
            
            similar_product_id = feature_info['product_ids'][idx]
            similarity_score = similarities[idx]
            
            try:
                product = Product.objects.get(id=similar_product_id, is_active=True)
                recommendations.append({
                    'product_id': similar_product_id,
                    'product': product,
                    'score': float(similarity_score),
                    'algorithm': 'content_based_similar',
                    'explanation': {'reason': 'Similar product features'}
                })
            except Product.DoesNotExist:
                continue
        
        return recommendations
    
    def _apply_diversity(self, base_score: float, product: Product, 
                        existing_recommendations: List[Dict], diversity_factor: float) -> float:
        """ë‹¤ì–‘ì„± ìš”ì†Œë¥¼ ì ìš©í•˜ì—¬ ì ìˆ˜ ì¡°ì •"""
        if not existing_recommendations or diversity_factor == 0:
            return base_score
        
        # ê¸°ì¡´ ì¶”ì²œë“¤ê³¼ì˜ ì¹´í…Œê³ ë¦¬/ë¸Œëœë“œ ì¤‘ë³µë„ ê³„ì‚°
        category_penalty = 0
        brand_penalty = 0
        
        existing_categories = [rec['product'].category_id for rec in existing_recommendations]
        existing_brands = [rec['product'].brand_id for rec in existing_recommendations]
        
        # ì¹´í…Œê³ ë¦¬ ì¤‘ë³µ íŒ¨ë„í‹°
        category_count = existing_categories.count(product.category_id)
        category_penalty = category_count * diversity_factor * 0.2
        
        # ë¸Œëœë“œ ì¤‘ë³µ íŒ¨ë„í‹°
        brand_count = existing_brands.count(product.brand_id)
        brand_penalty = brand_count * diversity_factor * 0.1
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚° (íŒ¨ë„í‹° ì ìš©)
        diversified_score = base_score * (1 - category_penalty - brand_penalty)
        return max(diversified_score, 0)  # ìŒìˆ˜ ë°©ì§€
    
    def _generate_explanation(self, user_id: int, product: Product, 
                            user_profile: np.ndarray, product_features: np.ndarray,
                            feature_info: Dict[str, Any]) -> Dict:
        """ì¶”ì²œ ì´ìœ  ìƒì„±"""
        explanations = []
        
        # ì‚¬ìš©ì ì¹´í…Œê³ ë¦¬ ì„ í˜¸ë„
        category_prefs = self.user_profile_builder.build_user_category_preference(user_id)
        
        if product.category and product.category.name in category_prefs:
            pref_score = category_prefs[product.category.name]
            if pref_score > 0.2:  # 20% ì´ìƒ ì„ í˜¸í•˜ëŠ” ê²½ìš°
                explanations.append(f"You often browse {product.category.name} category")
        
        # íŠ¹ì§•ë³„ ìœ ì‚¬ë„ ë¶„ì„
        feature_ranges = feature_info.get('feature_ranges', {})
        
        for feature_name, (start, end) in feature_ranges.items():
            if feature_name in ['category', 'brand']:
                user_feature_part = user_profile[start:end]
                product_feature_part = product_features[start:end]
                
                similarity = cosine_similarity([user_feature_part], [product_feature_part])[0][0]
                
                if similarity > 0.7:  # ë†’ì€ ìœ ì‚¬ë„
                    if feature_name == 'brand':
                        explanations.append(f"You like {product.brand.name} brand")
                    elif feature_name == 'category':
                        explanations.append(f"Matches your {product.category.name} preferences")
        
        return {
            'primary_reason': explanations[0] if explanations else "Based on your preferences",
            'all_reasons': explanations,
            'match_score': float(cosine_similarity([user_profile], [product_features])[0][0])
        }
    
    def _get_fallback_recommendations(self, user_id: int, num_recommendations: int) -> List[Dict]:
        """í´ë°± ì¶”ì²œ (ì¸ê¸° ìƒí’ˆ)"""
        popular_products = Product.objects.filter(
            is_active=True
        ).order_by('-view_count', '-purchase_count')[:num_recommendations]
        
        return [{
            'product_id': product.id,
            'product': product,
            'score': 1.0,
            'algorithm': 'popularity_fallback',
            'explanation': {'reason': 'Popular product'}
        } for product in popular_products]
```

### 4.2 ê³ ê¸‰ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§

```python
# apps/recommendations/algorithms/advanced_features.py
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sentence_transformers import SentenceTransformer
import torch
from typing import Dict, List, Tuple, Optional
from apps.products.models import Product
import logging

logger = logging.getLogger(__name__)

class AdvancedFeatureExtractor:
    """ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œê¸°"""
    
    def __init__(self):
        self.sentence_transformer = None
        self.price_clusterer = KMeans(n_clusters=5, random_state=42)
        self.popularity_scaler = StandardScaler()
        
    def _load_sentence_transformer(self):
        """Sentence Transformer ëª¨ë¸ ë¡œë“œ (ì§€ì—° ë¡œë”©)"""
        if self.sentence_transformer is None:
            try:
                self.sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')
                logger.info("Sentence Transformer model loaded")
            except Exception as e:
                logger.error(f"Failed to load Sentence Transformer: {str(e)}")
                self.sentence_transformer = None
    
    def extract_semantic_features(self, products: List[Product]) -> Optional[np.ndarray]:
        """ì˜ë¯¸ì  íŠ¹ì§• ì¶”ì¶œ (Sentence Embeddings)"""
        self._load_sentence_transformer()
        
        if self.sentence_transformer is None:
            return None
        
        try:
            # ìƒí’ˆëª…ê³¼ ì„¤ëª…ì„ ê²°í•©í•œ í…ìŠ¤íŠ¸
            texts = []
            for product in products:
                text = f"{product.name} {product.description}"
                texts.append(text[:512])  # í† í° ê¸¸ì´ ì œí•œ
            
            # ì„ë² ë”© ìƒì„±
            embeddings = self.sentence_transformer.encode(
                texts, 
                show_progress_bar=True,
                batch_size=32
            )
            
            logger.info(f"Generated semantic embeddings: {embeddings.shape}")
            return embeddings
            
        except Exception as e:
            logger.error(f"Error in semantic feature extraction: {str(e)}")
            return None
    
    def extract_behavioral_features(self, products: List[Product]) -> np.ndarray:
        """í–‰ë™ ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ"""
        features = []
        
        for product in products:
            # ì¸ê¸°ë„ ì§€í‘œ
            view_count = product.view_count
            purchase_count = product.purchase_count
            rating_count = product.rating_count
            average_rating = product.average_rating
            
            # CTR (Click Through Rate) ì¶”ì •
            ctr = purchase_count / max(view_count, 1)
            
            # ì „í™˜ìœ¨ (Conversion Rate)
            conversion_rate = purchase_count / max(view_count, 1)
            
            # í‰ì  ì‹ ë¢°ë„ (ë§ì€ í‰ì ì„ ë°›ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ë†’ìŒ)
            rating_reliability = min(rating_count / 100, 1.0)
            
            # ìƒí’ˆ ìˆ˜ëª… (ìƒì„±ì¼ë¡œë¶€í„° ê²½ê³¼ ì‹œê°„)
            age_days = (timezone.now() - product.created).days
            age_normalized = min(age_days / 365, 1.0)  # 1ë…„ ê¸°ì¤€ ì •ê·œí™”
            
            features.append([
                view_count,
                purchase_count,
                rating_count,
                average_rating,
                ctr,
                conversion_rate,
                rating_reliability,
                age_normalized
            ])
        
        features_array = np.array(features)
        
        # ì •ê·œí™”
        normalized_features = self.popularity_scaler.fit_transform(features_array)
        
        return normalized_features
    
    def extract_price_clustering_features(self, products: List[Product]) -> np.ndarray:
        """ê°€ê²© í´ëŸ¬ìŠ¤í„°ë§ íŠ¹ì§•"""
        prices = np.array([float(product.price) for product in products]).reshape(-1, 1)
        
        # ê°€ê²©ëŒ€ë³„ í´ëŸ¬ìŠ¤í„°ë§
        price_clusters = self.price_clusterer.fit_predict(prices)
        
        # ì›í•« ì¸ì½”ë”©
        cluster_features = np.zeros((len(products), self.price_clusterer.n_clusters))
        for i, cluster in enumerate(price_clusters):
            cluster_features[i, cluster] = 1
        
        # ê°€ê²© í†µê³„ íŠ¹ì§• ì¶”ê°€
        price_stats = []
        for price in prices.flatten():
            # ê°€ê²© ë¶„ìœ„ìˆ˜
            percentile_25 = np.percentile(prices, 25)
            percentile_50 = np.percentile(prices, 50)
            percentile_75 = np.percentile(prices, 75)
            
            price_features = [
                1 if price <= percentile_25 else 0,  # ì €ê°€
                1 if percentile_25 < price <= percentile_50 else 0,  # ì¤‘ì €ê°€
                1 if percentile_50 < price <= percentile_75 else 0,  # ì¤‘ê³ ê°€
                1 if price > percentile_75 else 0,  # ê³ ê°€
            ]
            price_stats.append(price_features)
        
        price_stats_array = np.array(price_stats)
        
        # í´ëŸ¬ìŠ¤í„° íŠ¹ì§•ê³¼ í†µê³„ íŠ¹ì§• ê²°í•©
        return np.hstack([cluster_features, price_stats_array])
    
    def extract_temporal_features(self, products: List[Product]) -> np.ndarray:
        """ì‹œê°„ì  íŠ¹ì§• ì¶”ì¶œ"""
        features = []
        
        for product in products:
            # ê³„ì ˆì„± íŠ¹ì§• (ìƒí’ˆ ìƒì„± ì›”)
            created_month = product.created.month
            season_features = [0, 0, 0, 0]  # ë´„, ì—¬ë¦„, ê°€ì„, ê²¨ìš¸
            
            if created_month in [3, 4, 5]:
                season_features[0] = 1  # ë´„
            elif created_month in [6, 7, 8]:
                season_features[1] = 1  # ì—¬ë¦„
            elif created_month in [9, 10, 11]:
                season_features[2] = 1  # ê°€ì„
            else:
                season_features[3] = 1  # ê²¨ìš¸
            
            # ìš”ì¼ íŠ¹ì§• (ìƒí’ˆ ìƒì„± ìš”ì¼)
            created_weekday = product.created.weekday()
            weekday_features = [0] * 7
            weekday_features[created_weekday] = 1
            
            # íŠ¸ë Œë“œ íŠ¹ì§•
            # ìµœê·¼ ìƒì„±ëœ ìƒí’ˆì¼ìˆ˜ë¡ ë†’ì€ íŠ¸ë Œë“œ ì ìˆ˜
            days_since_creation = (timezone.now() - product.created).days
            trend_score = max(0, 1 - days_since_creation / 365)  # 1ë…„ ê¸°ì¤€
            
            all_features = season_features + weekday_features + [trend_score]
            features.append(all_features)
        
        return np.array(features)

class MultiModalFeatureFusion:
    """ë‹¤ì¤‘ ëª¨ë‹¬ íŠ¹ì§• ìœµí•©"""
    
    def __init__(self, fusion_method: str = 'concatenation'):
        self.fusion_method = fusion_method
        self.feature_weights = {
            'semantic': 0.3,
            'behavioral': 0.25,
            'price': 0.2,
            'temporal': 0.15,
            'categorical': 0.1
        }
    
    def fuse_features(self, feature_dict: Dict[str, np.ndarray]) -> np.ndarray:
        """ì—¬ëŸ¬ íŠ¹ì§•ë“¤ì„ ìœµí•©"""
        if self.fusion_method == 'concatenation':
            return self._concatenate_features(feature_dict)
        elif self.fusion_method == 'weighted_sum':
            return self._weighted_sum_features(feature_dict)
        elif self.fusion_method == 'attention':
            return self._attention_fusion(feature_dict)
        else:
            raise ValueError(f"Unknown fusion method: {self.fusion_method}")
    
    def _concatenate_features(self, feature_dict: Dict[str, np.ndarray]) -> np.ndarray:
        """ë‹¨ìˆœ ì—°ê²°"""
        features_list = []
        
        for feature_name, features in feature_dict.items():
            if features is not None and features.size > 0:
                # ê°€ì¤‘ì¹˜ ì ìš©
                weight = self.feature_weights.get(feature_name, 1.0)
                weighted_features = features * weight
                features_list.append(weighted_features)
        
        if features_list:
            return np.hstack(features_list)
        else:
            return np.array([])
    
    def _weighted_sum_features(self, feature_dict: Dict[str, np.ndarray]) -> np.ndarray:
        """ê°€ì¤‘ í•©"""
        # ëª¨ë“  íŠ¹ì§•ì˜ ì°¨ì›ì„ ë§ì¶˜ í›„ ê°€ì¤‘ í‰ê· 
        target_dim = max(f.shape[1] for f in feature_dict.values() if f is not None)
        
        weighted_sum = None
        total_weight = 0
        
        for feature_name, features in feature_dict.items():
            if features is None:
                continue
            
            weight = self.feature_weights.get(feature_name, 1.0)
            
            # ì°¨ì› ë§ì¶”ê¸° (íŒ¨ë”© ë˜ëŠ” ì°¨ì› ì¶•ì†Œ)
            if features.shape[1] < target_dim:
                padded_features = np.pad(
                    features, 
                    ((0, 0), (0, target_dim - features.shape[1])), 
                    'constant'
                )
            elif features.shape[1] > target_dim:
                padded_features = features[:, :target_dim]
            else:
                padded_features = features
            
            if weighted_sum is None:
                weighted_sum = weight * padded_features
            else:
                weighted_sum += weight * padded_features
            
            total_weight += weight
        
        if weighted_sum is not None and total_weight > 0:
            return weighted_sum / total_weight
        else:
            return np.array([])
    
    def _attention_fusion(self, feature_dict: Dict[str, np.ndarray]) -> np.ndarray:
        """ì–´í…ì…˜ ê¸°ë°˜ ìœµí•© (ê°„ë‹¨í•œ ë²„ì „)"""
        # ê° íŠ¹ì§•ì˜ ì¤‘ìš”ë„ë¥¼ ë™ì ìœ¼ë¡œ ê³„ì‚°
        features_list = []
        attention_weights = []
        
        for feature_name, features in feature_dict.items():
            if features is None:
                continue
            
            # íŠ¹ì§•ì˜ ë¶„ì‚°ì„ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš”ë„ ê³„ì‚°
            feature_variance = np.var(features, axis=0).mean()
            attention_weight = np.exp(feature_variance) / (1 + np.exp(feature_variance))
            
            features_list.append(features)
            attention_weights.append(attention_weight)
        
        if not features_list:
            return np.array([])
        
        # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì •ê·œí™”
        attention_weights = np.array(attention_weights)
        attention_weights = attention_weights / attention_weights.sum()
        
        # ê°€ì¤‘ ì—°ê²°
        weighted_features = []
        for features, weight in zip(features_list, attention_weights):
            weighted_features.append(features * weight)
        
        return np.hstack(weighted_features)
```

---

## ğŸ”€ 5. í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬í˜„

ì—¬ëŸ¬ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ì„ ì¡°í•©í•˜ì—¬ ê°ê°ì˜ ì¥ì ì„ ì‚´ë¦¬ê³  ë‹¨ì ì„ ë³´ì™„í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.

### 5.1 í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ ë§¤ë‹ˆì €

```python
# apps/recommendations/algorithms/hybrid.py
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from django.core.cache import cache
from django.contrib.auth.models import User
from apps.recommendations.algorithms.collaborative import (
    UserBasedCollaborativeFiltering, 
    ItemBasedCollaborativeFiltering
)
from apps.recommendations.algorithms.content_based import ContentBasedRecommender
from apps.recommendations.models import UserInteraction, RecommendationRequest
from apps.products.models import Product
import logging

logger = logging.getLogger(__name__)

class HybridRecommendationStrategy:
    """í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì „ëµ ì¸í„°í˜ì´ìŠ¤"""
    
    def combine_recommendations(self, recommendations_dict: Dict[str, List[Dict]], 
                             user_id: int, num_recommendations: int) -> List[Dict]:
        raise NotImplementedError

class WeightedHybridStrategy(HybridRecommendationStrategy):
    """ê°€ì¤‘ì¹˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ"""
    
    def __init__(self, algorithm_weights: Dict[str, float] = None):
        self.algorithm_weights = algorithm_weights or {
            'collaborative_user': 0.4,
            'collaborative_item': 0.3,
            'content_based': 0.3
        }
    
    def combine_recommendations(self, recommendations_dict: Dict[str, List[Dict]], 
                             user_id: int, num_recommendations: int) -> List[Dict]:
        """ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì¶”ì²œ ì¡°í•©"""
        
        # ëª¨ë“  ì¶”ì²œëœ ìƒí’ˆë“¤ ìˆ˜ì§‘
        all_products = {}
        
        for algorithm, recommendations in recommendations_dict.items():
            weight = self.algorithm_weights.get(algorithm, 0.1)
            
            for rec in recommendations:
                product_id = rec['product_id']
                
                if product_id not in all_products:
                    all_products[product_id] = {
                        'product_id': product_id,
                        'product': rec['product'],
                        'total_score': 0,
                        'algorithm_scores': {},
                        'explanations': []
                    }
                
                # ê°€ì¤‘ì¹˜ ì ìš©í•œ ì ìˆ˜ í•©ì‚°
                weighted_score = rec['score'] * weight
                all_products[product_id]['total_score'] += weighted_score
                all_products[product_id]['algorithm_scores'][algorithm] = rec['score']
                
                # ì„¤ëª… ìˆ˜ì§‘
                if 'explanation' in rec:
                    all_products[product_id]['explanations'].append({
                        'algorithm': algorithm,
                        'explanation': rec['explanation']
                    })
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_products = sorted(
            all_products.values(), 
            key=lambda x: x['total_score'], 
            reverse=True
        )
        
        # ìµœì¢… ì¶”ì²œ ê²°ê³¼ ìƒì„±
        final_recommendations = []
        for i, product_data in enumerate(sorted_products[:num_recommendations]):
            final_recommendations.append({
                'product_id': product_data['product_id'],
                'product': product_data['product'],
                'score': product_data['total_score'],
                'rank': i + 1,
                'algorithm': 'weighted_hybrid',
                'algorithm_scores': product_data['algorithm_scores'],
                'explanation': self._generate_hybrid_explanation(product_data)
            })
        
        return final_recommendations
    
    def _generate_hybrid_explanation(self, product_data: Dict) -> Dict:
        """í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì„¤ëª… ìƒì„±"""
        explanations = product_data['explanations']
        algorithm_scores = product_data['algorithm_scores']
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ì¤€ ì•Œê³ ë¦¬ì¦˜ ì°¾ê¸°
        best_algorithm = max(algorithm_scores, key=algorithm_scores.get)
        best_score = algorithm_scores[best_algorithm]
        
        # ì£¼ìš” ì¶”ì²œ ì´ìœ  ê²°ì •
        primary_reasons = []
        for exp_data in explanations:
            if exp_data['algorithm'] == best_algorithm:
                exp = exp_data['explanation']
                if isinstance(exp, dict):
                    primary_reasons.append(exp.get('primary_reason', 'Recommended by multiple algorithms'))
                else:
                    primary_reasons.append('Recommended by multiple algorithms')
        
        return {
            'primary_reason': primary_reasons[0] if primary_reasons else 'Recommended by multiple algorithms',
            'contributing_algorithms': list(algorithm_scores.keys()),
            'best_algorithm': best_algorithm,
            'confidence_score': min(best_score, 1.0),
            'consensus_score': len(algorithm_scores) / 3.0  # 3ê°œ ì•Œê³ ë¦¬ì¦˜ ê¸°ì¤€
        }

class SwitchingHybridStrategy(HybridRecommendationStrategy):
    """ìŠ¤ìœ„ì¹­ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ (ìƒí™©ì— ë”°ë¼ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ)"""
    
    def __init__(self):
        self.min_interactions_for_cf = 10
        self.min_products_for_content = 50
    
    def combine_recommendations(self, recommendations_dict: Dict[str, List[Dict]], 
                             user_id: int, num_recommendations: int) -> List[Dict]:
        """ìƒí™©ì— ë”°ë¼ ìµœì  ì•Œê³ ë¦¬ì¦˜ ì„ íƒ"""
        
        # ì‚¬ìš©ì ìƒí™© ë¶„ì„
        user_context = self._analyze_user_context(user_id)
        
        # ì•Œê³ ë¦¬ì¦˜ ì„ íƒ ë¡œì§
        selected_algorithm = self._select_best_algorithm(user_context, recommendations_dict)
        
        logger.info(f"Switching strategy selected: {selected_algorithm} for user {user_id}")
        
        # ì„ íƒëœ ì•Œê³ ë¦¬ì¦˜ì˜ ì¶”ì²œ ë°˜í™˜
        if selected_algorithm in recommendations_dict:
            recommendations = recommendations_dict[selected_algorithm][:num_recommendations]
            
            # ì•Œê³ ë¦¬ì¦˜ ì •ë³´ ì¶”ê°€
            for rec in recommendations:
                rec['algorithm'] = f'switching_{selected_algorithm}'
                if 'explanation' not in rec:
                    rec['explanation'] = {}
                rec['explanation']['selection_reason'] = f'Best algorithm for your profile: {selected_algorithm}'
            
            return recommendations
        
        # í´ë°±: ê°€ì¤‘ì¹˜ ì „ëµ ì‚¬ìš©
        weighted_strategy = WeightedHybridStrategy()
        return weighted_strategy.combine_recommendations(recommendations_dict, user_id, num_recommendations)
    
    def _analyze_user_context(self, user_id: int) -> Dict:
        """ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
        # ì‚¬ìš©ì ìƒí˜¸ì‘ìš© í†µê³„
        interactions_count = UserInteraction.objects.filter(user_id=user_id).count()
        
        # ìƒí˜¸ì‘ìš© ë‹¤ì–‘ì„± (ì„œë¡œ ë‹¤ë¥¸ ìƒí’ˆ ìˆ˜)
        unique_products = UserInteraction.objects.filter(
            user_id=user_id
        ).values('product_id').distinct().count()
        
        # ìµœê·¼ í™œë™ (7ì¼ ì´ë‚´)
        from django.utils import timezone
        from datetime import timedelta
        
        recent_interactions = UserInteraction.objects.filter(
            user_id=user_id,
            created__gte=timezone.now() - timedelta(days=7)
        ).count()
        
        # ì‚¬ìš©ì í”„ë¡œí•„ ì™„ì„±ë„
        try:
            user = User.objects.get(id=user_id)
            profile_completeness = 0
            if hasattr(user, 'profile'):
                profile = user.profile
                if profile.preferred_categories.exists():
                    profile_completeness += 0.3
                if profile.preferred_brands.exists():
                    profile_completeness += 0.3
                if profile.preferred_price_range:
                    profile_completeness += 0.2
                if profile.age_range:
                    profile_completeness += 0.2
        except:
            profile_completeness = 0
        
        return {
            'interactions_count': interactions_count,
            'unique_products': unique_products,
            'recent_activity': recent_interactions,
            'profile_completeness': profile_completeness,
            'diversity_ratio': unique_products / max(interactions_count, 1)
        }
    
    def _select_best_algorithm(self, user_context: Dict, recommendations_dict: Dict) -> str:
        """ìµœì  ì•Œê³ ë¦¬ì¦˜ ì„ íƒ"""
        interactions_count = user_context['interactions_count']
        profile_completeness = user_context['profile_completeness']
        recent_activity = user_context['recent_activity']
        
        # ì‹ ê·œ ì‚¬ìš©ì (ìƒí˜¸ì‘ìš© ë¶€ì¡±)
        if interactions_count < self.min_interactions_for_cf:
            if profile_completeness > 0.5:
                return 'content_based'  # í”„ë¡œí•„ ì •ë³´ê°€ ìˆìœ¼ë©´ ì½˜í…ì¸  ê¸°ë°˜
            else:
                return 'popularity'  # ì¸ê¸°ë„ ê¸°ë°˜ í´ë°±
        
        # í™œë°œí•œ ì‚¬ìš©ì
        elif interactions_count >= self.min_interactions_for_cf:
            # ìµœê·¼ í™œë™ì´ ë§ìœ¼ë©´ í˜‘ì—… í•„í„°ë§
            if recent_activity >= 3:
                return 'collaborative_user'
            else:
                return 'collaborative_item'
        
        # ì¤‘ê°„ ë‹¨ê³„ ì‚¬ìš©ì
        else:
            return 'content_based'

class CascadingHybridStrategy(HybridRecommendationStrategy):
    """ìºìŠ¤ì¼€ì´ë”© í•˜ì´ë¸Œë¦¬ë“œ (ìˆœì°¨ì  í•„í„°ë§)"""
    
    def __init__(self, cascade_order: List[str] = None, min_recommendations_per_stage: int = 5):
        self.cascade_order = cascade_order or [
            'collaborative_user',
            'collaborative_item', 
            'content_based'
        ]
        self.min_recommendations_per_stage = min_recommendations_per_stage
    
    def combine_recommendations(self, recommendations_dict: Dict[str, List[Dict]], 
                             user_id: int, num_recommendations: int) -> List[Dict]:
        """ìˆœì°¨ì ìœ¼ë¡œ ì¶”ì²œ ìƒì„±"""
        
        final_recommendations = []
        used_product_ids = set()
        
        for algorithm in self.cascade_order:
            if algorithm not in recommendations_dict:
                continue
            
            # í˜„ì¬ ë‹¨ê³„ì—ì„œ í•„ìš”í•œ ì¶”ì²œ ìˆ˜
            needed = num_recommendations - len(final_recommendations)
            if needed <= 0:
                break
            
            # ì´ë¯¸ ì„ íƒëœ ìƒí’ˆ ì œì™¸í•˜ê³  ì¶”ì²œ ì¶”ê°€
            algorithm_recommendations = recommendations_dict[algorithm]
            
            stage_count = 0
            for rec in algorithm_recommendations:
                if stage_count >= max(needed, self.min_recommendations_per_stage):
                    break
                
                product_id = rec['product_id']
                if product_id not in used_product_ids:
                    # ìºìŠ¤ì¼€ì´ë”© ì •ë³´ ì¶”ê°€
                    rec_copy = rec.copy()
                    rec_copy['algorithm'] = f'cascading_{algorithm}'
                    rec_copy['cascade_stage'] = len(final_recommendations) // self.min_recommendations_per_stage + 1
                    
                    final_recommendations.append(rec_copy)
                    used_product_ids.add(product_id)
                    stage_count += 1
        
        # ìˆœìœ„ ì¬ì •ë ¬
        for i, rec in enumerate(final_recommendations):
            rec['rank'] = i + 1
        
        return final_recommendations[:num_recommendations]

class MixedHybridStrategy(HybridRecommendationStrategy):
    """ë¯¹ìŠ¤ë“œ í•˜ì´ë¸Œë¦¬ë“œ (ì•Œê³ ë¦¬ì¦˜ë³„ë¡œ ì¼ì • ë¹„ìœ¨ ë°°ë¶„)"""
    
    def __init__(self, algorithm_ratios: Dict[str, float] = None):
        self.algorithm_ratios = algorithm_ratios or {
            'collaborative_user': 0.4,
            'collaborative_item': 0.3,
            'content_based': 0.3
        }
    
    def combine_recommendations(self, recommendations_dict: Dict[str, List[Dict]], 
                             user_id: int, num_recommendations: int) -> List[Dict]:
        """ê° ì•Œê³ ë¦¬ì¦˜ì—ì„œ ì¼ì • ë¹„ìœ¨ë¡œ ì¶”ì²œ ì„ íƒ"""
        
        final_recommendations = []
        used_product_ids = set()
        
        # ê° ì•Œê³ ë¦¬ì¦˜ë³„ í• ë‹¹ ê°œìˆ˜ ê³„ì‚°
        algorithm_allocations = {}
        remaining_slots = num_recommendations
        
        for algorithm, ratio in self.algorithm_ratios.items():
            if algorithm in recommendations_dict:
                allocated = int(num_recommendations * ratio)
                algorithm_allocations[algorithm] = allocated
                remaining_slots -= allocated
        
        # ë‚¨ì€ ìŠ¬ë¡¯ì„ ì²« ë²ˆì§¸ ì•Œê³ ë¦¬ì¦˜ì— í• ë‹¹
        if remaining_slots > 0 and algorithm_allocations:
            first_algorithm = next(iter(algorithm_allocations.keys()))
            algorithm_allocations[first_algorithm] += remaining_slots
        
        # ê° ì•Œê³ ë¦¬ì¦˜ì—ì„œ í• ë‹¹ëœ ìˆ˜ë§Œí¼ ì¶”ì²œ ì„ íƒ
        for algorithm, allocated_count in algorithm_allocations.items():
            if algorithm not in recommendations_dict:
                continue
            
            algorithm_recommendations = recommendations_dict[algorithm]
            added_count = 0
            
            for rec in algorithm_recommendations:
                if added_count >= allocated_count:
                    break
                
                product_id = rec['product_id']
                if product_id not in used_product_ids:
                    rec_copy = rec.copy()
                    rec_copy['algorithm'] = f'mixed_{algorithm}'
                    rec_copy['allocation_source'] = algorithm
                    
                    final_recommendations.append(rec_copy)
                    used_product_ids.add(product_id)
                    added_count += 1
        
        # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì„ íƒì‚¬í•­)
        final_recommendations.sort(key=lambda x: x.get('score', 0), reverse=True)
        
        # ìˆœìœ„ ì¬ì •ë ¬
        for i, rec in enumerate(final_recommendations):
            rec['rank'] = i + 1
        
        return final_recommendations

class HybridRecommendationManager:
    """í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.collaborative_user = UserBasedCollaborativeFiltering()
        self.collaborative_item = ItemBasedCollaborativeFiltering()
        self.content_based = ContentBasedRecommender()
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì „ëµë“¤
        self.strategies = {
            'weighted': WeightedHybridStrategy(),
            'switching': SwitchingHybridStrategy(),
            'cascading': CascadingHybridStrategy(),
            'mixed': MixedHybridStrategy()
        }
        
        self.default_strategy = 'weighted'
    
    def recommend(self, user_id: int, num_recommendations: int = 10, 
                 strategy: str = None, context: Dict = None) -> List[Dict]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹¤í–‰"""
        
        if strategy is None:
            strategy = self._select_optimal_strategy(user_id, context)
        
        logger.info(f"Using hybrid strategy: {strategy} for user {user_id}")
        
        try:
            # ê° ì•Œê³ ë¦¬ì¦˜ë³„ ì¶”ì²œ ìƒì„±
            recommendations_dict = {}
            
            # User-Based Collaborative Filtering
            try:
                cf_user_recs = self.collaborative_user.recommend(
                    user_id, num_recommendations * 2  # ë” ë§ì´ ìƒì„± í›„ í•„í„°ë§
                )
                if cf_user_recs:
                    recommendations_dict['collaborative_user'] = cf_user_recs
            except Exception as e:
                logger.error(f"User-based CF failed: {str(e)}")
            
            # Item-Based Collaborative Filtering  
            try:
                cf_item_recs = self.collaborative_item.recommend(
                    user_id, num_recommendations * 2
                )
                if cf_item_recs:
                    recommendations_dict['collaborative_item'] = cf_item_recs
            except Exception as e:
                logger.error(f"Item-based CF failed: {str(e)}")
            
            # Content-Based Filtering
            try:
                content_recs = self.content_based.recommend(
                    user_id, num_recommendations * 2
                )
                if content_recs:
                    recommendations_dict['content_based'] = content_recs
            except Exception as e:
                logger.error(f"Content-based filtering failed: {str(e)}")
            
            # ì¶”ì²œì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì¸ê¸°ë„ ê¸°ë°˜ í´ë°±
            if not recommendations_dict:
                return self._get_popularity_recommendations(user_id, num_recommendations)
            
            # ì„ íƒëœ ì „ëµìœ¼ë¡œ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ìƒì„±
            hybrid_strategy = self.strategies.get(strategy, self.strategies[self.default_strategy])
            
            final_recommendations = hybrid_strategy.combine_recommendations(
                recommendations_dict, user_id, num_recommendations
            )
            
            # ë‹¤ì–‘ì„± ì¦ì§„ (ì„ íƒì‚¬í•­)
            if context and context.get('diversity_boost', False):
                final_recommendations = self._apply_diversity_boost(final_recommendations)
            
            return final_recommendations
            
        except Exception as e:
            logger.error(f"Hybrid recommendation failed: {str(e)}")
            return self._get_popularity_recommendations(user_id, num_recommendations)
    
    def _select_optimal_strategy(self, user_id: int, context: Dict = None) -> str:
        """ìµœì  í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ ì„ íƒ"""
        
        # ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ìˆ˜ í™•ì¸
        interaction_count = UserInteraction.objects.filter(user_id=user_id).count()
        
        # A/B í…ŒìŠ¤íŠ¸ ê·¸ë£¹ í™•ì¸
        ab_group = self._get_ab_test_group(user_id)
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì„ íƒ
        if context:
            if context.get('exploration_mode', False):
                return 'mixed'  # íƒí—˜ ëª¨ë“œì—ì„œëŠ” ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ í˜¼í•©
            elif context.get('precision_mode', False):
                return 'weighted'  # ì •í™•ë„ ìš°ì„ ì‹œ
        
        # ê¸°ë³¸ ì„ íƒ ë¡œì§
        if interaction_count < 5:
            return 'cascading'  # ì‹ ê·œ ì‚¬ìš©ì
        elif interaction_count < 20:
            return 'switching'  # ì¤‘ê°„ ì‚¬ìš©ì
        else:
            if ab_group == 'A':
                return 'weighted'
            else:
                return 'mixed'
    
    def _get_ab_test_group(self, user_id: int) -> str:
        """A/B í…ŒìŠ¤íŠ¸ ê·¸ë£¹ ê²°ì •"""
        # ì‚¬ìš©ì ID ê¸°ë°˜ í•´ì‹œë¡œ ì¼ê´€ëœ ê·¸ë£¹ ë°°ì •
        return 'A' if user_id % 2 == 0 else 'B'
    
    def _apply_diversity_boost(self, recommendations: List[Dict]) -> List[Dict]:
        """ë‹¤ì–‘ì„± ì¦ì§„ ì ìš©"""
        if len(recommendations) <= 1:
            return recommendations
        
        # ì¹´í…Œê³ ë¦¬/ë¸Œëœë“œ ë‹¤ì–‘ì„± í™•ì¸
        categories = set()
        brands = set()
        diversified_recs = []
        
        for rec in recommendations:
            product = rec['product']
            category_id = product.category_id if product.category else None
            brand_id = product.brand_id if product.brand else None
            
            # ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°
            diversity_bonus = 0
            
            if category_id not in categories:
                diversity_bonus += 0.1
                categories.add(category_id)
            
            if brand_id not in brands:
                diversity_bonus += 0.05
                brands.add(brand_id)
            
            # ì ìˆ˜ì— ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤ ì¶”ê°€
            rec['score'] += diversity_bonus
            diversified_recs.append(rec)
        
        # ìƒˆë¡œìš´ ì ìˆ˜ë¡œ ì¬ì •ë ¬
        diversified_recs.sort(key=lambda x: x['score'], reverse=True)
        
        # ìˆœìœ„ ì¬ì„¤ì •
        for i, rec in enumerate(diversified_recs):
            rec['rank'] = i + 1
        
        return diversified_recs
    
    def _get_popularity_recommendations(self, user_id: int, num_recommendations: int) -> List[Dict]:
        """ì¸ê¸°ë„ ê¸°ë°˜ í´ë°± ì¶”ì²œ"""
        popular_products = Product.objects.filter(
            is_active=True
        ).order_by('-purchase_count', '-view_count')[:num_recommendations]
        
        recommendations = []
        for i, product in enumerate(popular_products):
            recommendations.append({
                'product_id': product.id,
                'product': product,
                'score': 1.0 - (i * 0.05),  # ìˆœìœ„ë³„ ì ìˆ˜
                'rank': i + 1,
                'algorithm': 'popularity_fallback',
                'explanation': {
                    'primary_reason': 'Popular product',
                    'fallback_reason': 'Insufficient data for personalized recommendations'
                }
            })
        
        return recommendations
    
    def evaluate_strategy_performance(self, user_id: int, strategy: str, 
                                   recommendations: List[Dict]) -> Dict[str, float]:
        """ì „ëµ ì„±ê³¼ í‰ê°€"""
        
        # ì‹¤ì œ ì‚¬ìš©ì í–‰ë™ê³¼ ë¹„êµí•˜ì—¬ ì„±ê³¼ ì¸¡ì •
        # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ë‚˜ ì˜¤í”„ë¼ì¸ í‰ê°€ ë°ì´í„° ì‚¬ìš©)
        
        metrics = {
            'precision': 0.0,
            'recall': 0.0,
            'diversity': 0.0,
            'novelty': 0.0,
            'coverage': 0.0
        }
        
        if not recommendations:
            return metrics
        
        # ë‹¤ì–‘ì„± ê³„ì‚° (ì¹´í…Œê³ ë¦¬/ë¸Œëœë“œ ê¸°ì¤€)
        categories = set(rec['product'].category_id for rec in recommendations if rec['product'].category)
        brands = set(rec['product'].brand_id for rec in recommendations if rec['product'].brand)
        
        metrics['diversity'] = (len(categories) + len(brands)) / (2 * len(recommendations))
        
        # ì‹ ê·œì„± ê³„ì‚° (ì¸ê¸°ë„ ì—­ìˆ˜ ê¸°ì¤€)
        popularity_scores = [1.0 / max(rec['product'].view_count, 1) for rec in recommendations]
        metrics['novelty'] = np.mean(popularity_scores)
        
        # ì»¤ë²„ë¦¬ì§€ (ì „ì²´ ìƒí’ˆ ëŒ€ë¹„ ì¶”ì²œëœ ìƒí’ˆ ë¹„ìœ¨)
        total_products = Product.objects.filter(is_active=True).count()
        metrics['coverage'] = len(recommendations) / max(total_products, 1)
        
        return metrics
```

### 5.2 ì‹¤ì‹œê°„ ì¶”ì²œ ìµœì í™”

```python
# apps/recommendations/services.py
from typing import Dict, List, Optional, Any
from django.core.cache import cache
from django.contrib.auth.models import User
from django.utils import timezone
from datetime import timedelta
from apps.recommendations.algorithms.hybrid import HybridRecommendationManager
from apps.recommendations.models import (
    RecommendationRequest, RecommendationResult, 
    RecommendationAlgorithm, UserInteraction
)
import logging
import time
import hashlib

logger = logging.getLogger(__name__)

class RecommendationService:
    """ì¶”ì²œ ì„œë¹„ìŠ¤ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.hybrid_manager = HybridRecommendationManager()
        self.cache_timeout = 1800  # 30ë¶„
        self.max_cache_size = 1000
    
    def get_recommendations(self, user_id: int, num_recommendations: int = 10,
                          context: Dict = None, force_refresh: bool = False) -> Dict[str, Any]:
        """ë©”ì¸ ì¶”ì²œ API"""
        
        start_time = time.time()
        
        try:
            # ìºì‹œ í‚¤ ìƒì„±
            cache_key = self._generate_cache_key(user_id, num_recommendations, context)
            
            # ìºì‹œ í™•ì¸
            if not force_refresh:
                cached_result = cache.get(cache_key)
                if cached_result:
                    logger.info(f"Cache hit for user {user_id}")
                    cached_result['cache_hit'] = True
                    cached_result['response_time_ms'] = int((time.time() - start_time) * 1000)
                    return cached_result
            
            # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
            user_context = self._analyze_user_context(user_id, context)
            
            # ì¶”ì²œ ìƒì„±
            recommendations = self.hybrid_manager.recommend(
                user_id=user_id,
                num_recommendations=num_recommendations,
                context=user_context
            )
            
            # ì¶”ì²œ ìš”ì²­ ë¡œê¹…
            request_id = self._log_recommendation_request(
                user_id, recommendations, user_context, start_time
            )
            
            # ê²°ê³¼ í¬ë§·íŒ…
            result = {
                'recommendations': [
                    {
                        'product_id': rec['product_id'],
                        'product_name': rec['product'].name,
                        'product_price': float(rec['product'].price),
                        'product_image': getattr(rec['product'], 'image_url', ''),
                        'score': rec['score'],
                        'rank': rec['rank'],
                        'algorithm': rec['algorithm'],
                        'explanation': rec.get('explanation', {})
                    }
                    for rec in recommendations
                ],
                'total_count': len(recommendations),
                'algorithm_used': recommendations[0]['algorithm'] if recommendations else 'none',
                'request_id': request_id,
                'user_context': user_context,
                'cache_hit': False,
                'response_time_ms': int((time.time() - start_time) * 1000)
            }
            
            # ê²°ê³¼ ìºì‹±
            cache.set(cache_key, result, timeout=self.cache_timeout)
            
            return result
            
        except Exception as e:
            logger.error(f"Error in get_recommendations: {str(e)}")
            return {
                'recommendations': [],
                'total_count': 0,
                'algorithm_used': 'error',
                'request_id': '',
                'error': str(e),
                'response_time_ms': int((time.time() - start_time) * 1000)
            }
    
    def get_similar_products(self, product_id: int, num_recommendations: int = 10) -> List[Dict]:
        """ìœ ì‚¬ ìƒí’ˆ ì¶”ì²œ"""
        
        cache_key = f"similar_products_{product_id}_{num_recommendations}"
        cached_result = cache.get(cache_key)
        
        if cached_result:
            return cached_result
        
        try:
            # ì½˜í…ì¸  ê¸°ë°˜ ìœ ì‚¬ ìƒí’ˆ
            content_similar = self.hybrid_manager.content_based.get_similar_products(
                product_id, num_recommendations
            )
            
            # í˜‘ì—… í•„í„°ë§ ê¸°ë°˜ ìœ ì‚¬ ìƒí’ˆ
            cf_similar = self.hybrid_manager.collaborative_item.get_similar_items(
                product_id, num_recommendations
            )
            
            # ë‘ ê²°ê³¼ ì¡°í•© (ê°€ì¤‘ í‰ê· )
            combined_results = self._combine_similar_products(content_similar, cf_similar)
            
            # ìºì‹±
            cache.set(cache_key, combined_results, timeout=3600)  # 1ì‹œê°„
            
            return combined_results
            
        except Exception as e:
            logger.error(f"Error in get_similar_products: {str(e)}")
            return []
    
    def update_user_interaction(self, user_id: int, product_id: int, 
                              interaction_type: str, value: float = None,
                              context: Dict = None) -> bool:
        """ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ì—…ë°ì´íŠ¸"""
        
        try:
            # ìƒí˜¸ì‘ìš© ê¸°ë¡
            interaction = UserInteraction.objects.create(
                user_id=user_id,
                product_id=product_id,
                interaction_type=interaction_type,
                value=value,
                session_id=context.get('session_id', '') if context else '',
                device_type=context.get('device_type', '') if context else '',
                source=context.get('source', '') if context else ''
            )
            
            # ê´€ë ¨ ìºì‹œ ë¬´íš¨í™”
            self._invalidate_user_cache(user_id)
            
            # ì‹¤ì‹œê°„ ëª¨ë¸ ì—…ë°ì´íŠ¸ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)
            from apps.recommendations.tasks import update_user_profile
            update_user_profile.delay(user_id)
            
            logger.info(f"Interaction recorded: user={user_id}, product={product_id}, type={interaction_type}")
            return True
            
        except Exception as e:
            logger.error(f"Error recording interaction: {str(e)}")
            return False
    
    def get_recommendation_explanation(self, user_id: int, product_id: int) -> Dict:
        """ì¶”ì²œ ì´ìœ  ìƒì„¸ ì„¤ëª…"""
        
        try:
            # ìµœê·¼ ì¶”ì²œ ê²°ê³¼ì—ì„œ í•´ë‹¹ ìƒí’ˆ ì°¾ê¸°
            recent_request = RecommendationRequest.objects.filter(
                user_id=user_id,
                created__gte=timezone.now() - timedelta(hours=1)
            ).first()
            
            if recent_request:
                result = RecommendationResult.objects.filter(
                    request=recent_request,
                    product_id=product_id
                ).first()
                
                if result:
                    return {
                        'explanation': result.explanation,
                        'score': result.score,
                        'rank': result.rank,
                        'algorithm_used': recent_request.algorithm.name if recent_request.algorithm else 'unknown'
                    }
            
            # ì‹¤ì‹œê°„ ì„¤ëª… ìƒì„± (ìºì‹œëœ ì¶”ì²œì´ ì—†ëŠ” ê²½ìš°)
            return self._generate_realtime_explanation(user_id, product_id)
            
        except Exception as e:
            logger.error(f"Error generating explanation: {str(e)}")
            return {'explanation': 'Unable to generate explanation'}
    
    def _generate_cache_key(self, user_id: int, num_recommendations: int, context: Dict = None) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_parts = [f"rec_{user_id}_{num_recommendations}"]
        
        if context:
            # ì»¨í…ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ì—¬ ì¼ê´€ëœ í‚¤ ìƒì„±
            context_str = '_'.join(f"{k}:{v}" for k, v in sorted(context.items()))
            key_parts.append(context_str)
        
        # í•´ì‹œë¡œ í‚¤ ê¸¸ì´ ì œí•œ
        key_string = '_'.join(key_parts)
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _analyze_user_context(self, user_id: int, context: Dict = None) -> Dict:
        """ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
        
        user_context = {
            'user_id': user_id,
            'timestamp': timezone.now().isoformat(),
        }
        
        # ì™¸ë¶€ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        if context:
            user_context.update(context)
        
        # ì‚¬ìš©ì í™œë™ íŒ¨í„´ ë¶„ì„
        recent_interactions = UserInteraction.objects.filter(
            user_id=user_id,
            created__gte=timezone.now() - timedelta(days=7)
        ).count()
        
        user_context['recent_activity_level'] = 'high' if recent_interactions >= 10 else 'low'
        
        # ì‹œê°„ëŒ€ ë¶„ì„
        current_hour = timezone.now().hour
        if 6 <= current_hour < 12:
            user_context['time_segment'] = 'morning'
        elif 12 <= current_hour < 18:
            user_context['time_segment'] = 'afternoon'
        elif 18 <= current_hour < 22:
            user_context['time_segment'] = 'evening'
        else:
            user_context['time_segment'] = 'night'
        
        return user_context
    
    def _log_recommendation_request(self, user_id: int, recommendations: List[Dict],
                                  context: Dict, start_time: float) -> str:
        """ì¶”ì²œ ìš”ì²­ ë¡œê¹…"""
        
        try:
            response_time = int((time.time() - start_time) * 1000)
            
            # ì¶”ì²œ ìš”ì²­ ë¡œê·¸ ìƒì„±
            request = RecommendationRequest.objects.create(
                user_id=user_id,
                num_requested=len(recommendations),
                num_returned=len(recommendations),
                response_time_ms=response_time,
                context=context
            )
            
            # ê°œë³„ ì¶”ì²œ ê²°ê³¼ ë¡œê·¸
            results_to_create = []
            for rec in recommendations:
                results_to_create.append(
                    RecommendationResult(
                        request=request,
                        product_id=rec['product_id'],
                        rank=rec['rank'],
                        score=rec['score'],
                        explanation=rec.get('explanation', {})
                    )
                )
            
            # ë²Œí¬ ìƒì„±
            RecommendationResult.objects.bulk_create(results_to_create)
            
            return str(request.id)
            
        except Exception as e:
            logger.error(f"Error logging recommendation request: {str(e)}")
            return ''
    
    def _combine_similar_products(self, content_similar: List[Dict], 
                                cf_similar: List[Dict]) -> List[Dict]:
        """ìœ ì‚¬ ìƒí’ˆ ê²°ê³¼ ì¡°í•©"""
        
        # ìƒí’ˆë³„ ì ìˆ˜ í•©ì‚°
        product_scores = {}
        
        # ì½˜í…ì¸  ê¸°ë°˜ ì ìˆ˜ (ê°€ì¤‘ì¹˜ 0.6)
        for item in content_similar:
            product_id = item['product_id']
            product_scores[product_id] = {
                'product': item['product'],
                'content_score': item['score'] * 0.6,
                'cf_score': 0
            }
        
        # í˜‘ì—… í•„í„°ë§ ì ìˆ˜ (ê°€ì¤‘ì¹˜ 0.4)
        for item in cf_similar:
            product_id = item['product_id']
            if product_id in product_scores:
                product_scores[product_id]['cf_score'] = item['similarity_score'] * 0.4
            else:
                product_scores[product_id] = {
                    'product': item['product'],
                    'content_score': 0,
                    'cf_score': item['similarity_score'] * 0.4
                }
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
        combined_results = []
        for product_id, scores in product_scores.items():
            total_score = scores['content_score'] + scores['cf_score']
            combined_results.append({
                'product_id': product_id,
                'product': scores['product'],
                'score': total_score,
                'algorithm': 'hybrid_similar'
            })
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        combined_results.sort(key=lambda x: x['score'], reverse=True)
        
        return combined_results
    
    def _invalidate_user_cache(self, user_id: int):
        """ì‚¬ìš©ì ê´€ë ¨ ìºì‹œ ë¬´íš¨í™”"""
        
        # ì‚¬ìš©ìë³„ ìºì‹œ íŒ¨í„´ìœ¼ë¡œ ì‚­ì œ
        cache_patterns = [
            f"rec_{user_id}_*",
            f"user_profile_{user_id}",
            f"user_similarities_{user_id}_*"
        ]
        
        # Redisì˜ ê²½ìš° íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì‚­ì œ ê°€ëŠ¥
        # Django ê¸°ë³¸ ìºì‹œì˜ ê²½ìš° ê°œë³„ í‚¤ ê´€ë¦¬ í•„ìš”
        try:
            cache.delete_pattern(f"rec_{user_id}_*")
        except AttributeError:
            # íŒ¨í„´ ì‚­ì œë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” ìºì‹œ ë°±ì—”ë“œ
            pass
    
    def _generate_realtime_explanation(self, user_id: int, product_id: int) -> Dict:
        """ì‹¤ì‹œê°„ ì¶”ì²œ ì„¤ëª… ìƒì„±"""
        
        try:
            # ì‚¬ìš©ìì˜ ìµœê·¼ ìƒí˜¸ì‘ìš© ë¶„ì„
            recent_interactions = UserInteraction.objects.filter(
                user_id=user_id
            ).select_related('product').order_by('-created')[:10]
            
            explanations = []
            
            # ìœ ì‚¬í•œ ìƒí’ˆ êµ¬ë§¤/ì¡°íšŒ ì´ë ¥
            for interaction in recent_interactions:
                if interaction.product.category_id:
                    try:
                        target_product = Product.objects.get(id=product_id)
                        if (target_product.category_id == interaction.product.category_id):
                            explanations.append(f"Similar to {interaction.product.name} you viewed")
                            break
                    except Product.DoesNotExist:
                        pass
            
            return {
                'explanation': {
                    'primary_reason': explanations[0] if explanations else 'Recommended for you',
                    'confidence': 0.7,
                    'generated_realtime': True
                }
            }
            
        except Exception as e:
            logger.error(f"Error generating realtime explanation: {str(e)}")
            return {'explanation': 'Unable to generate explanation'}
```

---

## ğŸ¯ 6. Django Ninja API ë° ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤

Django Ninjaë¥¼ í™œìš©í•˜ì—¬ ê³ ì„±ëŠ¥ ì¶”ì²œ APIë¥¼ êµ¬í˜„í•˜ê³  ì‹¤ì‹œê°„ ì¶”ì²œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### 6.1 Django Ninja API êµ¬í˜„

```python
# apps/recommendations/api.py
from ninja import NinjaAPI, Schema, Query
from ninja.security import HttpBearer
from django.shortcuts import get_object_or_404
from django.contrib.auth.models import User
from django.http import HttpRequest
from typing import List, Optional, Dict, Any
from datetime import datetime
from apps.recommendations.services import RecommendationService
from apps.recommendations.models import UserInteraction
from apps.products.models import Product
import logging

logger = logging.getLogger(__name__)

# API ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
api = NinjaAPI(
    title="Recommendation Engine API",
    version="1.0.0",
    description="Advanced recommendation system powered by Django Ninja",
    docs_url="/recommendations/docs/"
)

# ì¸ì¦ ìŠ¤í‚¤ë§ˆ
class AuthBearer(HttpBearer):
    def authenticate(self, request: HttpRequest, token: str):
        try:
            # JWT í† í° ê²€ì¦ ë¡œì§ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” JWT ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œ
            if token.startswith("user_"):
                user_id = int(token.split("_")[1])
                return User.objects.get(id=user_id)
        except:
            pass
        return None

auth = AuthBearer()

# ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
class RecommendationRequest(Schema):
    num_recommendations: int = 10
    strategy: Optional[str] = None
    context: Optional[Dict[str, Any]] = None
    force_refresh: bool = False
    diversity_boost: bool = False

class ProductSchema(Schema):
    id: int
    name: str
    price: float
    image_url: Optional[str] = None
    category: Optional[str] = None
    brand: Optional[str] = None
    rating: Optional[float] = None

class RecommendationItemSchema(Schema):
    product_id: int
    product_name: str
    product_price: float
    product_image: Optional[str] = None
    score: float
    rank: int
    algorithm: str
    explanation: Dict[str, Any]

class RecommendationResponse(Schema):
    recommendations: List[RecommendationItemSchema]
    total_count: int
    algorithm_used: str
    request_id: str
    cache_hit: bool
    response_time_ms: int
    user_context: Optional[Dict[str, Any]] = None

class InteractionRequest(Schema):
    product_id: int
    interaction_type: str  # 'view', 'like', 'purchase', 'cart_add', 'share'
    value: Optional[float] = None
    session_id: Optional[str] = None
    device_type: Optional[str] = None
    source: Optional[str] = None

class InteractionResponse(Schema):
    success: bool
    message: str
    interaction_id: Optional[int] = None

class SimilarProductsResponse(Schema):
    products: List[RecommendationItemSchema]
    total_count: int
    reference_product_id: int

class ExplanationResponse(Schema):
    product_id: int
    explanation: Dict[str, Any]
    score: float
    algorithm_used: str

# ì¶”ì²œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
recommendation_service = RecommendationService()

@api.post("/recommendations", response=RecommendationResponse, auth=auth)
def get_recommendations(request: HttpRequest, data: RecommendationRequest):
    """ê°œì¸í™”ëœ ì¶”ì²œ ëª©ë¡ ì¡°íšŒ"""
    
    user = request.auth
    
    try:
        # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
        context = data.context or {}
        context.update({
            'diversity_boost': data.diversity_boost,
            'request_timestamp': datetime.now().isoformat(),
            'user_agent': request.META.get('HTTP_USER_AGENT', ''),
            'ip_address': request.META.get('REMOTE_ADDR', '')
        })
        
        # ì¶”ì²œ ìƒì„±
        result = recommendation_service.get_recommendations(
            user_id=user.id,
            num_recommendations=data.num_recommendations,
            context=context,
            force_refresh=data.force_refresh
        )
        
        # ì‘ë‹µ í¬ë§·íŒ…
        return RecommendationResponse(
            recommendations=[
                RecommendationItemSchema(**rec) for rec in result['recommendations']
            ],
            total_count=result['total_count'],
            algorithm_used=result['algorithm_used'],
            request_id=result['request_id'],
            cache_hit=result.get('cache_hit', False),
            response_time_ms=result['response_time_ms'],
            user_context=result.get('user_context')
        )
        
    except Exception as e:
        logger.error(f"Error in get_recommendations API: {str(e)}")
        return RecommendationResponse(
            recommendations=[],
            total_count=0,
            algorithm_used="error",
            request_id="",
            cache_hit=False,
            response_time_ms=0
        )

@api.post("/interactions", response=InteractionResponse, auth=auth)
def record_interaction(request: HttpRequest, data: InteractionRequest):
    """ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ê¸°ë¡"""
    
    user = request.auth
    
    try:
        # ìƒí˜¸ì‘ìš© ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
        context = {
            'session_id': data.session_id,
            'device_type': data.device_type,
            'source': data.source,
            'timestamp': datetime.now().isoformat(),
            'user_agent': request.META.get('HTTP_USER_AGENT', ''),
            'referer': request.META.get('HTTP_REFERER', '')
        }
        
        # ìƒí˜¸ì‘ìš© ê¸°ë¡
        success = recommendation_service.update_user_interaction(
            user_id=user.id,
            product_id=data.product_id,
            interaction_type=data.interaction_type,
            value=data.value,
            context=context
        )
        
        if success:
            # ìµœê·¼ ìƒì„±ëœ ìƒí˜¸ì‘ìš© ID ì¡°íšŒ
            interaction = UserInteraction.objects.filter(
                user=user,
                product_id=data.product_id,
                interaction_type=data.interaction_type
            ).order_by('-created').first()
            
            return InteractionResponse(
                success=True,
                message="Interaction recorded successfully",
                interaction_id=interaction.id if interaction else None
            )
        else:
            return InteractionResponse(
                success=False,
                message="Failed to record interaction"
            )
            
    except Exception as e:
        logger.error(f"Error recording interaction: {str(e)}")
        return InteractionResponse(
            success=False,
            message=f"Error: {str(e)}"
        )

@api.get("/products/{product_id}/similar", response=SimilarProductsResponse)
def get_similar_products(request: HttpRequest, product_id: int, num_items: int = Query(10)):
    """ìœ ì‚¬ ìƒí’ˆ ì¶”ì²œ"""
    
    try:
        # ìƒí’ˆ ì¡´ì¬ í™•ì¸
        product = get_object_or_404(Product, id=product_id)
        
        # ìœ ì‚¬ ìƒí’ˆ ì¡°íšŒ
        similar_products = recommendation_service.get_similar_products(
            product_id=product_id,
            num_recommendations=num_items
        )
        
        # ì‘ë‹µ í¬ë§·íŒ…
        formatted_products = []
        for item in similar_products:
            formatted_products.append(
                RecommendationItemSchema(
                    product_id=item['product_id'],
                    product_name=item['product'].name,
                    product_price=float(item['product'].price),
                    product_image=getattr(item['product'], 'image_url', ''),
                    score=item['score'],
                    rank=len(formatted_products) + 1,
                    algorithm=item['algorithm'],
                    explanation={'similarity_reason': 'Product similarity based on features'}
                )
            )
        
        return SimilarProductsResponse(
            products=formatted_products,
            total_count=len(formatted_products),
            reference_product_id=product_id
        )
        
    except Exception as e:
        logger.error(f"Error getting similar products: {str(e)}")
        return SimilarProductsResponse(
            products=[],
            total_count=0,
            reference_product_id=product_id
        )

@api.get("/recommendations/{product_id}/explanation", response=ExplanationResponse, auth=auth)
def get_recommendation_explanation(request: HttpRequest, product_id: int):
    """ì¶”ì²œ ì´ìœ  ìƒì„¸ ì„¤ëª…"""
    
    user = request.auth
    
    try:
        explanation = recommendation_service.get_recommendation_explanation(
            user_id=user.id,
            product_id=product_id
        )
        
        return ExplanationResponse(
            product_id=product_id,
            explanation=explanation.get('explanation', {}),
            score=explanation.get('score', 0.0),
            algorithm_used=explanation.get('algorithm_used', 'unknown')
        )
        
    except Exception as e:
        logger.error(f"Error getting explanation: {str(e)}")
        return ExplanationResponse(
            product_id=product_id,
            explanation={'error': str(e)},
            score=0.0,
            algorithm_used='error'
        )

@api.get("/users/stats", auth=auth)
def get_user_stats(request: HttpRequest):
    """ì‚¬ìš©ì ì¶”ì²œ í†µê³„"""
    
    user = request.auth
    
    try:
        from django.db.models import Count, Avg
        from django.utils import timezone
        from datetime import timedelta
        
        # ìµœê·¼ 30ì¼ í†µê³„
        thirty_days_ago = timezone.now() - timedelta(days=30)
        
        stats = {
            'total_interactions': UserInteraction.objects.filter(user=user).count(),
            'recent_interactions': UserInteraction.objects.filter(
                user=user, 
                created__gte=thirty_days_ago
            ).count(),
            'interaction_types': list(
                UserInteraction.objects.filter(user=user)
                .values('interaction_type')
                .annotate(count=Count('id'))
                .order_by('-count')
            ),
            'favorite_categories': [],  # êµ¬í˜„ í•„ìš”
            'recommendation_accuracy': 0.75,  # ì‹¤ì œ ê³„ì‚° í•„ìš”
        }
        
        return stats
        
    except Exception as e:
        logger.error(f"Error getting user stats: {str(e)}")
        return {'error': str(e)}

# ê´€ë¦¬ììš© API
@api.get("/admin/algorithms/performance")
def get_algorithm_performance(request: HttpRequest):
    """ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ í†µê³„ (ê´€ë¦¬ììš©)"""
    
    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê´€ë¦¬ì ê¶Œí•œ ì²´í¬ í•„ìš”
    
    try:
        from django.db.models import Avg, Count
        from apps.recommendations.models import RecommendationRequest
        
        # ì•Œê³ ë¦¬ì¦˜ë³„ ì„±ëŠ¥ í†µê³„
        performance_stats = RecommendationRequest.objects.values(
            'algorithm__name'
        ).annotate(
            avg_response_time=Avg('response_time_ms'),
            total_requests=Count('id'),
            avg_returned=Avg('num_returned')
        ).order_by('-total_requests')
        
        return {
            'algorithm_performance': list(performance_stats),
            'total_requests': RecommendationRequest.objects.count(),
            'avg_response_time': RecommendationRequest.objects.aggregate(
                avg=Avg('response_time_ms')
            )['avg']
        }
        
    except Exception as e:
        logger.error(f"Error getting performance stats: {str(e)}")
        return {'error': str(e)}

@api.post("/admin/cache/clear")
def clear_recommendation_cache(request: HttpRequest):
    """ì¶”ì²œ ìºì‹œ í´ë¦¬ì–´ (ê´€ë¦¬ììš©)"""
    
    try:
        from django.core.cache import cache
        
        # íŒ¨í„´ ê¸°ë°˜ ìºì‹œ ì‚­ì œ
        try:
            cache.delete_pattern("rec_*")
            cache.delete_pattern("similar_products_*")
            cache.delete_pattern("user_profile_*")
            return {'success': True, 'message': 'Cache cleared successfully'}
        except AttributeError:
            # íŒ¨í„´ ì‚­ì œë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” ìºì‹œ ë°±ì—”ë“œ
            cache.clear()
            return {'success': True, 'message': 'All cache cleared'}
            
    except Exception as e:
        logger.error(f"Error clearing cache: {str(e)}")
        return {'success': False, 'error': str(e)}

# í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
@api.get("/health")
def health_check(request: HttpRequest):
    """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
    
    try:
        from django.db import connection
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì²´í¬
        with connection.cursor() as cursor:
            cursor.execute("SELECT 1")
        
        # ìºì‹œ ì—°ê²° ì²´í¬
        from django.core.cache import cache
        cache.set('health_check', 'ok', 10)
        cache_status = cache.get('health_check') == 'ok'
        
        return {
            'status': 'healthy',
            'timestamp': datetime.now().isoformat(),
            'database': 'ok',
            'cache': 'ok' if cache_status else 'error',
            'version': '1.0.0'
        }
        
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        return {
            'status': 'unhealthy',
            'timestamp': datetime.now().isoformat(),
            'error': str(e)
        }
```

### 6.2 ì‹¤ì‹œê°„ ì¶”ì²œ WebSocket ì„œë¹„ìŠ¤

```python
# apps/recommendations/websocket.py
import json
import asyncio
import logging
from channels.generic.websocket import AsyncWebsocketConsumer
from channels.db import database_sync_to_async
from django.contrib.auth.models import User
from apps.recommendations.services import RecommendationService
from apps.recommendations.models import UserInteraction

logger = logging.getLogger(__name__)

class RecommendationConsumer(AsyncWebsocketConsumer):
    """ì‹¤ì‹œê°„ ì¶”ì²œ WebSocket ì»¨ìŠˆë¨¸"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.user_id = None
        self.room_group_name = None
        self.recommendation_service = RecommendationService()
    
    async def connect(self):
        """WebSocket ì—°ê²°"""
        
        try:
            # URLì—ì„œ ì‚¬ìš©ì ID ì¶”ì¶œ
            self.user_id = self.scope['url_route']['kwargs']['user_id']
            
            # ì¸ì¦ í™•ì¸ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” JWT í† í° ê²€ì¦)
            user = await self.get_user(self.user_id)
            if not user:
                await self.close(code=4001)
                return
            
            # ê·¸ë£¹ ì´ë¦„ ìƒì„±
            self.room_group_name = f"recommendations_{self.user_id}"
            
            # ê·¸ë£¹ ì°¸ê°€
            await self.channel_layer.group_add(
                self.room_group_name,
                self.channel_name
            )
            
            await self.accept()
            
            # ì—°ê²° í™•ì¸ ë©”ì‹œì§€ ì „ì†¡
            await self.send(text_data=json.dumps({
                'type': 'connection_established',
                'message': f'Connected to recommendation stream for user {self.user_id}',
                'timestamp': self._get_timestamp()
            }))
            
            # ì´ˆê¸° ì¶”ì²œ ì „ì†¡
            await self.send_initial_recommendations()
            
        except Exception as e:
            logger.error(f"WebSocket connection error: {str(e)}")
            await self.close(code=4000)
    
    async def disconnect(self, close_code):
        """WebSocket ì—°ê²° í•´ì œ"""
        
        if self.room_group_name:
            await self.channel_layer.group_discard(
                self.room_group_name,
                self.channel_name
            )
    
    async def receive(self, text_data):
        """í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë©”ì‹œì§€ ìˆ˜ì‹ """
        
        try:
            data = json.loads(text_data)
            message_type = data.get('type')
            
            if message_type == 'get_recommendations':
                await self.handle_recommendation_request(data)
            
            elif message_type == 'record_interaction':
                await self.handle_interaction(data)
            
            elif message_type == 'get_similar_products':
                await self.handle_similar_products_request(data)
            
            else:
                await self.send_error("Unknown message type")
                
        except json.JSONDecodeError:
            await self.send_error("Invalid JSON format")
        except Exception as e:
            logger.error(f"WebSocket receive error: {str(e)}")
            await self.send_error(str(e))
    
    async def handle_recommendation_request(self, data):
        """ì¶”ì²œ ìš”ì²­ ì²˜ë¦¬"""
        
        try:
            num_recommendations = data.get('num_recommendations', 10)
            context = data.get('context', {})
            force_refresh = data.get('force_refresh', False)
            
            # ì‹¤ì‹œê°„ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
            context.update({
                'channel': 'websocket',
                'real_time': True,
                'session_type': 'streaming'
            })
            
            # ì¶”ì²œ ìƒì„± (ë¹„ë™ê¸°)
            recommendations = await self.get_recommendations_async(
                self.user_id, num_recommendations, context, force_refresh
            )
            
            # í´ë¼ì´ì–¸íŠ¸ì— ì „ì†¡
            await self.send(text_data=json.dumps({
                'type': 'recommendations',
                'data': recommendations,
                'timestamp': self._get_timestamp()
            }))
            
        except Exception as e:
            await self.send_error(f"Recommendation error: {str(e)}")
    
    async def handle_interaction(self, data):
        """ìƒí˜¸ì‘ìš© ê¸°ë¡ ì²˜ë¦¬"""
        
        try:
            product_id = data.get('product_id')
            interaction_type = data.get('interaction_type')
            value = data.get('value')
            
            if not product_id or not interaction_type:
                await self.send_error("Missing product_id or interaction_type")
                return
            
            # ìƒí˜¸ì‘ìš© ê¸°ë¡ (ë¹„ë™ê¸°)
            success = await self.record_interaction_async(
                self.user_id, product_id, interaction_type, value
            )
            
            if success:
                await self.send(text_data=json.dumps({
                    'type': 'interaction_recorded',
                    'product_id': product_id,
                    'interaction_type': interaction_type,
                    'timestamp': self._get_timestamp()
                }))
                
                # ì‹¤ì‹œê°„ ì¶”ì²œ ì—…ë°ì´íŠ¸ íŠ¸ë¦¬ê±°
                await self.trigger_recommendation_update()
            else:
                await self.send_error("Failed to record interaction")
                
        except Exception as e:
            await self.send_error(f"Interaction error: {str(e)}")
    
    async def handle_similar_products_request(self, data):
        """ìœ ì‚¬ ìƒí’ˆ ìš”ì²­ ì²˜ë¦¬"""
        
        try:
            product_id = data.get('product_id')
            num_items = data.get('num_items', 10)
            
            if not product_id:
                await self.send_error("Missing product_id")
                return
            
            # ìœ ì‚¬ ìƒí’ˆ ì¡°íšŒ (ë¹„ë™ê¸°)
            similar_products = await self.get_similar_products_async(product_id, num_items)
            
            await self.send(text_data=json.dumps({
                'type': 'similar_products',
                'product_id': product_id,
                'data': similar_products,
                'timestamp': self._get_timestamp()
            }))
            
        except Exception as e:
            await self.send_error(f"Similar products error: {str(e)}")
    
    async def send_initial_recommendations(self):
        """ì´ˆê¸° ì¶”ì²œ ì „ì†¡"""
        
        try:
            recommendations = await self.get_recommendations_async(self.user_id, 10)
            
            await self.send(text_data=json.dumps({
                'type': 'initial_recommendations',
                'data': recommendations,
                'timestamp': self._get_timestamp()
            }))
            
        except Exception as e:
            logger.error(f"Initial recommendations error: {str(e)}")
    
    async def trigger_recommendation_update(self):
        """ì¶”ì²œ ì—…ë°ì´íŠ¸ íŠ¸ë¦¬ê±°"""
        
        try:
            # ì§§ì€ ì§€ì—° í›„ ì—…ë°ì´íŠ¸ëœ ì¶”ì²œ ì „ì†¡
            await asyncio.sleep(1)
            
            recommendations = await self.get_recommendations_async(
                self.user_id, 10, {'real_time_update': True}, force_refresh=True
            )
            
            await self.send(text_data=json.dumps({
                'type': 'recommendations_updated',
                'data': recommendations,
                'timestamp': self._get_timestamp()
            }))
            
        except Exception as e:
            logger.error(f"Recommendation update error: {str(e)}")
    
    async def send_error(self, message):
        """ì—ëŸ¬ ë©”ì‹œì§€ ì „ì†¡"""
        
        await self.send(text_data=json.dumps({
            'type': 'error',
            'message': message,
            'timestamp': self._get_timestamp()
        }))
    
    def _get_timestamp(self):
        """í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ ë°˜í™˜"""
        from datetime import datetime
        return datetime.now().isoformat()
    
    # ë¹„ë™ê¸° ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—…
    @database_sync_to_async
    def get_user(self, user_id):
        """ì‚¬ìš©ì ì¡°íšŒ"""
        try:
            return User.objects.get(id=user_id)
        except User.DoesNotExist:
            return None
    
    @database_sync_to_async
    def get_recommendations_async(self, user_id, num_recommendations, context=None, force_refresh=False):
        """ë¹„ë™ê¸° ì¶”ì²œ ì¡°íšŒ"""
        return self.recommendation_service.get_recommendations(
            user_id=user_id,
            num_recommendations=num_recommendations,
            context=context or {},
            force_refresh=force_refresh
        )
    
    @database_sync_to_async
    def record_interaction_async(self, user_id, product_id, interaction_type, value=None):
        """ë¹„ë™ê¸° ìƒí˜¸ì‘ìš© ê¸°ë¡"""
        return self.recommendation_service.update_user_interaction(
            user_id=user_id,
            product_id=product_id,
            interaction_type=interaction_type,
            value=value,
            context={'channel': 'websocket'}
        )
    
    @database_sync_to_async
    def get_similar_products_async(self, product_id, num_items):
        """ë¹„ë™ê¸° ìœ ì‚¬ ìƒí’ˆ ì¡°íšŒ"""
        return self.recommendation_service.get_similar_products(
            product_id=product_id,
            num_recommendations=num_items
        )
    
    # ê·¸ë£¹ ë©”ì‹œì§€ í•¸ë“¤ëŸ¬
    async def recommendation_update(self, event):
        """ê·¸ë£¹ìœ¼ë¡œë¶€í„° ì¶”ì²œ ì—…ë°ì´íŠ¸ ë©”ì‹œì§€ ìˆ˜ì‹ """
        
        await self.send(text_data=json.dumps({
            'type': 'recommendation_update',
            'data': event['data'],
            'timestamp': self._get_timestamp()
        }))

# WebSocket ë¼ìš°íŒ… ì„¤ì •
# apps/recommendations/routing.py
from django.urls import re_path
from . import websocket

websocket_urlpatterns = [
    re_path(r'ws/recommendations/(?P<user_id>\d+)/$', websocket.RecommendationConsumer.as_asgi()),
]
```

### 6.3 ì„±ëŠ¥ ìµœì í™” ë° ëª¨ë‹ˆí„°ë§

```python
# apps/recommendations/monitoring.py
import time
import logging
from functools import wraps
from django.core.cache import cache
from django.db import connection
from django.conf import settings
from typing import Dict, Any, Callable
import json

logger = logging.getLogger(__name__)

class RecommendationMetrics:
    """ì¶”ì²œ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.metrics_cache_prefix = "metrics_"
        self.metrics_ttl = 3600  # 1ì‹œê°„
    
    def record_api_call(self, endpoint: str, response_time: float, 
                       status_code: int, user_id: int = None):
        """API í˜¸ì¶œ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        
        try:
            # ë©”íŠ¸ë¦­ í‚¤ ìƒì„±
            timestamp = int(time.time())
            minute_bucket = timestamp // 60  # 1ë¶„ ë‹¨ìœ„ ë²„í‚·
            
            metrics_key = f"{self.metrics_cache_prefix}api_{endpoint}_{minute_bucket}"
            
            # ê¸°ì¡´ ë©”íŠ¸ë¦­ ì¡°íšŒ
            current_metrics = cache.get(metrics_key, {
                'calls': 0,
                'total_response_time': 0,
                'status_codes': {},
                'unique_users': set()
            })
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            current_metrics['calls'] += 1
            current_metrics['total_response_time'] += response_time
            current_metrics['status_codes'][status_code] = (
                current_metrics['status_codes'].get(status_code, 0) + 1
            )
            
            if user_id:
                current_metrics['unique_users'].add(user_id)
            
            # ìºì‹œ ì €ì¥
            cache.set(metrics_key, current_metrics, self.metrics_ttl)
            
        except Exception as e:
            logger.error(f"Error recording API metrics: {str(e)}")
    
    def record_algorithm_performance(self, algorithm: str, execution_time: float, 
                                   user_id: int, num_results: int, cache_hit: bool):
        """ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        
        try:
            timestamp = int(time.time())
            hour_bucket = timestamp // 3600  # 1ì‹œê°„ ë‹¨ìœ„
            
            metrics_key = f"{self.metrics_cache_prefix}algo_{algorithm}_{hour_bucket}"
            
            current_metrics = cache.get(metrics_key, {
                'executions': 0,
                'total_execution_time': 0,
                'cache_hits': 0,
                'total_results': 0,
                'unique_users': set()
            })
            
            current_metrics['executions'] += 1
            current_metrics['total_execution_time'] += execution_time
            current_metrics['total_results'] += num_results
            current_metrics['unique_users'].add(user_id)
            
            if cache_hit:
                current_metrics['cache_hits'] += 1
            
            cache.set(metrics_key, current_metrics, self.metrics_ttl * 24)  # 24ì‹œê°„ ë³´ê´€
            
        except Exception as e:
            logger.error(f"Error recording algorithm metrics: {str(e)}")
    
    def get_performance_summary(self, hours: int = 24) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ"""
        
        try:
            current_time = int(time.time())
            summary = {
                'api_performance': {},
                'algorithm_performance': {},
                'system_health': {}
            }
            
            # API ì„±ëŠ¥ ì§‘ê³„
            for hour_offset in range(hours):
                hour_bucket = (current_time - (hour_offset * 3600)) // 3600
                
                # API ë©”íŠ¸ë¦­ íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Redis SCAN ë“± ì‚¬ìš©)
                # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œ
                pass
            
            # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
            summary['system_health'] = self._check_system_health()
            
            return summary
            
        except Exception as e:
            logger.error(f"Error getting performance summary: {str(e)}")
            return {}
    
    def _check_system_health(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        
        health = {
            'database': 'unknown',
            'cache': 'unknown',
            'memory_usage': 0,
            'active_connections': 0
        }
        
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ
            with connection.cursor() as cursor:
                cursor.execute("SELECT 1")
                health['database'] = 'healthy'
        except:
            health['database'] = 'unhealthy'
        
        try:
            # ìºì‹œ ìƒíƒœ
            cache.set('health_test', 'ok', 10)
            health['cache'] = 'healthy' if cache.get('health_test') == 'ok' else 'unhealthy'
        except:
            health['cache'] = 'unhealthy'
        
        # í™œì„± ì—°ê²° ìˆ˜
        health['active_connections'] = len(connection.queries)
        
        return health

# ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸° ì¸ìŠ¤í„´ìŠ¤
metrics = RecommendationMetrics()

def track_performance(endpoint_name: str):
    """ì„±ëŠ¥ ì¶”ì  ë°ì½”ë ˆì´í„°"""
    
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            
            try:
                result = func(*args, **kwargs)
                status_code = 200
                
                # ì‚¬ìš©ì ID ì¶”ì¶œ ì‹œë„
                user_id = None
                if args and hasattr(args[0], 'auth') and args[0].auth:
                    user_id = args[0].auth.id
                
                return result
                
            except Exception as e:
                status_code = 500
                raise
                
            finally:
                execution_time = (time.time() - start_time) * 1000  # ms
                
                # ë©”íŠ¸ë¦­ ê¸°ë¡
                metrics.record_api_call(
                    endpoint=endpoint_name,
                    response_time=execution_time,
                    status_code=status_code,
                    user_id=user_id
                )
        
        return wrapper
    return decorator

def track_algorithm(algorithm_name: str):
    """ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ì¶”ì  ë°ì½”ë ˆì´í„°"""
    
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            
            result = func(*args, **kwargs)
            
            execution_time = (time.time() - start_time) * 1000  # ms
            
            # ê²°ê³¼ì—ì„œ ë©”íŠ¸ë¦­ ì •ë³´ ì¶”ì¶œ
            num_results = len(result) if isinstance(result, list) else 0
            cache_hit = kwargs.get('cache_hit', False)
            user_id = kwargs.get('user_id', 0)
            
            # ë©”íŠ¸ë¦­ ê¸°ë¡
            metrics.record_algorithm_performance(
                algorithm=algorithm_name,
                execution_time=execution_time,
                user_id=user_id,
                num_results=num_results,
                cache_hit=cache_hit
            )
            
            return result
        
        return wrapper
    return decorator

class RecommendationProfiler:
    """ì¶”ì²œ ì‹œìŠ¤í…œ í”„ë¡œíŒŒì¼ëŸ¬"""
    
    def __init__(self):
        self.active_profiles = {}
    
    def start_profile(self, profile_id: str, context: Dict = None):
        """í”„ë¡œíŒŒì¼ë§ ì‹œì‘"""
        
        self.active_profiles[profile_id] = {
            'start_time': time.time(),
            'context': context or {},
            'steps': []
        }
    
    def add_step(self, profile_id: str, step_name: str, duration: float = None):
        """í”„ë¡œíŒŒì¼ë§ ë‹¨ê³„ ì¶”ê°€"""
        
        if profile_id in self.active_profiles:
            step_time = time.time()
            
            self.active_profiles[profile_id]['steps'].append({
                'name': step_name,
                'timestamp': step_time,
                'duration': duration
            })
    
    def end_profile(self, profile_id: str) -> Dict[str, Any]:
        """í”„ë¡œíŒŒì¼ë§ ì¢…ë£Œ"""
        
        if profile_id not in self.active_profiles:
            return {}
        
        profile = self.active_profiles.pop(profile_id)
        total_time = time.time() - profile['start_time']
        
        return {
            'profile_id': profile_id,
            'total_time_ms': total_time * 1000,
            'context': profile['context'],
            'steps': profile['steps'],
            'steps_count': len(profile['steps'])
        }

# ì „ì—­ í”„ë¡œíŒŒì¼ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
profiler = RecommendationProfiler()

# ëª¨ë‹ˆí„°ë§ ë¯¸ë“¤ì›¨ì–´
class RecommendationMonitoringMiddleware:
    """ì¶”ì²œ API ëª¨ë‹ˆí„°ë§ ë¯¸ë“¤ì›¨ì–´"""
    
    def __init__(self, get_response):
        self.get_response = get_response
    
    def __call__(self, request):
        # ì¶”ì²œ API ê²½ë¡œ í™•ì¸
        if '/recommendations/' in request.path:
            start_time = time.time()
            
            response = self.get_response(request)
            
            # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
            response_time = (time.time() - start_time) * 1000
            
            # ë©”íŠ¸ë¦­ ê¸°ë¡
            endpoint = self._extract_endpoint(request.path)
            user_id = getattr(request.user, 'id', None) if hasattr(request, 'user') else None
            
            metrics.record_api_call(
                endpoint=endpoint,
                response_time=response_time,
                status_code=response.status_code,
                user_id=user_id
            )
            
            # ì‘ë‹µ í—¤ë”ì— ì„±ëŠ¥ ì •ë³´ ì¶”ê°€
            response['X-Response-Time'] = f"{response_time:.2f}ms"
            
            return response
        
        return self.get_response(request)
    
    def _extract_endpoint(self, path: str) -> str:
        """ê²½ë¡œì—ì„œ ì—”ë“œí¬ì¸íŠ¸ëª… ì¶”ì¶œ"""
        
        if '/recommendations' in path and path.endswith('/recommendations'):
            return 'get_recommendations'
        elif '/interactions' in path:
            return 'record_interaction'
        elif '/similar' in path:
            return 'get_similar_products'
        elif '/explanation' in path:
            return 'get_explanation'
        else:
            return 'unknown'
```

---

## ğŸ“Š 7. A/B í…ŒìŠ¤íŠ¸ ë° ì„±ëŠ¥ ë¶„ì„

ì¶”ì²œ ì‹œìŠ¤í…œì˜ íš¨ê³¼ë¥¼ ì¸¡ì •í•˜ê³  ìµœì í™”í•˜ê¸° ìœ„í•œ A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ì™€ ì„±ëŠ¥ ë¶„ì„ ë„êµ¬ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

### 7.1 A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬

```python
# apps/recommendations/ab_testing.py
import hashlib
import random
import logging
from typing import Dict, List, Any, Optional, Tuple
from enum import Enum
from datetime import datetime, timedelta
from django.db import models
from django.contrib.auth.models import User
from django.utils import timezone
from django.core.cache import cache
import json
import numpy as np
from scipy import stats

logger = logging.getLogger(__name__)

class ExperimentStatus(models.TextChoices):
    DRAFT = 'draft', 'Draft'
    ACTIVE = 'active', 'Active'
    PAUSED = 'paused', 'Paused'
    COMPLETED = 'completed', 'Completed'

class ExperimentType(models.TextChoices):
    ALGORITHM_COMPARISON = 'algorithm', 'Algorithm Comparison'
    PARAMETER_TUNING = 'parameter', 'Parameter Tuning'
    UI_VARIATION = 'ui', 'UI Variation'
    HYBRID_STRATEGY = 'hybrid', 'Hybrid Strategy'

class Experiment(models.Model):
    """A/B í…ŒìŠ¤íŠ¸ ì‹¤í—˜"""
    
    name = models.CharField(max_length=200)
    description = models.TextField()
    experiment_type = models.CharField(max_length=20, choices=ExperimentType.choices)
    status = models.CharField(max_length=20, choices=ExperimentStatus.choices, default=ExperimentStatus.DRAFT)
    
    # ì‹¤í—˜ ì„¤ì •
    traffic_allocation = models.FloatField(default=1.0)  # 0.0-1.0
    start_date = models.DateTimeField()
    end_date = models.DateTimeField()
    
    # ì„±ê³µ ì§€í‘œ
    primary_metric = models.CharField(max_length=100)  # 'ctr', 'conversion', 'engagement'
    secondary_metrics = models.JSONField(default=list)
    
    # ì‹¤í—˜ êµ¬ì„±
    variants = models.JSONField(default=dict)  # {'control': {...}, 'variant_a': {...}}
    target_sample_size = models.IntegerField(default=1000)
    confidence_level = models.FloatField(default=0.95)
    
    created = models.DateTimeField(auto_now_add=True)
    updated = models.DateTimeField(auto_now=True)
    
    class Meta:
        db_table = 'recommendation_experiments'
    
    def __str__(self):
        return f"{self.name} ({self.status})"
    
    def is_active(self) -> bool:
        """ì‹¤í—˜ì´ í™œì„± ìƒíƒœì¸ì§€ í™•ì¸"""
        now = timezone.now()
        return (
            self.status == ExperimentStatus.ACTIVE and
            self.start_date <= now <= self.end_date
        )
    
    def get_variant_for_user(self, user_id: int) -> str:
        """ì‚¬ìš©ìì—ê²Œ í• ë‹¹ëœ ë³€í˜• ë°˜í™˜"""
        
        if not self.is_active():
            return 'control'
        
        # ì¼ê´€ëœ í•´ì‹œ ê¸°ë°˜ ë³€í˜• í• ë‹¹
        hash_input = f"{self.id}_{user_id}".encode('utf-8')
        hash_value = int(hashlib.md5(hash_input).hexdigest(), 16)
        
        # íŠ¸ë˜í”½ í• ë‹¹ ë¹„ìœ¨ í™•ì¸
        if (hash_value % 1000) / 1000.0 > self.traffic_allocation:
            return 'control'
        
        # ë³€í˜• ì„ íƒ
        variants = list(self.variants.keys())
        variant_index = hash_value % len(variants)
        
        return variants[variant_index]

class ExperimentAssignment(models.Model):
    """ì‹¤í—˜ í• ë‹¹ ê¸°ë¡"""
    
    experiment = models.ForeignKey(Experiment, on_delete=models.CASCADE)
    user_id = models.IntegerField()
    variant = models.CharField(max_length=50)
    assigned_at = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        db_table = 'experiment_assignments'
        unique_together = ['experiment', 'user_id']

class ExperimentEvent(models.Model):
    """ì‹¤í—˜ ì´ë²¤íŠ¸ ê¸°ë¡"""
    
    experiment = models.ForeignKey(Experiment, on_delete=models.CASCADE)
    user_id = models.IntegerField()
    variant = models.CharField(max_length=50)
    event_type = models.CharField(max_length=100)  # 'recommendation_click', 'purchase', etc.
    event_value = models.FloatField(null=True, blank=True)
    metadata = models.JSONField(default=dict)
    created = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        db_table = 'experiment_events'

class ABTestManager:
    """A/B í…ŒìŠ¤íŠ¸ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.cache_timeout = 300  # 5ë¶„
    
    def assign_user_to_experiments(self, user_id: int) -> Dict[str, str]:
        """ì‚¬ìš©ìë¥¼ í™œì„± ì‹¤í—˜ì— í• ë‹¹"""
        
        cache_key = f"experiment_assignments_{user_id}"
        assignments = cache.get(cache_key)
        
        if assignments:
            return assignments
        
        # í™œì„± ì‹¤í—˜ ì¡°íšŒ
        active_experiments = Experiment.objects.filter(
            status=ExperimentStatus.ACTIVE,
            start_date__lte=timezone.now(),
            end_date__gte=timezone.now()
        )
        
        assignments = {}
        
        for experiment in active_experiments:
            # ê¸°ì¡´ í• ë‹¹ í™•ì¸
            existing_assignment = ExperimentAssignment.objects.filter(
                experiment=experiment,
                user_id=user_id
            ).first()
            
            if existing_assignment:
                variant = existing_assignment.variant
            else:
                # ìƒˆë¡œìš´ í• ë‹¹
                variant = experiment.get_variant_for_user(user_id)
                
                # í• ë‹¹ ê¸°ë¡
                ExperimentAssignment.objects.create(
                    experiment=experiment,
                    user_id=user_id,
                    variant=variant
                )
            
            assignments[experiment.name] = variant
        
        # ìºì‹œ ì €ì¥
        cache.set(cache_key, assignments, self.cache_timeout)
        
        return assignments
    
    def get_recommendation_config(self, user_id: int) -> Dict[str, Any]:
        """ì‚¬ìš©ìì˜ ì‹¤í—˜ í• ë‹¹ì— ë”°ë¥¸ ì¶”ì²œ ì„¤ì •"""
        
        assignments = self.assign_user_to_experiments(user_id)
        config = {
            'algorithm': 'weighted_hybrid',  # ê¸°ë³¸ê°’
            'parameters': {},
            'experiments': assignments
        }
        
        # ì‹¤í—˜ë³„ ì„¤ì • ì ìš©
        for experiment_name, variant in assignments.items():
            try:
                experiment = Experiment.objects.get(
                    name=experiment_name,
                    status=ExperimentStatus.ACTIVE
                )
                
                variant_config = experiment.variants.get(variant, {})
                
                # ì•Œê³ ë¦¬ì¦˜ ì‹¤í—˜
                if experiment.experiment_type == ExperimentType.ALGORITHM_COMPARISON:
                    config['algorithm'] = variant_config.get('algorithm', config['algorithm'])
                
                # íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í—˜
                elif experiment.experiment_type == ExperimentType.PARAMETER_TUNING:
                    config['parameters'].update(variant_config.get('parameters', {}))
                
                # í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ ì‹¤í—˜
                elif experiment.experiment_type == ExperimentType.HYBRID_STRATEGY:
                    config['hybrid_strategy'] = variant_config.get('strategy', 'weighted')
                    config['strategy_params'] = variant_config.get('params', {})
                
            except Experiment.DoesNotExist:
                logger.warning(f"Experiment {experiment_name} not found")
        
        return config
    
    def record_event(self, user_id: int, event_type: str, event_value: float = None,
                    metadata: Dict = None) -> bool:
        """ì‹¤í—˜ ì´ë²¤íŠ¸ ê¸°ë¡"""
        
        try:
            assignments = self.assign_user_to_experiments(user_id)
            
            for experiment_name, variant in assignments.items():
                try:
                    experiment = Experiment.objects.get(name=experiment_name)
                    
                    ExperimentEvent.objects.create(
                        experiment=experiment,
                        user_id=user_id,
                        variant=variant,
                        event_type=event_type,
                        event_value=event_value,
                        metadata=metadata or {}
                    )
                    
                except Experiment.DoesNotExist:
                    continue
            
            return True
            
        except Exception as e:
            logger.error(f"Error recording experiment event: {str(e)}")
            return False
    
    def analyze_experiment(self, experiment_id: int) -> Dict[str, Any]:
        """ì‹¤í—˜ ê²°ê³¼ ë¶„ì„"""
        
        try:
            experiment = Experiment.objects.get(id=experiment_id)
            
            # ë³€í˜•ë³„ í†µê³„ ìˆ˜ì§‘
            variant_stats = {}
            
            for variant_name in experiment.variants.keys():
                stats_data = self._calculate_variant_stats(experiment, variant_name)
                variant_stats[variant_name] = stats_data
            
            # í†µê³„ì  ìœ ì˜ì„± ê²€ì •
            significance_results = self._perform_significance_tests(
                experiment, variant_stats
            )
            
            # ê²°ê³¼ ìš”ì•½
            analysis_result = {
                'experiment_id': experiment_id,
                'experiment_name': experiment.name,
                'status': experiment.status,
                'duration_days': (timezone.now() - experiment.start_date).days,
                'variant_statistics': variant_stats,
                'significance_tests': significance_results,
                'recommendations': self._generate_recommendations(
                    experiment, variant_stats, significance_results
                )
            }
            
            return analysis_result
            
        except Experiment.DoesNotExist:
            return {'error': 'Experiment not found'}
        except Exception as e:
            logger.error(f"Error analyzing experiment: {str(e)}")
            return {'error': str(e)}
    
    def _calculate_variant_stats(self, experiment: Experiment, variant: str) -> Dict[str, Any]:
        """ë³€í˜•ë³„ í†µê³„ ê³„ì‚°"""
        
        # í• ë‹¹ëœ ì‚¬ìš©ì ìˆ˜
        total_users = ExperimentAssignment.objects.filter(
            experiment=experiment,
            variant=variant
        ).count()
        
        if total_users == 0:
            return {
                'total_users': 0,
                'events': {},
                'conversion_rate': 0,
                'confidence_interval': (0, 0)
            }
        
        # ì´ë²¤íŠ¸ë³„ í†µê³„
        events_data = {}
        
        # ì£¼ìš” ì§€í‘œ ì´ë²¤íŠ¸
        primary_events = ExperimentEvent.objects.filter(
            experiment=experiment,
            variant=variant,
            event_type=experiment.primary_metric
        )
        
        primary_count = primary_events.count()
        primary_conversion = primary_count / total_users if total_users > 0 else 0
        
        # ì´ë²¤íŠ¸ ê°’ í‰ê·  (ë§¤ì¶œ ë“±)
        primary_values = primary_events.exclude(event_value__isnull=True).values_list('event_value', flat=True)
        avg_value = np.mean(list(primary_values)) if primary_values else 0
        
        events_data[experiment.primary_metric] = {
            'count': primary_count,
            'conversion_rate': primary_conversion,
            'average_value': float(avg_value),
            'total_value': float(np.sum(list(primary_values))) if primary_values else 0
        }
        
        # ë³´ì¡° ì§€í‘œë“¤
        for secondary_metric in experiment.secondary_metrics:
            secondary_events = ExperimentEvent.objects.filter(
                experiment=experiment,
                variant=variant,
                event_type=secondary_metric
            )
            
            secondary_count = secondary_events.count()
            secondary_conversion = secondary_count / total_users if total_users > 0 else 0
            
            events_data[secondary_metric] = {
                'count': secondary_count,
                'conversion_rate': secondary_conversion
            }
        
        # ì‹ ë¢°êµ¬ê°„ ê³„ì‚° (ì£¼ìš” ì§€í‘œ ê¸°ì¤€)
        confidence_interval = self._calculate_confidence_interval(
            primary_count, total_users, experiment.confidence_level
        )
        
        return {
            'total_users': total_users,
            'events': events_data,
            'conversion_rate': primary_conversion,
            'confidence_interval': confidence_interval,
            'statistical_power': self._calculate_statistical_power(
                primary_count, total_users
            )
        }
    
    def _perform_significance_tests(self, experiment: Experiment, 
                                  variant_stats: Dict[str, Any]) -> Dict[str, Any]:
        """í†µê³„ì  ìœ ì˜ì„± ê²€ì •"""
        
        variants = list(variant_stats.keys())
        
        if len(variants) < 2:
            return {'error': 'Need at least 2 variants for comparison'}
        
        results = {}
        
        # ì»¨íŠ¸ë¡¤ ê·¸ë£¹ (ë³´í†µ 'control' ë˜ëŠ” ì²« ë²ˆì§¸ ë³€í˜•)
        control_variant = 'control' if 'control' in variants else variants[0]
        control_stats = variant_stats[control_variant]
        
        for variant in variants:
            if variant == control_variant:
                continue
            
            variant_data = variant_stats[variant]
            
            # ë¹„ìœ¨ ì°¨ì´ ê²€ì • (Z-test)
            z_stat, p_value = self._proportion_z_test(
                control_stats['events'][experiment.primary_metric]['count'],
                control_stats['total_users'],
                variant_data['events'][experiment.primary_metric]['count'],
                variant_data['total_users']
            )
            
            # íš¨ê³¼ í¬ê¸° ê³„ì‚°
            effect_size = (
                variant_data['conversion_rate'] - control_stats['conversion_rate']
            )
            
            # ìƒëŒ€ì  ê°œì„ ë„
            relative_improvement = (
                effect_size / control_stats['conversion_rate'] 
                if control_stats['conversion_rate'] > 0 else 0
            )
            
            results[f"{control_variant}_vs_{variant}"] = {
                'z_statistic': float(z_stat),
                'p_value': float(p_value),
                'effect_size': float(effect_size),
                'relative_improvement': float(relative_improvement),
                'is_significant': p_value < (1 - experiment.confidence_level),
                'confidence_level': experiment.confidence_level
            }
        
        return results
    
    def _proportion_z_test(self, x1: int, n1: int, x2: int, n2: int) -> Tuple[float, float]:
        """ë¹„ìœ¨ ì°¨ì´ì— ëŒ€í•œ Z ê²€ì •"""
        
        if n1 == 0 or n2 == 0:
            return 0.0, 1.0
        
        p1 = x1 / n1
        p2 = x2 / n2
        
        # í•©ë™ ë¹„ìœ¨
        p_pooled = (x1 + x2) / (n1 + n2)
        
        # í‘œì¤€ì˜¤ì°¨
        se = np.sqrt(p_pooled * (1 - p_pooled) * (1/n1 + 1/n2))
        
        if se == 0:
            return 0.0, 1.0
        
        # Z í†µê³„ëŸ‰
        z = (p2 - p1) / se
        
        # p-ê°’ (ì–‘ì¸¡ê²€ì •)
        p_value = 2 * (1 - stats.norm.cdf(abs(z)))
        
        return z, p_value
    
    def _calculate_confidence_interval(self, successes: int, trials: int, 
                                     confidence: float) -> Tuple[float, float]:
        """ì‹ ë¢°êµ¬ê°„ ê³„ì‚°"""
        
        if trials == 0:
            return (0.0, 0.0)
        
        p = successes / trials
        z_score = stats.norm.ppf(1 - (1 - confidence) / 2)
        
        se = np.sqrt(p * (1 - p) / trials)
        margin_error = z_score * se
        
        lower = max(0, p - margin_error)
        upper = min(1, p + margin_error)
        
        return (float(lower), float(upper))
    
    def _calculate_statistical_power(self, successes: int, trials: int) -> float:
        """í†µê³„ì  ê²€ì •ë ¥ ê³„ì‚°"""
        
        if trials == 0:
            return 0.0
        
        # ê°„ë‹¨í•œ ê²€ì •ë ¥ ì¶”ì • (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ê³„ì‚° í•„ìš”)
        p = successes / trials
        se = np.sqrt(p * (1 - p) / trials)
        
        # íš¨ê³¼ í¬ê¸° 0.05ë¥¼ ê°ì§€í•  ìˆ˜ ìˆëŠ” ê²€ì •ë ¥ ì¶”ì •
        effect_size = 0.05
        power = 1 - stats.norm.cdf(1.96 - effect_size / se)
        
        return float(max(0, min(1, power)))
    
    def _generate_recommendations(self, experiment: Experiment, 
                                variant_stats: Dict[str, Any], 
                                significance_results: Dict[str, Any]) -> List[str]:
        """ì‹¤í—˜ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ìƒ˜í”Œ í¬ê¸° í™•ì¸
        total_users = sum(stats['total_users'] for stats in variant_stats.values())
        
        if total_users < experiment.target_sample_size:
            recommendations.append(
                f"Sample size ({total_users}) is below target ({experiment.target_sample_size}). "
                "Consider extending the experiment duration."
            )
        
        # ìœ ì˜ì„± ê²€ì • ê²°ê³¼ í™•ì¸
        significant_results = [
            result for result in significance_results.values()
            if isinstance(result, dict) and result.get('is_significant', False)
        ]
        
        if significant_results:
            best_result = max(significant_results, key=lambda x: x['relative_improvement'])
            recommendations.append(
                f"Significant improvement found: {best_result['relative_improvement']:.2%} "
                f"relative improvement with p-value {best_result['p_value']:.4f}"
            )
        else:
            recommendations.append(
                "No statistically significant differences found. "
                "Consider testing with larger effect sizes or longer duration."
            )
        
        # ê²€ì •ë ¥ í™•ì¸
        low_power_variants = [
            variant for variant, stats in variant_stats.items()
            if stats['statistical_power'] < 0.8
        ]
        
        if low_power_variants:
            recommendations.append(
                f"Low statistical power detected for variants: {', '.join(low_power_variants)}. "
                "Consider increasing sample size."
            )
        
        return recommendations

# A/B í…ŒìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
ab_test_manager = ABTestManager()
```

### 7.2 ì„±ëŠ¥ ë¶„ì„ ë° ìµœì í™”

```python
# apps/recommendations/performance_analyzer.py
import time
import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
from django.db.models import Avg, Count, Q, F
from django.utils import timezone
from django.core.cache import cache
from apps.recommendations.models import (
    RecommendationRequest, RecommendationResult, 
    UserInteraction, ExperimentEvent
)
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
import base64

logger = logging.getLogger(__name__)

class PerformanceAnalyzer:
    """ì¶”ì²œ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.cache_prefix = "perf_analysis_"
        self.cache_timeout = 3600  # 1ì‹œê°„
    
    def analyze_recommendation_performance(self, days: int = 30) -> Dict[str, Any]:
        """ì¶”ì²œ ì„±ëŠ¥ ì¢…í•© ë¶„ì„"""
        
        cache_key = f"{self.cache_prefix}recommendation_perf_{days}"
        cached_result = cache.get(cache_key)
        
        if cached_result:
            return cached_result
        
        end_date = timezone.now()
        start_date = end_date - timedelta(days=days)
        
        analysis_result = {
            'period': {
                'start_date': start_date.isoformat(),
                'end_date': end_date.isoformat(),
                'days': days
            },
            'overall_metrics': self._calculate_overall_metrics(start_date, end_date),
            'algorithm_comparison': self._compare_algorithm_performance(start_date, end_date),
            'response_time_analysis': self._analyze_response_times(start_date, end_date),
            'user_engagement_metrics': self._calculate_engagement_metrics(start_date, end_date),
            'recommendation_quality': self._assess_recommendation_quality(start_date, end_date),
            'trends': self._analyze_performance_trends(start_date, end_date)
        }
        
        cache.set(cache_key, analysis_result, self.cache_timeout)
        return analysis_result
    
    def _calculate_overall_metrics(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """ì „ì²´ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        
        # ì¶”ì²œ ìš”ì²­ í†µê³„
        total_requests = RecommendationRequest.objects.filter(
            created__range=[start_date, end_date]
        ).count()
        
        # í‰ê·  ì‘ë‹µ ì‹œê°„
        avg_response_time = RecommendationRequest.objects.filter(
            created__range=[start_date, end_date]
        ).aggregate(avg_time=Avg('response_time_ms'))['avg_time'] or 0
        
        # ì„±ê³µë¥  (ì—ëŸ¬ê°€ ì—†ëŠ” ìš”ì²­ ë¹„ìœ¨)
        successful_requests = RecommendationRequest.objects.filter(
            created__range=[start_date, end_date],
            num_returned__gt=0
        ).count()
        
        success_rate = successful_requests / total_requests if total_requests > 0 else 0
        
        # ìºì‹œ íˆíŠ¸ìœ¨ (ì¶”ì •)
        cache_hits = RecommendationRequest.objects.filter(
            created__range=[start_date, end_date],
            response_time_ms__lt=50  # 50ms ë¯¸ë§Œì„ ìºì‹œ íˆíŠ¸ë¡œ ì¶”ì •
        ).count()
        
        cache_hit_rate = cache_hits / total_requests if total_requests > 0 else 0
        
        # ì‚¬ìš©ì ì°¸ì—¬ë„
        total_interactions = UserInteraction.objects.filter(
            created__range=[start_date, end_date]
        ).count()
        
        unique_users = UserInteraction.objects.filter(
            created__range=[start_date, end_date]
        ).values('user_id').distinct().count()
        
        return {
            'total_requests': total_requests,
            'avg_response_time_ms': float(avg_response_time),
            'success_rate': float(success_rate),
            'cache_hit_rate': float(cache_hit_rate),
            'total_interactions': total_interactions,
            'unique_active_users': unique_users,
            'avg_interactions_per_user': float(total_interactions / unique_users) if unique_users > 0 else 0
        }
    
    def _compare_algorithm_performance(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """ì•Œê³ ë¦¬ì¦˜ë³„ ì„±ëŠ¥ ë¹„êµ"""
        
        # ì•Œê³ ë¦¬ì¦˜ë³„ ìš”ì²­ ìˆ˜ ë° ì‘ë‹µ ì‹œê°„
        algorithm_stats = {}
        
        # RecommendationRequestì— algorithm í•„ë“œê°€ ìˆë‹¤ê³  ê°€ì •
        requests_by_algorithm = RecommendationRequest.objects.filter(
            created__range=[start_date, end_date]
        ).values('algorithm__name').annotate(
            count=Count('id'),
            avg_response_time=Avg('response_time_ms'),
            avg_results=Avg('num_returned')
        )
        
        for stat in requests_by_algorithm:
            algorithm_name = stat['algorithm__name'] or 'unknown'
            
            # í•´ë‹¹ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìƒì„±ëœ ì¶”ì²œì˜ ìƒí˜¸ì‘ìš©ìœ¨ ê³„ì‚°
            algorithm_interactions = self._calculate_algorithm_engagement(
                algorithm_name, start_date, end_date
            )
            
            algorithm_stats[algorithm_name] = {
                'request_count': stat['count'],
                'avg_response_time_ms': float(stat['avg_response_time'] or 0),
                'avg_results_returned': float(stat['avg_results'] or 0),
                'engagement_metrics': algorithm_interactions
            }
        
        return algorithm_stats
    
    def _calculate_algorithm_engagement(self, algorithm: str, start_date: datetime, 
                                     end_date: datetime) -> Dict[str, float]:
        """ì•Œê³ ë¦¬ì¦˜ë³„ ì‚¬ìš©ì ì°¸ì—¬ë„ ê³„ì‚°"""
        
        # í•´ë‹¹ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì¶”ì²œë°›ì€ ìƒí’ˆë“¤ì˜ ìƒí˜¸ì‘ìš© ë¶„ì„
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì¶”ì²œ ê²°ê³¼ì™€ ìƒí˜¸ì‘ìš©ì„ ì—°ê²°í•˜ëŠ” ë¡œì§ í•„ìš”
        
        return {
            'click_through_rate': 0.0,  # ì‹¤ì œ ê³„ì‚° í•„ìš”
            'conversion_rate': 0.0,     # ì‹¤ì œ ê³„ì‚° í•„ìš”
            'engagement_score': 0.0     # ì¢…í•© ì°¸ì—¬ë„ ì ìˆ˜
        }
    
    def _analyze_response_times(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """ì‘ë‹µ ì‹œê°„ ë¶„ì„"""
        
        response_times = list(
            RecommendationRequest.objects.filter(
                created__range=[start_date, end_date]
            ).values_list('response_time_ms', flat=True)
        )
        
        if not response_times:
            return {'error': 'No data available'}
        
        np_times = np.array(response_times)
        
        return {
            'mean': float(np.mean(np_times)),
            'median': float(np.median(np_times)),
            'std': float(np.std(np_times)),
            'min': float(np.min(np_times)),
            'max': float(np.max(np_times)),
            'percentiles': {
                'p50': float(np.percentile(np_times, 50)),
                'p90': float(np.percentile(np_times, 90)),
                'p95': float(np.percentile(np_times, 95)),
                'p99': float(np.percentile(np_times, 99))
            },
            'sla_compliance': {
                'under_100ms': float(np.sum(np_times < 100) / len(np_times)),
                'under_500ms': float(np.sum(np_times < 500) / len(np_times)),
                'under_1000ms': float(np.sum(np_times < 1000) / len(np_times))
            }
        }
    
    def _calculate_engagement_metrics(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì°¸ì—¬ë„ ì§€í‘œ ê³„ì‚°"""
        
        # ìƒí˜¸ì‘ìš© ìœ í˜•ë³„ í†µê³„
        interaction_stats = UserInteraction.objects.filter(
            created__range=[start_date, end_date]
        ).values('interaction_type').annotate(
            count=Count('id')
        )
        
        # ì¼ì¼ í™œì„± ì‚¬ìš©ì
        daily_active_users = []
        current_date = start_date.date()
        end_date_date = end_date.date()
        
        while current_date <= end_date_date:
            day_start = timezone.make_aware(datetime.combine(current_date, datetime.min.time()))
            day_end = day_start + timedelta(days=1)
            
            dau = UserInteraction.objects.filter(
                created__range=[day_start, day_end]
            ).values('user_id').distinct().count()
            
            daily_active_users.append({
                'date': current_date.isoformat(),
                'active_users': dau
            })
            
            current_date += timedelta(days=1)
        
        # ì‚¬ìš©ì ì„¸ì…˜ ë¶„ì„
        session_metrics = self._analyze_user_sessions(start_date, end_date)
        
        return {
            'interaction_breakdown': {
                stat['interaction_type']: stat['count'] 
                for stat in interaction_stats
            },
            'daily_active_users': daily_active_users,
            'session_metrics': session_metrics
        }
    
    def _analyze_user_sessions(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì„¸ì…˜ ë¶„ì„"""
        
        # ì‚¬ìš©ìë³„ ì„¸ì…˜ ê¸¸ì´ ë° ìƒí˜¸ì‘ìš© ìˆ˜ ë¶„ì„
        user_sessions = UserInteraction.objects.filter(
            created__range=[start_date, end_date]
        ).values('user_id').annotate(
            total_interactions=Count('id'),
            session_duration=F('created__max') - F('created__min')
        )
        
        if not user_sessions:
            return {}
        
        # ì„¸ì…˜ë‹¹ ìƒí˜¸ì‘ìš© ìˆ˜ ë¶„í¬
        interactions_per_session = [session['total_interactions'] for session in user_sessions]
        
        return {
            'avg_interactions_per_session': float(np.mean(interactions_per_session)),
            'median_interactions_per_session': float(np.median(interactions_per_session)),
            'sessions_with_multiple_interactions': len([x for x in interactions_per_session if x > 1])
        }
    
    def _assess_recommendation_quality(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """ì¶”ì²œ í’ˆì§ˆ í‰ê°€"""
        
        # ì¶”ì²œ ë‹¤ì–‘ì„± ë¶„ì„
        diversity_metrics = self._calculate_recommendation_diversity(start_date, end_date)
        
        # ì»¤ë²„ë¦¬ì§€ ë¶„ì„
        coverage_metrics = self._calculate_catalog_coverage(start_date, end_date)
        
        # ì‹ ê·œì„± ë¶„ì„
        novelty_metrics = self._calculate_recommendation_novelty(start_date, end_date)
        
        return {
            'diversity': diversity_metrics,
            'coverage': coverage_metrics,
            'novelty': novelty_metrics
        }
    
    def _calculate_recommendation_diversity(self, start_date: datetime, end_date: datetime) -> Dict[str, float]:
        """ì¶”ì²œ ë‹¤ì–‘ì„± ê³„ì‚°"""
        
        # ì¶”ì²œëœ ìƒí’ˆë“¤ì˜ ì¹´í…Œê³ ë¦¬/ë¸Œëœë“œ ë¶„í¬ ë¶„ì„
        recommendations = RecommendationResult.objects.filter(
            request__created__range=[start_date, end_date]
        ).select_related('product')
        
        if not recommendations:
            return {'category_diversity': 0.0, 'brand_diversity': 0.0}
        
        # ì¹´í…Œê³ ë¦¬ ë¶„í¬
        categories = [rec.product.category_id for rec in recommendations if rec.product.category_id]
        category_diversity = len(set(categories)) / len(categories) if categories else 0
        
        # ë¸Œëœë“œ ë¶„í¬
        brands = [rec.product.brand_id for rec in recommendations if rec.product.brand_id]
        brand_diversity = len(set(brands)) / len(brands) if brands else 0
        
        return {
            'category_diversity': float(category_diversity),
            'brand_diversity': float(brand_diversity)
        }
    
    def _calculate_catalog_coverage(self, start_date: datetime, end_date: datetime) -> Dict[str, float]:
        """ì¹´íƒˆë¡œê·¸ ì»¤ë²„ë¦¬ì§€ ê³„ì‚°"""
        
        from apps.products.models import Product
        
        # ì „ì²´ í™œì„± ìƒí’ˆ ìˆ˜
        total_active_products = Product.objects.filter(is_active=True).count()
        
        # ì¶”ì²œëœ ê³ ìœ  ìƒí’ˆ ìˆ˜
        recommended_products = RecommendationResult.objects.filter(
            request__created__range=[start_date, end_date]
        ).values('product_id').distinct().count()
        
        coverage = recommended_products / total_active_products if total_active_products > 0 else 0
        
        return {
            'total_products': total_active_products,
            'recommended_products': recommended_products,
            'coverage_rate': float(coverage)
        }
    
    def _calculate_recommendation_novelty(self, start_date: datetime, end_date: datetime) -> Dict[str, float]:
        """ì¶”ì²œ ì‹ ê·œì„± ê³„ì‚°"""
        
        # ì¸ê¸°ë„ê°€ ë‚®ì€ ìƒí’ˆë“¤ì´ ì–¼ë§ˆë‚˜ ì¶”ì²œë˜ì—ˆëŠ”ì§€ ë¶„ì„
        recommendations = RecommendationResult.objects.filter(
            request__created__range=[start_date, end_date]
        ).select_related('product')
        
        if not recommendations:
            return {'novelty_score': 0.0}
        
        # ìƒí’ˆ ì¸ê¸°ë„ ì—­ìˆ˜ì˜ í‰ê· ìœ¼ë¡œ ì‹ ê·œì„± ì ìˆ˜ ê³„ì‚°
        novelty_scores = []
        for rec in recommendations:
            product = rec.product
            popularity = max(product.view_count, 1)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            novelty_score = 1.0 / popularity
            novelty_scores.append(novelty_score)
        
        avg_novelty = np.mean(novelty_scores) if novelty_scores else 0
        
        return {
            'novelty_score': float(avg_novelty),
            'long_tail_percentage': float(len([s for s in novelty_scores if s > 0.001]) / len(novelty_scores))
        }
    
    def _analyze_performance_trends(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„"""
        
        # ì¼ë³„ ì„±ëŠ¥ ì§€í‘œ ì¶”ì´
        daily_metrics = []
        current_date = start_date.date()
        end_date_date = end_date.date()
        
        while current_date <= end_date_date:
            day_start = timezone.make_aware(datetime.combine(current_date, datetime.min.time()))
            day_end = day_start + timedelta(days=1)
            
            day_requests = RecommendationRequest.objects.filter(
                created__range=[day_start, day_end]
            )
            
            day_interactions = UserInteraction.objects.filter(
                created__range=[day_start, day_end]
            )
            
            daily_metric = {
                'date': current_date.isoformat(),
                'requests_count': day_requests.count(),
                'avg_response_time': day_requests.aggregate(
                    avg=Avg('response_time_ms')
                )['avg'] or 0,
                'interactions_count': day_interactions.count(),
                'unique_users': day_interactions.values('user_id').distinct().count()
            }
            
            daily_metrics.append(daily_metric)
            current_date += timedelta(days=1)
        
        return {
            'daily_metrics': daily_metrics,
            'trend_analysis': self._calculate_trends(daily_metrics)
        }
    
    def _calculate_trends(self, daily_metrics: List[Dict]) -> Dict[str, str]:
        """íŠ¸ë Œë“œ ë°©í–¥ ê³„ì‚°"""
        
        if len(daily_metrics) < 2:
            return {}
        
        # ê°„ë‹¨í•œ íŠ¸ë Œë“œ ë¶„ì„ (ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ë¹„êµ)
        first_day = daily_metrics[0]
        last_day = daily_metrics[-1]
        
        trends = {}
        
        for metric in ['requests_count', 'avg_response_time', 'interactions_count', 'unique_users']:
            first_value = first_day.get(metric, 0)
            last_value = last_day.get(metric, 0)
            
            if first_value == 0:
                trends[metric] = 'stable'
            else:
                change_percent = ((last_value - first_value) / first_value) * 100
                
                if change_percent > 5:
                    trends[metric] = 'increasing'
                elif change_percent < -5:
                    trends[metric] = 'decreasing'
                else:
                    trends[metric] = 'stable'
        
        return trends
    
    def generate_performance_report(self, days: int = 30) -> str:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        analysis = self.analyze_recommendation_performance(days)
        
        report_lines = [
            f"# Recommendation System Performance Report",
            f"**Analysis Period:** {analysis['period']['start_date'][:10]} to {analysis['period']['end_date'][:10]}",
            "",
            "## Overall Metrics",
            f"- Total Requests: {analysis['overall_metrics']['total_requests']:,}",
            f"- Average Response Time: {analysis['overall_metrics']['avg_response_time_ms']:.2f}ms",
            f"- Success Rate: {analysis['overall_metrics']['success_rate']:.2%}",
            f"- Cache Hit Rate: {analysis['overall_metrics']['cache_hit_rate']:.2%}",
            f"- Unique Active Users: {analysis['overall_metrics']['unique_active_users']:,}",
            "",
            "## Response Time Analysis",
        ]
        
        rt_analysis = analysis['response_time_analysis']
        if 'error' not in rt_analysis:
            report_lines.extend([
                f"- Median: {rt_analysis['median']:.2f}ms",
                f"- 95th Percentile: {rt_analysis['percentiles']['p95']:.2f}ms",
                f"- 99th Percentile: {rt_analysis['percentiles']['p99']:.2f}ms",
                f"- SLA Compliance (<500ms): {rt_analysis['sla_compliance']['under_500ms']:.2%}",
            ])
        
        report_lines.extend([
            "",
            "## Quality Metrics",
            f"- Catalog Coverage: {analysis['recommendation_quality']['coverage']['coverage_rate']:.2%}",
            f"- Category Diversity: {analysis['recommendation_quality']['diversity']['category_diversity']:.2%}",
            f"- Brand Diversity: {analysis['recommendation_quality']['diversity']['brand_diversity']:.2%}",
        ])
        
        return "\n".join(report_lines)

# ì„±ëŠ¥ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤
performance_analyzer = PerformanceAnalyzer()
```

### 7.3 ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

```python
# apps/recommendations/dashboard.py
from django.shortcuts import render
from django.http import JsonResponse
from django.views.decorators.cache import cache_page
from django.utils.decorators import method_decorator
from django.views import View
from django.contrib.admin.views.decorators import staff_member_required
from django.utils import timezone
from datetime import timedelta
import json
from apps.recommendations.performance_analyzer import performance_analyzer
from apps.recommendations.ab_testing import ab_test_manager
from apps.recommendations.monitoring import metrics

class RecommendationDashboardView(View):
    """ì¶”ì²œ ì‹œìŠ¤í…œ ëŒ€ì‹œë³´ë“œ"""
    
    @method_decorator(staff_member_required)
    def get(self, request):
        """ëŒ€ì‹œë³´ë“œ í˜ì´ì§€ ë Œë”ë§"""
        
        # ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸
        context = {
            'title': 'Recommendation System Dashboard',
            'current_time': timezone.now(),
            'refresh_interval': 30000  # 30ì´ˆë§ˆë‹¤ ìë™ ìƒˆë¡œê³ ì¹¨
        }
        
        return render(request, 'recommendations/dashboard.html', context)

class DashboardAPIView(View):
    """ëŒ€ì‹œë³´ë“œ API ì—”ë“œí¬ì¸íŠ¸"""
    
    @method_decorator(cache_page(60))  # 1ë¶„ ìºì‹œ
    def get(self, request):
        """ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ë°ì´í„° ì œê³µ"""
        
        try:
            # ì‹œê°„ ë²”ìœ„ íŒŒë¼ë¯¸í„°
            hours = int(request.GET.get('hours', 24))
            
            # ì‹¤ì‹œê°„ ì„±ëŠ¥ ë¶„ì„
            performance_data = performance_analyzer.analyze_recommendation_performance(hours // 24)
            
            # A/B í…ŒìŠ¤íŠ¸ í˜„í™©
            ab_test_data = self._get_active_experiments()
            
            # ì‹œìŠ¤í…œ ìƒíƒœ
            system_health = metrics.metrics.get_performance_summary(hours)
            
            # ì•Œë¦¼ ë° ê²½ê³ 
            alerts = self._generate_alerts(performance_data)
            
            dashboard_data = {
                'timestamp': timezone.now().isoformat(),
                'performance': performance_data,
                'ab_tests': ab_test_data,
                'system_health': system_health,
                'alerts': alerts,
                'real_time_metrics': self._get_real_time_metrics()
            }
            
            return JsonResponse(dashboard_data)
            
        except Exception as e:
            return JsonResponse({
                'error': str(e),
                'timestamp': timezone.now().isoformat()
            }, status=500)
    
    def _get_active_experiments(self):
        """í™œì„± A/B í…ŒìŠ¤íŠ¸ ì¡°íšŒ"""
        
        from apps.recommendations.models import Experiment, ExperimentStatus
        
        active_experiments = Experiment.objects.filter(
            status=ExperimentStatus.ACTIVE,
            start_date__lte=timezone.now(),
            end_date__gte=timezone.now()
        )
        
        experiments_data = []
        
        for experiment in active_experiments:
            # ì‹¤í—˜ ë¶„ì„
            analysis = ab_test_manager.analyze_experiment(experiment.id)
            
            experiments_data.append({
                'id': experiment.id,
                'name': experiment.name,
                'type': experiment.experiment_type,
                'start_date': experiment.start_date.isoformat(),
                'end_date': experiment.end_date.isoformat(),
                'progress': self._calculate_experiment_progress(experiment),
                'preliminary_results': analysis.get('significance_tests', {}),
                'sample_size': sum(
                    stats.get('total_users', 0) 
                    for stats in analysis.get('variant_statistics', {}).values()
                )
            })
        
        return experiments_data
    
    def _calculate_experiment_progress(self, experiment):
        """ì‹¤í—˜ ì§„í–‰ë¥  ê³„ì‚°"""
        
        now = timezone.now()
        total_duration = experiment.end_date - experiment.start_date
        elapsed_duration = now - experiment.start_date
        
        if elapsed_duration.total_seconds() <= 0:
            return 0.0
        
        progress = elapsed_duration.total_seconds() / total_duration.total_seconds()
        return min(100.0, max(0.0, progress * 100))
    
    def _generate_alerts(self, performance_data):
        """ì„±ëŠ¥ ê¸°ë°˜ ì•Œë¦¼ ìƒì„±"""
        
        alerts = []
        
        # ì‘ë‹µ ì‹œê°„ ì•Œë¦¼
        if 'response_time_analysis' in performance_data:
            rt_data = performance_data['response_time_analysis']
            if 'percentiles' in rt_data:
                p95_time = rt_data['percentiles']['p95']
                if p95_time > 1000:  # 1ì´ˆ ì´ˆê³¼
                    alerts.append({
                        'level': 'error',
                        'message': f'High response time: P95 = {p95_time:.0f}ms',
                        'metric': 'response_time',
                        'value': p95_time,
                        'threshold': 1000
                    })
                elif p95_time > 500:  # 500ms ì´ˆê³¼
                    alerts.append({
                        'level': 'warning',
                        'message': f'Elevated response time: P95 = {p95_time:.0f}ms',
                        'metric': 'response_time',
                        'value': p95_time,
                        'threshold': 500
                    })
        
        # ì„±ê³µë¥  ì•Œë¦¼
        if 'overall_metrics' in performance_data:
            overall = performance_data['overall_metrics']
            success_rate = overall.get('success_rate', 1.0)
            
            if success_rate < 0.95:  # 95% ë¯¸ë§Œ
                alerts.append({
                    'level': 'error' if success_rate < 0.90 else 'warning',
                    'message': f'Low success rate: {success_rate:.1%}',
                    'metric': 'success_rate',
                    'value': success_rate,
                    'threshold': 0.95
                })
        
        # ì¶”ì²œ í’ˆì§ˆ ì•Œë¦¼
        if 'recommendation_quality' in performance_data:
            quality = performance_data['recommendation_quality']
            coverage = quality.get('coverage', {}).get('coverage_rate', 0)
            
            if coverage < 0.1:  # 10% ë¯¸ë§Œ ì»¤ë²„ë¦¬ì§€
                alerts.append({
                    'level': 'warning',
                    'message': f'Low catalog coverage: {coverage:.1%}',
                    'metric': 'coverage',
                    'value': coverage,
                    'threshold': 0.1
                })
        
        return alerts
    
    def _get_real_time_metrics(self):
        """ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        
        from django.core.cache import cache
        from apps.recommendations.models import RecommendationRequest
        
        # ìµœê·¼ 5ë¶„ê°„ì˜ ë©”íŠ¸ë¦­
        five_minutes_ago = timezone.now() - timedelta(minutes=5)
        
        recent_requests = RecommendationRequest.objects.filter(
            created__gte=five_minutes_ago
        )
        
        return {
            'requests_last_5min': recent_requests.count(),
            'avg_response_time_5min': recent_requests.aggregate(
                avg=models.Avg('response_time_ms')
            )['avg'] or 0,
            'cache_status': self._check_cache_status(),
            'active_sessions': self._estimate_active_sessions()
        }
    
    def _check_cache_status(self):
        """ìºì‹œ ìƒíƒœ í™•ì¸"""
        
        try:
            from django.core.cache import cache
            
            # í…ŒìŠ¤íŠ¸ í‚¤ë¡œ ìºì‹œ ìƒíƒœ í™•ì¸
            test_key = f"cache_health_check_{timezone.now().timestamp()}"
            cache.set(test_key, "ok", 10)
            
            if cache.get(test_key) == "ok":
                cache.delete(test_key)
                return "healthy"
            else:
                return "unhealthy"
                
        except Exception:
            return "error"
    
    def _estimate_active_sessions(self):
        """í™œì„± ì„¸ì…˜ ìˆ˜ ì¶”ì •"""
        
        # ìµœê·¼ 30ë¶„ê°„ í™œë™í•œ ê³ ìœ  ì‚¬ìš©ì ìˆ˜ë¡œ ì¶”ì •
        thirty_minutes_ago = timezone.now() - timedelta(minutes=30)
        
        from apps.recommendations.models import UserInteraction
        
        active_users = UserInteraction.objects.filter(
            created__gte=thirty_minutes_ago
        ).values('user_id').distinct().count()
        
        return active_users

# URL ì„¤ì •
# urls.py
from django.urls import path
from .dashboard import RecommendationDashboardView, DashboardAPIView

urlpatterns = [
    path('dashboard/', RecommendationDashboardView.as_view(), name='recommendation_dashboard'),
    path('dashboard/api/', DashboardAPIView.as_view(), name='dashboard_api'),
]
```

---

## ğŸš€ 8. ë°°í¬ ë° ìš´ì˜ ê°€ì´ë“œ

í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ ì•ˆì •ì ì¸ ì¶”ì²œ ì‹œìŠ¤í…œ ë°°í¬ì™€ ìš´ì˜ì„ ìœ„í•œ ì¢…í•© ê°€ì´ë“œì…ë‹ˆë‹¤.

### 8.1 Docker ì»¨í…Œì´ë„ˆí™”

```dockerfile
# Dockerfile
FROM python:3.11-slim

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    libc6-dev \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /app

# Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY . .

# ì •ì  íŒŒì¼ ìˆ˜ì§‘
RUN python manage.py collectstatic --noinput

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000

# í—¬ìŠ¤ì²´í¬ ì„¤ì •
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/recommendations/health || exit 1

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
CMD ["gunicorn", "--bind", "0.0.0.0:8000", "--workers", "4", "config.wsgi:application"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DEBUG=False
      - DATABASE_URL=postgresql://postgres:password@db:5432/recommendations
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/1
    volumes:
      - ./media:/app/media
      - ./logs:/app/logs
    depends_on:
      - db
      - redis
    restart: unless-stopped

  db:
    image: postgres:15
    environment:
      - POSTGRES_DB=recommendations
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./db_init:/docker-entrypoint-initdb.d
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  celery:
    build: .
    command: celery -A config worker -l info
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/recommendations
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/1
    volumes:
      - ./logs:/app/logs
    depends_on:
      - db
      - redis
    restart: unless-stopped

  celery-beat:
    build: .
    command: celery -A config beat -l info
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/recommendations
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/1
    volumes:
      - ./logs:/app/logs
    depends_on:
      - db
      - redis
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/ssl/certs
      - ./media:/app/media
    depends_on:
      - app
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
```

```nginx
# nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream app {
        server app:8000;
    }

    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=100r/m;
    limit_req_zone $binary_remote_addr zone=recommendations:10m rate=30r/m;

    server {
        listen 80;
        server_name localhost;
        
        # Redirect to HTTPS in production
        # return 301 https://$server_name$request_uri;

        client_max_body_size 10M;
        
        # Static files
        location /static/ {
            alias /app/static/;
            expires 1y;
            add_header Cache-Control "public, immutable";
        }
        
        location /media/ {
            alias /app/media/;
            expires 1y;
            add_header Cache-Control "public, immutable";
        }

        # Health check (no rate limiting)
        location /recommendations/health {
            proxy_pass http://app;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
        }

        # API endpoints with rate limiting
        location /api/ {
            limit_req zone=api burst=20 nodelay;
            
            proxy_pass http://app;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Timeout settings
            proxy_connect_timeout 5s;
            proxy_send_timeout 10s;
            proxy_read_timeout 30s;
        }

        # Recommendation endpoints with stricter rate limiting
        location /recommendations/ {
            limit_req zone=recommendations burst=10 nodelay;
            
            proxy_pass http://app;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # WebSocket proxy for real-time recommendations
        location /ws/ {
            proxy_pass http://app;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
        }

        # Default proxy
        location / {
            proxy_pass http://app;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
    }
}
```

### 8.2 Kubernetes ë°°í¬

```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: recommendation-system

---
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: recommendation-system
data:
  DEBUG: "False"
  ALLOWED_HOSTS: "*"
  DATABASE_HOST: "postgresql"
  REDIS_HOST: "redis"

---
# k8s/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: app-secrets
  namespace: recommendation-system
type: Opaque
data:
  SECRET_KEY: <base64-encoded-secret-key>
  DATABASE_PASSWORD: <base64-encoded-db-password>

---
# k8s/postgresql.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgresql
  namespace: recommendation-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgresql
  template:
    metadata:
      labels:
        app: postgresql
    spec:
      containers:
      - name: postgresql
        image: postgres:15
        env:
        - name: POSTGRES_DB
          value: recommendations
        - name: POSTGRES_USER
          value: postgres
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: app-secrets
              key: DATABASE_PASSWORD
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
      volumes:
      - name: postgres-storage
        persistentVolumeClaim:
          claimName: postgres-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: postgresql
  namespace: recommendation-system
spec:
  selector:
    app: postgresql
  ports:
  - port: 5432
    targetPort: 5432

---
# k8s/redis.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: recommendation-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
        volumeMounts:
        - name: redis-storage
          mountPath: /data
      volumes:
      - name: redis-storage
        persistentVolumeClaim:
          claimName: redis-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: recommendation-system
spec:
  selector:
    app: redis
  ports:
  - port: 6379
    targetPort: 6379

---
# k8s/app.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: recommendation-app
  namespace: recommendation-system
spec:
  replicas: 3
  selector:
    matchLabels:
      app: recommendation-app
  template:
    metadata:
      labels:
        app: recommendation-app
    spec:
      containers:
      - name: app
        image: recommendation-system:latest
        ports:
        - containerPort: 8000
        env:
        - name: SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: app-secrets
              key: SECRET_KEY
        - name: DATABASE_URL
          value: "postgresql://postgres:$(DATABASE_PASSWORD)@postgresql:5432/recommendations"
        - name: REDIS_URL
          value: "redis://redis:6379/0"
        envFrom:
        - configMapRef:
            name: app-config
        livenessProbe:
          httpGet:
            path: /recommendations/health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /recommendations/health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"

---
apiVersion: v1
kind: Service
metadata:
  name: recommendation-app-service
  namespace: recommendation-system
spec:
  selector:
    app: recommendation-app
  ports:
  - port: 80
    targetPort: 8000
  type: ClusterIP

---
# k8s/celery.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: celery-worker
  namespace: recommendation-system
spec:
  replicas: 2
  selector:
    matchLabels:
      app: celery-worker
  template:
    metadata:
      labels:
        app: celery-worker
    spec:
      containers:
      - name: celery-worker
        image: recommendation-system:latest
        command: ["celery", "-A", "config", "worker", "-l", "info"]
        env:
        - name: DATABASE_URL
          value: "postgresql://postgres:$(DATABASE_PASSWORD)@postgresql:5432/recommendations"
        - name: CELERY_BROKER_URL
          value: "redis://redis:6379/1"
        envFrom:
        - configMapRef:
            name: app-config
        - secretKeyRef:
            name: app-secrets
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"

---
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: recommendation-ingress
  namespace: recommendation-system
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/rate-limit-window: "1m"
spec:
  rules:
  - host: recommendations.yourdomain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: recommendation-app-service
            port:
              number: 80

---
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: recommendation-app-hpa
  namespace: recommendation-system
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: recommendation-app
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

### 8.3 ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

```python
# monitoring/prometheus.py
import time
import logging
from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST
from django.http import HttpResponse
from django.views import View
from functools import wraps

# Prometheus ë©”íŠ¸ë¦­ ì •ì˜
RECOMMENDATION_REQUESTS = Counter(
    'recommendation_requests_total',
    'Total recommendation requests',
    ['algorithm', 'status']
)

RECOMMENDATION_DURATION = Histogram(
    'recommendation_duration_seconds',
    'Time spent processing recommendations',
    ['algorithm']
)

ACTIVE_USERS = Gauge(
    'active_users_current',
    'Current number of active users'
)

CACHE_HIT_RATE = Gauge(
    'cache_hit_rate',
    'Current cache hit rate'
)

AB_TEST_ASSIGNMENTS = Counter(
    'ab_test_assignments_total',
    'Total A/B test assignments',
    ['experiment', 'variant']
)

logger = logging.getLogger(__name__)

def track_recommendations(algorithm_name: str):
    """ì¶”ì²œ ë©”íŠ¸ë¦­ ì¶”ì  ë°ì½”ë ˆì´í„°"""
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            status = 'success'
            
            try:
                result = func(*args, **kwargs)
                return result
            except Exception as e:
                status = 'error'
                raise
            finally:
                duration = time.time() - start_time
                
                RECOMMENDATION_REQUESTS.labels(
                    algorithm=algorithm_name, 
                    status=status
                ).inc()
                
                RECOMMENDATION_DURATION.labels(
                    algorithm=algorithm_name
                ).observe(duration)
        
        return wrapper
    return decorator

class PrometheusMetricsView(View):
    """Prometheus ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸"""
    
    def get(self, request):
        """ë©”íŠ¸ë¦­ ë°ì´í„° ë°˜í™˜"""
        
        # ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self._update_real_time_metrics()
        
        # Prometheus í˜•ì‹ìœ¼ë¡œ ë©”íŠ¸ë¦­ ìƒì„±
        metrics_data = generate_latest()
        
        return HttpResponse(
            metrics_data,
            content_type=CONTENT_TYPE_LATEST
        )
    
    def _update_real_time_metrics(self):
        """ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        
        try:
            from django.utils import timezone
            from datetime import timedelta
            from apps.recommendations.models import UserInteraction
            
            # í™œì„± ì‚¬ìš©ì ìˆ˜ (ìµœê·¼ 30ë¶„)
            thirty_minutes_ago = timezone.now() - timedelta(minutes=30)
            active_users_count = UserInteraction.objects.filter(
                created__gte=thirty_minutes_ago
            ).values('user_id').distinct().count()
            
            ACTIVE_USERS.set(active_users_count)
            
            # ìºì‹œ íˆíŠ¸ìœ¨ ê³„ì‚°
            from django.core.cache import cache
            cache_stats = self._calculate_cache_hit_rate()
            CACHE_HIT_RATE.set(cache_stats)
            
        except Exception as e:
            logger.error(f"Error updating real-time metrics: {str(e)}")
    
    def _calculate_cache_hit_rate(self) -> float:
        """ìºì‹œ íˆíŠ¸ìœ¨ ê³„ì‚°"""
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Redis INFO ëª…ë ¹ì´ë‚˜ ìºì‹œ í†µê³„ ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œ
        try:
            from django.core.cache import cache
            
            # í…ŒìŠ¤íŠ¸ í‚¤ë“¤ë¡œ ìºì‹œ ìƒíƒœ í™•ì¸
            test_keys = [f"cache_test_{i}" for i in range(10)]
            hits = 0
            
            for key in test_keys:
                cache.set(key, "test", 60)
                if cache.get(key) == "test":
                    hits += 1
                cache.delete(key)
            
            return hits / len(test_keys)
            
        except Exception:
            return 0.0
```

```yaml
# monitoring/grafana-dashboard.json
{
  "dashboard": {
    "id": null,
    "title": "Recommendation System Dashboard",
    "tags": ["recommendations", "ml"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Recommendation Requests Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(recommendation_requests_total[5m])",
            "legendFormat": "{{algorithm}} - {{status}}"
          }
        ],
        "yAxes": [
          {
            "label": "Requests/sec"
          }
        ]
      },
      {
        "id": 2,
        "title": "Response Time Percentiles",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, rate(recommendation_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          },
          {
            "expr": "histogram_quantile(0.90, rate(recommendation_duration_seconds_bucket[5m]))",
            "legendFormat": "90th percentile"
          },
          {
            "expr": "histogram_quantile(0.99, rate(recommendation_duration_seconds_bucket[5m]))",
            "legendFormat": "99th percentile"
          }
        ]
      },
      {
        "id": 3,
        "title": "Active Users",
        "type": "singlestat",
        "targets": [
          {
            "expr": "active_users_current"
          }
        ]
      },
      {
        "id": 4,
        "title": "Cache Hit Rate",
        "type": "singlestat",
        "targets": [
          {
            "expr": "cache_hit_rate * 100"
          }
        ],
        "postfix": "%"
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "10s"
  }
}
```

### 8.4 CI/CD íŒŒì´í”„ë¼ì¸

```yaml
# .github/workflows/ci-cd.yml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}/recommendation-system

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_recommendations
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Run linting
      run: |
        flake8 apps/
        black --check apps/
        isort --check-only apps/
    
    - name: Run tests
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_recommendations
        REDIS_URL: redis://localhost:6379/0
        SECRET_KEY: test-secret-key
      run: |
        python manage.py test
        
    - name: Run security checks
      run: |
        bandit -r apps/
        safety check

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
    
    - name: Log in to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v4
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha
          type=raw,value=latest,enable={{is_default_branch}}
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'latest'
    
    - name: Configure kubectl
      run: |
        echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > kubeconfig
        export KUBECONFIG=kubeconfig
    
    - name: Deploy to Kubernetes
      env:
        KUBECONFIG: kubeconfig
        IMAGE_TAG: ${{ github.sha }}
      run: |
        # Update image tag in deployment
        sed -i "s|recommendation-system:latest|${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.IMAGE_TAG }}|g" k8s/app.yaml
        
        # Apply Kubernetes manifests
        kubectl apply -f k8s/
        
        # Wait for deployment to complete
        kubectl rollout status deployment/recommendation-app -n recommendation-system
    
    - name: Run post-deployment tests
      env:
        KUBECONFIG: kubeconfig
      run: |
        # Health check
        kubectl exec -n recommendation-system deployment/recommendation-app -- curl -f http://localhost:8000/recommendations/health
        
        # Basic API test
        APP_URL=$(kubectl get ingress recommendation-ingress -n recommendation-system -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
        curl -f http://$APP_URL/recommendations/health

  performance-test:
    needs: deploy
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Run load tests
      run: |
        # Install k6
        curl https://github.com/grafana/k6/releases/download/v0.46.0/k6-v0.46.0-linux-amd64.tar.gz -L | tar xvz --strip-components 1
        
        # Run performance tests
        ./k6 run --vus 50 --duration 5m performance-tests/load-test.js
```

```javascript
// performance-tests/load-test.js
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate } from 'k6/metrics';

// ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­
export const errorRate = new Rate('errors');

export const options = {
  stages: [
    { duration: '2m', target: 10 }, // ì›Œë°ì—…
    { duration: '5m', target: 50 }, // ì •ìƒ ë¶€í•˜
    { duration: '2m', target: 100 }, // ìŠ¤íŒŒì´í¬ í…ŒìŠ¤íŠ¸
    { duration: '1m', target: 0 }, // ì¿¨ë‹¤ìš´
  ],
  thresholds: {
    http_req_duration: ['p(95)<1000'], // 95% of requests under 1s
    http_req_failed: ['rate<0.1'], // Error rate under 10%
    errors: ['rate<0.1'],
  },
};

const BASE_URL = __ENV.BASE_URL || 'http://localhost:8000';

export default function() {
  // í—¬ìŠ¤ ì²´í¬
  let healthResponse = http.get(`${BASE_URL}/recommendations/health`);
  check(healthResponse, {
    'health check status is 200': (r) => r.status === 200,
  });

  // ì¶”ì²œ API í…ŒìŠ¤íŠ¸
  const recommendationPayload = JSON.stringify({
    num_recommendations: 10,
    strategy: 'weighted',
    context: {
      test_load: true
    }
  });

  const params = {
    headers: {
      'Content-Type': 'application/json',
      'Authorization': 'Bearer test-token',
    },
  };

  let recommendationResponse = http.post(
    `${BASE_URL}/api/recommendations`,
    recommendationPayload,
    params
  );

  const success = check(recommendationResponse, {
    'recommendation status is 200': (r) => r.status === 200,
    'recommendation response time < 2s': (r) => r.timings.duration < 2000,
    'has recommendations': (r) => {
      const body = JSON.parse(r.body);
      return body.recommendations && body.recommendations.length > 0;
    },
  });

  errorRate.add(!success);

  sleep(1);
}
```

### 8.5 ìš´ì˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

```python
# ops/health_checks.py
from django.core.management.base import BaseCommand
from django.db import connection
from django.core.cache import cache
from apps.recommendations.services import RecommendationService
from apps.recommendations.models import RecommendationRequest
from django.utils import timezone
from datetime import timedelta
import requests
import logging

logger = logging.getLogger(__name__)

class Command(BaseCommand):
    help = 'Run comprehensive health checks for the recommendation system'
    
    def add_arguments(self, parser):
        parser.add_argument('--detailed', action='store_true', help='Run detailed checks')
        parser.add_argument('--fix', action='store_true', help='Attempt to fix issues')
    
    def handle(self, *args, **options):
        self.detailed = options['detailed']
        self.fix_issues = options['fix']
        
        checks = [
            ('Database Connection', self.check_database),
            ('Redis Connection', self.check_redis),
            ('Recommendation Service', self.check_recommendation_service),
            ('API Endpoints', self.check_api_endpoints),
            ('Celery Workers', self.check_celery_workers),
            ('System Resources', self.check_system_resources),
            ('Data Quality', self.check_data_quality),
        ]
        
        if self.detailed:
            checks.extend([
                ('Model Performance', self.check_model_performance),
                ('A/B Test Status', self.check_ab_tests),
                ('Cache Performance', self.check_cache_performance),
            ])
        
        results = []
        
        for check_name, check_func in checks:
            self.stdout.write(f"Running {check_name}...")
            try:
                result = check_func()
                status = "âœ“ PASS" if result['status'] == 'ok' else "âœ— FAIL"
                self.stdout.write(f"{status}: {result['message']}")
                results.append((check_name, result))
            except Exception as e:
                self.stdout.write(f"âœ— ERROR: {str(e)}")
                results.append((check_name, {'status': 'error', 'message': str(e)}))
        
        # ê²°ê³¼ ìš”ì•½
        passed = sum(1 for _, result in results if result['status'] == 'ok')
        total = len(results)
        
        self.stdout.write(f"\n=== Health Check Summary ===")
        self.stdout.write(f"Passed: {passed}/{total}")
        
        if passed < total:
            self.stdout.write("Issues detected. Check logs for details.")
            return 1
        else:
            self.stdout.write("All checks passed!")
            return 0
    
    def check_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° ìƒíƒœ í™•ì¸"""
        try:
            with connection.cursor() as cursor:
                cursor.execute("SELECT 1")
                
            # ìµœê·¼ ë°ì´í„° í™•ì¸
            recent_requests = RecommendationRequest.objects.filter(
                created__gte=timezone.now() - timedelta(hours=1)
            ).count()
            
            return {
                'status': 'ok',
                'message': f'Database connection OK. Recent requests: {recent_requests}'
            }
            
        except Exception as e:
            return {'status': 'fail', 'message': f'Database error: {str(e)}'}
    
    def check_redis(self):
        """Redis ì—°ê²° ë° ì„±ëŠ¥ í™•ì¸"""
        try:
            cache.set('health_check', 'ok', 10)
            value = cache.get('health_check')
            
            if value != 'ok':
                return {'status': 'fail', 'message': 'Redis cache not working'}
            
            # Redis ì •ë³´ í™•ì¸ (ì„ íƒì )
            if self.detailed:
                # Redis í†µê³„ ì •ë³´ ìˆ˜ì§‘
                pass
            
            return {'status': 'ok', 'message': 'Redis connection OK'}
            
        except Exception as e:
            return {'status': 'fail', 'message': f'Redis error: {str(e)}'}
    
    def check_recommendation_service(self):
        """ì¶”ì²œ ì„œë¹„ìŠ¤ ê¸°ëŠ¥ í™•ì¸"""
        try:
            service = RecommendationService()
            
            # í…ŒìŠ¤íŠ¸ ì‚¬ìš©ìë¡œ ì¶”ì²œ ìƒì„±
            test_user_id = 1
            recommendations = service.get_recommendations(
                user_id=test_user_id,
                num_recommendations=5
            )
            
            if not recommendations or len(recommendations.get('recommendations', [])) == 0:
                return {'status': 'fail', 'message': 'No recommendations generated'}
            
            response_time = recommendations.get('response_time_ms', 0)
            if response_time > 5000:  # 5ì´ˆ ì´ˆê³¼
                return {
                    'status': 'warn', 
                    'message': f'Slow response time: {response_time}ms'
                }
            
            return {
                'status': 'ok',
                'message': f'Recommendation service OK. Response time: {response_time}ms'
            }
            
        except Exception as e:
            return {'status': 'fail', 'message': f'Recommendation service error: {str(e)}'}
    
    def check_api_endpoints(self):
        """API ì—”ë“œí¬ì¸íŠ¸ ìƒíƒœ í™•ì¸"""
        try:
            # í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
            import subprocess
            result = subprocess.run(
                ['curl', '-f', 'http://localhost:8000/recommendations/health'],
                capture_output=True,
                timeout=10
            )
            
            if result.returncode != 0:
                return {'status': 'fail', 'message': 'Health endpoint not responding'}
            
            return {'status': 'ok', 'message': 'API endpoints responding'}
            
        except subprocess.TimeoutExpired:
            return {'status': 'fail', 'message': 'API endpoint timeout'}
        except Exception as e:
            return {'status': 'fail', 'message': f'API check error: {str(e)}'}
    
    def check_celery_workers(self):
        """Celery ì›Œì»¤ ìƒíƒœ í™•ì¸"""
        try:
            from celery import current_app
            
            inspect = current_app.control.inspect()
            stats = inspect.stats()
            
            if not stats:
                return {'status': 'fail', 'message': 'No Celery workers found'}
            
            active_workers = len(stats)
            return {
                'status': 'ok',
                'message': f'Celery workers active: {active_workers}'
            }
            
        except Exception as e:
            return {'status': 'fail', 'message': f'Celery check error: {str(e)}'}
    
    def check_system_resources(self):
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸"""
        try:
            import psutil
            
            # CPU ì‚¬ìš©ë¥ 
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            
            # ë””ìŠ¤í¬ ì‚¬ìš©ë¥ 
            disk = psutil.disk_usage('/')
            disk_percent = disk.percent
            
            warnings = []
            
            if cpu_percent > 80:
                warnings.append(f'High CPU usage: {cpu_percent}%')
            
            if memory_percent > 85:
                warnings.append(f'High memory usage: {memory_percent}%')
            
            if disk_percent > 90:
                warnings.append(f'High disk usage: {disk_percent}%')
            
            if warnings:
                return {'status': 'warn', 'message': '; '.join(warnings)}
            
            return {
                'status': 'ok',
                'message': f'Resources OK (CPU: {cpu_percent}%, RAM: {memory_percent}%, Disk: {disk_percent}%)'
            }
            
        except ImportError:
            return {'status': 'skip', 'message': 'psutil not available'}
        except Exception as e:
            return {'status': 'fail', 'message': f'Resource check error: {str(e)}'}
    
    def check_data_quality(self):
        """ë°ì´í„° í’ˆì§ˆ í™•ì¸"""
        try:
            from apps.recommendations.models import UserInteraction
            from apps.products.models import Product
            
            # ìµœê·¼ ìƒí˜¸ì‘ìš© ìˆ˜
            recent_interactions = UserInteraction.objects.filter(
                created__gte=timezone.now() - timedelta(days=1)
            ).count()
            
            # í™œì„± ìƒí’ˆ ìˆ˜
            active_products = Product.objects.filter(is_active=True).count()
            
            if recent_interactions == 0:
                return {'status': 'warn', 'message': 'No recent user interactions'}
            
            if active_products == 0:
                return {'status': 'fail', 'message': 'No active products'}
            
            return {
                'status': 'ok',
                'message': f'Data OK (Interactions: {recent_interactions}, Products: {active_products})'
            }
            
        except Exception as e:
            return {'status': 'fail', 'message': f'Data quality check error: {str(e)}'}
    
    def check_model_performance(self):
        """ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ í™•ì¸"""
        try:
            # ìµœê·¼ 24ì‹œê°„ ì¶”ì²œ ì„±ëŠ¥ ë¶„ì„
            from apps.recommendations.performance_analyzer import performance_analyzer
            
            analysis = performance_analyzer.analyze_recommendation_performance(1)
            
            avg_response_time = analysis['overall_metrics']['avg_response_time_ms']
            success_rate = analysis['overall_metrics']['success_rate']
            
            issues = []
            
            if avg_response_time > 1000:
                issues.append(f'Slow response time: {avg_response_time:.0f}ms')
            
            if success_rate < 0.95:
                issues.append(f'Low success rate: {success_rate:.2%}')
            
            if issues:
                return {'status': 'warn', 'message': '; '.join(issues)}
            
            return {
                'status': 'ok',
                'message': f'Performance OK (Response: {avg_response_time:.0f}ms, Success: {success_rate:.2%})'
            }
            
        except Exception as e:
            return {'status': 'fail', 'message': f'Performance check error: {str(e)}'}
    
    def check_ab_tests(self):
        """A/B í…ŒìŠ¤íŠ¸ ìƒíƒœ í™•ì¸"""
        try:
            from apps.recommendations.models import Experiment, ExperimentStatus
            
            active_experiments = Experiment.objects.filter(
                status=ExperimentStatus.ACTIVE
            ).count()
            
            return {
                'status': 'ok',
                'message': f'A/B tests: {active_experiments} active experiments'
            }
            
        except Exception as e:
            return {'status': 'fail', 'message': f'A/B test check error: {str(e)}'}
    
    def check_cache_performance(self):
        """ìºì‹œ ì„±ëŠ¥ í™•ì¸"""
        try:
            import time
            
            # ìºì‹œ ì‘ë‹µ ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            cache.set('perf_test', 'data', 60)
            cache.get('perf_test')
            cache_time = (time.time() - start_time) * 1000
            
            if cache_time > 100:  # 100ms ì´ˆê³¼
                return {
                    'status': 'warn',
                    'message': f'Slow cache response: {cache_time:.2f}ms'
                }
            
            return {
                'status': 'ok',
                'message': f'Cache performance OK: {cache_time:.2f}ms'
            }
            
        except Exception as e:
            return {'status': 'fail', 'message': f'Cache performance check error: {str(e)}'}
```

---

## ğŸ¯ ë§ˆë¬´ë¦¬

ì´ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Django Ninjaë¥¼ í™œìš©í•˜ì—¬ ì™„ì „í•œ ì¶”ì²œ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ì „ ê³¼ì •ì„ ë‹¤ë¤˜ìŠµë‹ˆë‹¤. í˜‘ì—… í•„í„°ë§ë¶€í„° ì½˜í…ì¸  ê¸°ë°˜ í•„í„°ë§, í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ, ê·¸ë¦¬ê³  í”„ë¡œë•ì…˜ ë°°í¬ê¹Œì§€ ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” ì¢…í•©ì ì¸ ì†”ë£¨ì…˜ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤.

### ğŸ” í•µì‹¬ í¬ì¸íŠ¸

1. **í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜**: ëª¨ë“ˆí™”ëœ ì„¤ê³„ë¡œ ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ ì¶”ê°€ ìš©ì´
2. **ì„±ëŠ¥ ìµœì í™”**: ìºì‹±, ë¹„ë™ê¸° ì²˜ë¦¬, ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ì„ í†µí•œ ì‘ë‹µ ì†ë„ í–¥ìƒ
3. **A/B í…ŒìŠ¤íŠ¸**: ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •ì„ ìœ„í•œ ì‹¤í—˜ í”„ë ˆì„ì›Œí¬
4. **ì‹¤ì‹œê°„ ì²˜ë¦¬**: WebSocketì„ í†µí•œ ì‹¤ì‹œê°„ ì¶”ì²œ ì—…ë°ì´íŠ¸
5. **í”„ë¡œë•ì…˜ ì¤€ë¹„**: ëª¨ë‹ˆí„°ë§, ë¡œê¹…, CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

### ğŸš€ ì¶”ê°€ í•™ìŠµ ë°©í–¥

- **ë”¥ëŸ¬ë‹ ì¶”ì²œ**: TensorFlow/PyTorchë¥¼ í™œìš©í•œ ì‹ ê²½ë§ ê¸°ë°˜ ì¶”ì²œ
- **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**: Kafka/Kinesisë¥¼ ì´ìš©í•œ ëŒ€ìš©ëŸ‰ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬
- **ë¶„ì‚° ì²˜ë¦¬**: Sparkë¥¼ í™œìš©í•œ ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬
- **ê³ ê¸‰ í‰ê°€ ì§€í‘œ**: Precision@K, NDCG, Diversity ë“± ê³ ê¸‰ ë©”íŠ¸ë¦­ êµ¬í˜„

ì´ ì¶”ì²œ ì‹œìŠ¤í…œì€ ë°±ì—”ë“œ ë©´ì ‘ì—ì„œ ê¸°ìˆ ì  ê¹Šì´ì™€ ì‹¤ë¬´ ê²½í—˜ì„ ë™ì‹œì— ë³´ì—¬ì¤„ ìˆ˜ ìˆëŠ” ì™„ë²½í•œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. ê° ì»´í¬ë„ŒíŠ¸ë¥¼ ë‹¨ê³„ì ìœ¼ë¡œ êµ¬í˜„í•˜ë©´ì„œ ë¨¸ì‹ ëŸ¬ë‹, ë°±ì—”ë“œ ì•„í‚¤í…ì²˜, ê·¸ë¦¬ê³  DevOps ì „ë°˜ì— ëŒ€í•œ ì´í•´ë¥¼ ë†’ì—¬ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.