---
layout: post
title: "Django Ninjaì™€ Redisë¡œ êµ¬ì¶•í•˜ëŠ” ì „ëµì  ìºì‹œ ê´€ë¦¬ ì‹œìŠ¤í…œ"
date: 2026-01-08 10:00:00 +0900
categories: [Django, Python, Redis, Performance]
tags: [Django, Django Ninja, Redis, Cache, Async, Performance, ModelSchema, Cache Invalidation]
image: "/assets/img/posts/2026-01-08-django-ninja-redis-cache-strategy.webp"
---

Django NinjaëŠ” FastAPIì—ì„œ ì˜ê°ì„ ë°›ì€ ê³ ì„±ëŠ¥ ë¹„ë™ê¸° API í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ ê¸€ì—ì„œëŠ” Django Ninjaì˜ ë¹„ë™ê¸° ê¸°ëŠ¥ê³¼ Redisë¥¼ ê²°í•©í•˜ì—¬ ì „ëµì ì¸ ìºì‹œ ê´€ë¦¬ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ë‹¨ìˆœí•œ ìºì‹œ ì €ì¥ì„ ë„˜ì–´, ë²„ì „ ê´€ë¦¬, ë¬´íš¨í™” ì „ëµ, ê·¸ë¦¬ê³  ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ íŒ¨í„´ë“¤ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## ğŸ¯ ì™œ Django Ninjaì™€ Redisë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ”ê°€?

Djangoì˜ ì „í†µì ì¸ ORMê³¼ í…œí”Œë¦¿ ì‹œìŠ¤í…œì€ ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì‘ë™í•˜ë©°, ëŒ€ê·œëª¨ íŠ¸ë˜í”½ í™˜ê²½ì—ì„œ ì„±ëŠ¥ ë³‘ëª©ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Django NinjaëŠ” ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì§€ì›í•˜ì—¬ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , Redisì™€ ê²°í•©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì´ì ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

**ì„±ëŠ¥ í–¥ìƒ**
- ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ë¶€í•˜ë¥¼ 70-90% ê°ì†Œ
- API ì‘ë‹µ ì‹œê°„ì„ ë°€ë¦¬ì´ˆ ë‹¨ìœ„ë¡œ ê°œì„  (í‰ê·  10-50ms)
- ë™ì‹œ ì—°ê²° ì²˜ë¦¬ ëŠ¥ë ¥ 10ë°° ì´ìƒ ì¦ê°€

**í™•ì¥ì„±**
- ìˆ˜í‰ì  í™•ì¥ì´ ìš©ì´í•œ ì•„í‚¤í…ì²˜
- ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê°„ ë°ì´í„° ê³µìœ  ê³„ì¸µ
- ì„¸ì…˜ ìŠ¤í† ì–´, ë¶„ì‚° ë½ ë“± ë‹¤ì–‘í•œ ìš©ë„ë¡œ í™œìš©

**ë¹„ìš© ì ˆê°**
- ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ í¬ê¸° ì¶•ì†Œ ê°€ëŠ¥
- RDS ì½ê¸° ì „ìš© ë³µì œë³¸ ë¹„ìš© ì ˆê°
- ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì  ì‚¬ìš©

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ì„¤ê³„ ë° í™˜ê²½ ì„¤ì •

ì „ëµì ì¸ ìºì‹œ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê¸° ì „ì— ì˜¬ë°”ë¥¸ ì•„í‚¤í…ì²˜ì™€ í™˜ê²½ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. í”„ë¡œë•ì…˜ í™˜ê²½ì„ ê³ ë ¤í•œ ì„¤ì •ìœ¼ë¡œ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.

### í”„ë¡œì íŠ¸ êµ¬ì¡°

```plaintext
myproject/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ products/
â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â”œâ”€â”€ schemas.py
â”‚   â”‚   â”œâ”€â”€ api.py
â”‚   â”‚   â””â”€â”€ cache.py
â”‚   â””â”€â”€ users/
â”‚       â”œâ”€â”€ models.py
â”‚       â”œâ”€â”€ schemas.py
â”‚       â”œâ”€â”€ api.py
â”‚       â””â”€â”€ cache.py
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ cache/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ manager.py
â”‚   â”‚   â”œâ”€â”€ keys.py
â”‚   â”‚   â”œâ”€â”€ versioning.py
â”‚   â”‚   â””â”€â”€ invalidation.py
â”‚   â””â”€â”€ redis_client.py
â””â”€â”€ settings/
    â”œâ”€â”€ base.py
    â””â”€â”€ production.py
```

### í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# ê¸°ë³¸ íŒ¨í‚¤ì§€
pip install django django-ninja redis aioredis

# ì§ë ¬í™” ë° ìœ í‹¸ë¦¬í‹°
pip install orjson pydantic

# ë¹„ë™ê¸° ë°ì´í„°ë² ì´ìŠ¤ (PostgreSQL ì‚¬ìš© ì‹œ)
pip install psycopg[binary] psycopg[pool]

# ëª¨ë‹ˆí„°ë§ (ì„ íƒì‚¬í•­)
pip install django-redis-cache prometheus-client
```

### Redis ì„¤ì • (settings/base.py)

```python
# settings/base.py
import os
from typing import Dict, Any

# Redis ê¸°ë³¸ ì„¤ì •
REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
REDIS_DB = int(os.getenv("REDIS_DB", 0))
REDIS_PASSWORD = os.getenv("REDIS_PASSWORD", None)

# Redis ì—°ê²° í’€ ì„¤ì •
REDIS_CONFIG: Dict[str, Any] = {
    "host": REDIS_HOST,
    "port": REDIS_PORT,
    "db": REDIS_DB,
    "password": REDIS_PASSWORD,
    "decode_responses": True,  # ë¬¸ìì—´ë¡œ ìë™ ë””ì½”ë”©
    "encoding": "utf-8",
    "max_connections": 50,  # ì—°ê²° í’€ ìµœëŒ€ í¬ê¸°
    "socket_timeout": 5,
    "socket_connect_timeout": 5,
    "retry_on_timeout": True,
}

# ìºì‹œ ì „ëµ ì„¤ì •
CACHE_CONFIG = {
    "default_ttl": 3600,  # 1ì‹œê°„
    "version_prefix": "v1",  # ìºì‹œ ë²„ì „
    "enable_compression": True,  # í° ë°ì´í„° ì••ì¶•
    "compression_threshold": 1024,  # 1KB ì´ìƒ ì••ì¶•
}
```

### Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (core/redis_client.py)

ë¹„ë™ê¸°ì™€ ë™ê¸° í´ë¼ì´ì–¸íŠ¸ë¥¼ ëª¨ë‘ ì œê³µí•˜ëŠ” Redis í´ë¼ì´ì–¸íŠ¸ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

```python
# core/redis_client.py
import redis.asyncio as aioredis
import redis
from django.conf import settings
from typing import Optional
import logging

logger = logging.getLogger(__name__)


class RedisClient:
    """Redis í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤"""
    
    _async_instance: Optional[aioredis.Redis] = None
    _sync_instance: Optional[redis.Redis] = None
    
    @classmethod
    async def get_async_client(cls) -> aioredis.Redis:
        """ë¹„ë™ê¸° Redis í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
        if cls._async_instance is None:
            try:
                cls._async_instance = await aioredis.from_url(
                    f"redis://{settings.REDIS_HOST}:{settings.REDIS_PORT}/{settings.REDIS_DB}",
                    password=settings.REDIS_PASSWORD,
                    encoding="utf-8",
                    decode_responses=True,
                    max_connections=50,
                )
                # ì—°ê²° í…ŒìŠ¤íŠ¸
                await cls._async_instance.ping()
                logger.info("Async Redis client connected successfully")
            except Exception as e:
                logger.error(f"Failed to connect to Redis: {e}")
                raise
        return cls._async_instance
    
    @classmethod
    def get_sync_client(cls) -> redis.Redis:
        """ë™ê¸° Redis í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜ (Django ì‹œê·¸ë„ ë“±ì—ì„œ ì‚¬ìš©)"""
        if cls._sync_instance is None:
            cls._sync_instance = redis.Redis(**settings.REDIS_CONFIG)
            try:
                cls._sync_instance.ping()
                logger.info("Sync Redis client connected successfully")
            except Exception as e:
                logger.error(f"Failed to connect to Redis: {e}")
                raise
        return cls._sync_instance
    
    @classmethod
    async def close_async_client(cls):
        """ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œ"""
        if cls._async_instance:
            await cls._async_instance.close()
            cls._async_instance = None
    
    @classmethod
    def close_sync_client(cls):
        """ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œ"""
        if cls._sync_instance:
            cls._sync_instance.close()
            cls._sync_instance = None


# í¸ì˜ í•¨ìˆ˜
async def get_redis() -> aioredis.Redis:
    """ë¹„ë™ê¸° Redis í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
    return await RedisClient.get_async_client()


def get_redis_sync() -> redis.Redis:
    """ë™ê¸° Redis í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
    return RedisClient.get_sync_client()
```

## ğŸ”‘ ìºì‹œ í‚¤ ì „ëµ ì„¤ê³„

íš¨ê³¼ì ì¸ ìºì‹œ ê´€ë¦¬ì˜ í•µì‹¬ì€ ì²´ê³„ì ì¸ í‚¤ ë„¤ì´ë° ì „ëµì…ë‹ˆë‹¤. ì¼ê´€ëœ í‚¤ êµ¬ì¡°ëŠ” ìºì‹œ ë¬´íš¨í™”, ëª¨ë‹ˆí„°ë§, ë””ë²„ê¹…ì„ í¬ê²Œ ê°„ì†Œí™”í•©ë‹ˆë‹¤.

### ìºì‹œ í‚¤ ë§¤ë‹ˆì € (core/cache/keys.py)

```python
# core/cache/keys.py
from typing import Optional, Union, List, Dict, Any
from django.conf import settings
import hashlib
import json


class CacheKeyManager:
    """ìºì‹œ í‚¤ ìƒì„± ë° ê´€ë¦¬"""
    
    # í‚¤ ë„¤ì„ìŠ¤í˜ì´ìŠ¤
    NAMESPACE_SEPARATOR = ":"
    VERSION_SEPARATOR = ":v"
    
    def __init__(self, namespace: str, version: Optional[str] = None):
        """
        Args:
            namespace: ìºì‹œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ (ì˜ˆ: 'product', 'user', 'order')
            version: ìºì‹œ ë²„ì „ (ê¸°ë³¸ê°’: settings.CACHE_CONFIG['version_prefix'])
        """
        self.namespace = namespace
        self.version = version or settings.CACHE_CONFIG.get('version_prefix', 'v1')
    
    def generate_key(
        self, 
        resource_id: Optional[Union[int, str]] = None,
        action: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        ìºì‹œ í‚¤ ìƒì„±
        
        íŒ¨í„´: {version}:{namespace}:{resource_id}:{action}:{hash}
        ì˜ˆì‹œ: v1:product:123:detail:a3f2c9
        
        Args:
            resource_id: ë¦¬ì†ŒìŠ¤ ID (ì„ íƒ)
            action: ì•¡ì…˜ ì´ë¦„ (ì„ íƒ)
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„° (ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°, í•„í„° ë“±)
        """
        parts = [self.version, self.namespace]
        
        if resource_id is not None:
            parts.append(str(resource_id))
        
        if action:
            parts.append(action)
        
        # ì¶”ê°€ íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ í•´ì‹œ ìƒì„±
        if kwargs:
            param_hash = self._generate_param_hash(kwargs)
            parts.append(param_hash)
        
        return self.NAMESPACE_SEPARATOR.join(parts)
    
    def _generate_param_hash(self, params: Dict[str, Any]) -> str:
        """íŒŒë¼ë¯¸í„°ë¥¼ í•´ì‹±í•˜ì—¬ ì§§ì€ ë¬¸ìì—´ ìƒì„±"""
        # ì •ë ¬ëœ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        sorted_params = json.dumps(params, sort_keys=True, default=str)
        # SHA-256 í•´ì‹œì˜ ì• 8ìë¦¬ ì‚¬ìš©
        return hashlib.sha256(sorted_params.encode()).hexdigest()[:8]
    
    def list_key(self, **filters) -> str:
        """ë¦¬ìŠ¤íŠ¸ ìºì‹œ í‚¤ ìƒì„±"""
        return self.generate_key(action="list", **filters)
    
    def detail_key(self, resource_id: Union[int, str]) -> str:
        """ìƒì„¸ ì¡°íšŒ ìºì‹œ í‚¤ ìƒì„±"""
        return self.generate_key(resource_id=resource_id, action="detail")
    
    def count_key(self, **filters) -> str:
        """ì¹´ìš´íŠ¸ ìºì‹œ í‚¤ ìƒì„±"""
        return self.generate_key(action="count", **filters)
    
    def pattern(self, resource_id: Optional[Union[int, str]] = None) -> str:
        """
        íŒ¨í„´ ë§¤ì¹­ìš© í‚¤ ìƒì„± (ì™€ì¼ë“œì¹´ë“œ)
        íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ ëª¨ë“  í‚¤ë¥¼ ì‚­ì œí•  ë•Œ ì‚¬ìš©
        """
        parts = [self.version, self.namespace]
        if resource_id is not None:
            parts.append(str(resource_id))
        parts.append("*")
        return self.NAMESPACE_SEPARATOR.join(parts)
    
    def increment_version(self) -> str:
        """
        ë²„ì „ì„ ì¦ê°€ì‹œì¼œ ìƒˆë¡œìš´ í‚¤ ë§¤ë‹ˆì € ë°˜í™˜
        ì „ì²´ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë¬´íš¨í™” ì‹œ ì‚¬ìš©
        """
        # v1 -> v2, v2 -> v3
        if self.version.startswith('v'):
            try:
                current_num = int(self.version[1:])
                new_version = f"v{current_num + 1}"
            except ValueError:
                new_version = f"{self.version}_new"
        else:
            new_version = f"{self.version}_v2"
        
        return CacheKeyManager(self.namespace, new_version)


# ì‚¬ì „ ì •ì˜ëœ í‚¤ ë§¤ë‹ˆì €
class CacheKeys:
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì „ì²´ì—ì„œ ì‚¬ìš©í•˜ëŠ” ìºì‹œ í‚¤ ë§¤ë‹ˆì €"""
    
    PRODUCT = CacheKeyManager("product")
    USER = CacheKeyManager("user")
    ORDER = CacheKeyManager("order")
    CATEGORY = CacheKeyManager("category")
    
    @classmethod
    def get_manager(cls, namespace: str) -> CacheKeyManager:
        """ë™ì ìœ¼ë¡œ í‚¤ ë§¤ë‹ˆì € ìƒì„±"""
        return CacheKeyManager(namespace)
```

**í‚¤ ìƒì„± ì˜ˆì‹œ:**

```python
# ìƒí’ˆ ìƒì„¸ ì¡°íšŒ
key = CacheKeys.PRODUCT.detail_key(123)
# ê²°ê³¼: "v1:product:123:detail"

# í•„í„°ë§ëœ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸
key = CacheKeys.PRODUCT.list_key(category="electronics", price_min=100, price_max=1000)
# ê²°ê³¼: "v1:product:list:a3f2c9e1"

# íŠ¹ì • ìƒí’ˆì˜ ëª¨ë“  ìºì‹œ ì‚­ì œ íŒ¨í„´
pattern = CacheKeys.PRODUCT.pattern(123)
# ê²°ê³¼: "v1:product:123:*"
```

## ğŸ“¦ ModelSchema ì§ë ¬í™”ì™€ ìºì‹œ ì €ì¥

Django Ninjaì˜ ModelSchemaë¥¼ ì‚¬ìš©í•˜ë©´ Django ëª¨ë¸ì„ ìë™ìœ¼ë¡œ Pydantic ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í™œìš©í•˜ì—¬ íƒ€ì… ì•ˆì •ì„±ì„ ë³´ì¥í•˜ë©´ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ìºì‹œë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ëª¨ë¸ ì •ì˜ (apps/products/models.py)

```python
# apps/products/models.py
from django.db import models
from django.utils import timezone


class Category(models.Model):
    name = models.CharField(max_length=100, unique=True)
    slug = models.SlugField(unique=True)
    description = models.TextField(blank=True)
    created_at = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        db_table = "categories"
        verbose_name_plural = "Categories"
    
    def __str__(self):
        return self.name


class Product(models.Model):
    STATUS_CHOICES = [
        ('active', 'Active'),
        ('inactive', 'Inactive'),
        ('out_of_stock', 'Out of Stock'),
    ]
    
    name = models.CharField(max_length=200, db_index=True)
    slug = models.SlugField(unique=True)
    description = models.TextField()
    price = models.DecimalField(max_digits=10, decimal_places=2)
    stock = models.IntegerField(default=0)
    status = models.CharField(max_length=20, choices=STATUS_CHOICES, default='active')
    category = models.ForeignKey(Category, on_delete=models.CASCADE, related_name='products')
    
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    class Meta:
        db_table = "products"
        indexes = [
            models.Index(fields=['status', 'created_at']),
            models.Index(fields=['category', 'status']),
        ]
    
    def __str__(self):
        return self.name
```

### Schema ì •ì˜ (apps/products/schemas.py)

```python
# apps/products/schemas.py
from ninja import ModelSchema, Schema
from typing import Optional, List
from datetime import datetime
from decimal import Decimal
from .models import Product, Category


class CategorySchema(ModelSchema):
    """ì¹´í…Œê³ ë¦¬ ìŠ¤í‚¤ë§ˆ"""
    class Config:
        model = Category
        model_fields = ['id', 'name', 'slug', 'description']


class ProductSchema(ModelSchema):
    """ìƒí’ˆ ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ"""
    category: CategorySchema
    
    class Config:
        model = Product
        model_fields = [
            'id', 'name', 'slug', 'description', 
            'price', 'stock', 'status', 'created_at', 'updated_at'
        ]


class ProductListSchema(ModelSchema):
    """ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ìš© ê²½ëŸ‰ ìŠ¤í‚¤ë§ˆ"""
    category_name: str
    
    class Config:
        model = Product
        model_fields = ['id', 'name', 'slug', 'price', 'stock', 'status']
    
    @staticmethod
    def resolve_category_name(obj):
        return obj.category.name


class ProductFilterSchema(Schema):
    """ìƒí’ˆ í•„í„°ë§ ìŠ¤í‚¤ë§ˆ"""
    category_id: Optional[int] = None
    status: Optional[str] = None
    min_price: Optional[Decimal] = None
    max_price: Optional[Decimal] = None
    search: Optional[str] = None
    offset: int = 0
    limit: int = 20


class PaginatedProductsSchema(Schema):
    """í˜ì´ì§€ë„¤ì´ì…˜ëœ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ì‘ë‹µ"""
    items: List[ProductListSchema]
    total: int
    offset: int
    limit: int
```

### ìºì‹œ ë§¤ë‹ˆì € êµ¬í˜„ (core/cache/manager.py)

ì´ì œ ìºì‹œ ì €ì¥, ì¡°íšŒ, ì‚­ì œë¥¼ ê´€ë¦¬í•˜ëŠ” í†µí•© ë§¤ë‹ˆì €ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

```python
# core/cache/manager.py
import orjson
from typing import Optional, Type, TypeVar, Generic, List, Any
from pydantic import BaseModel
from django.conf import settings
import logging
import zlib
from core.redis_client import get_redis
from core.cache.keys import CacheKeyManager

logger = logging.getLogger(__name__)

T = TypeVar('T', bound=BaseModel)


class CacheManager(Generic[T]):
    """
    ì œë„¤ë¦­ ìºì‹œ ë§¤ë‹ˆì €
    Pydantic ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ íƒ€ì… ì•ˆì „ ìºì‹œ ê´€ë¦¬
    """
    
    def __init__(
        self, 
        schema_class: Type[T],
        key_manager: CacheKeyManager,
        default_ttl: Optional[int] = None,
        enable_compression: bool = True
    ):
        """
        Args:
            schema_class: Pydantic ìŠ¤í‚¤ë§ˆ í´ë˜ìŠ¤
            key_manager: ìºì‹œ í‚¤ ë§¤ë‹ˆì €
            default_ttl: ê¸°ë³¸ TTL (ì´ˆ) - Noneì´ë©´ ì„¤ì •ê°’ ì‚¬ìš©
            enable_compression: ë°ì´í„° ì••ì¶• ì‚¬ìš© ì—¬ë¶€
        """
        self.schema_class = schema_class
        self.key_manager = key_manager
        self.default_ttl = default_ttl or settings.CACHE_CONFIG.get('default_ttl', 3600)
        self.enable_compression = enable_compression
        self.compression_threshold = settings.CACHE_CONFIG.get('compression_threshold', 1024)
    
    async def get(self, key: str) -> Optional[T]:
        """
        ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ
        
        Args:
            key: ìºì‹œ í‚¤
            
        Returns:
            ìŠ¤í‚¤ë§ˆ ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” None
        """
        try:
            redis = await get_redis()
            data = await redis.get(key)
            
            if data is None:
                logger.debug(f"Cache miss: {key}")
                return None
            
            # ì••ì¶• í•´ì œ
            if self.enable_compression and data.startswith('COMPRESSED:'):
                compressed_data = data[11:]  # 'COMPRESSED:' ì œê±°
                data = zlib.decompress(compressed_data.encode('latin1')).decode('utf-8')
            
            # JSON íŒŒì‹± ë° ìŠ¤í‚¤ë§ˆ ê²€ì¦
            parsed_data = orjson.loads(data)
            instance = self.schema_class(**parsed_data)
            
            logger.debug(f"Cache hit: {key}")
            return instance
            
        except Exception as e:
            logger.error(f"Cache get error for key {key}: {e}")
            return None
    
    async def get_many(self, keys: List[str]) -> List[Optional[T]]:
        """
        ì—¬ëŸ¬ ìºì‹œ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒ (íŒŒì´í”„ë¼ì¸ ì‚¬ìš©)
        
        Args:
            keys: ìºì‹œ í‚¤ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ìŠ¤í‚¤ë§ˆ ì¸ìŠ¤í„´ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ None)
        """
        if not keys:
            return []
        
        try:
            redis = await get_redis()
            values = await redis.mget(keys)
            
            results = []
            for key, data in zip(keys, values):
                if data is None:
                    results.append(None)
                    continue
                
                try:
                    # ì••ì¶• í•´ì œ
                    if self.enable_compression and data.startswith('COMPRESSED:'):
                        compressed_data = data[11:]
                        data = zlib.decompress(compressed_data.encode('latin1')).decode('utf-8')
                    
                    parsed_data = orjson.loads(data)
                    instance = self.schema_class(**parsed_data)
                    results.append(instance)
                except Exception as e:
                    logger.error(f"Failed to parse cached data for key {key}: {e}")
                    results.append(None)
            
            return results
            
        except Exception as e:
            logger.error(f"Cache get_many error: {e}")
            return [None] * len(keys)
    
    async def set(
        self, 
        key: str, 
        value: T, 
        ttl: Optional[int] = None
    ) -> bool:
        """
        ìºì‹œì— ë°ì´í„° ì €ì¥
        
        Args:
            key: ìºì‹œ í‚¤
            value: ì €ì¥í•  ìŠ¤í‚¤ë§ˆ ì¸ìŠ¤í„´ìŠ¤
            ttl: TTL (ì´ˆ) - Noneì´ë©´ default_ttl ì‚¬ìš©
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            redis = await get_redis()
            
            # Pydantic ëª¨ë¸ì„ JSONìœ¼ë¡œ ì§ë ¬í™”
            json_data = orjson.dumps(
                value.dict(),
                option=orjson.OPT_SERIALIZE_NUMPY | orjson.OPT_NAIVE_UTC
            ).decode('utf-8')
            
            # ì••ì¶• ì ìš©
            if self.enable_compression and len(json_data) > self.compression_threshold:
                compressed = zlib.compress(json_data.encode('utf-8'))
                data_to_store = 'COMPRESSED:' + compressed.decode('latin1')
                logger.debug(f"Compressed data from {len(json_data)} to {len(data_to_store)} bytes")
            else:
                data_to_store = json_data
            
            # TTL ì„¤ì •
            expire_time = ttl if ttl is not None else self.default_ttl
            
            # Redisì— ì €ì¥
            await redis.setex(key, expire_time, data_to_store)
            logger.debug(f"Cache set: {key} (TTL: {expire_time}s)")
            return True
            
        except Exception as e:
            logger.error(f"Cache set error for key {key}: {e}")
            return False
    
    async def set_many(
        self, 
        items: List[tuple[str, T]], 
        ttl: Optional[int] = None
    ) -> bool:
        """
        ì—¬ëŸ¬ ìºì‹œ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì €ì¥ (íŒŒì´í”„ë¼ì¸ ì‚¬ìš©)
        
        Args:
            items: (í‚¤, ê°’) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            ttl: TTL (ì´ˆ)
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not items:
            return True
        
        try:
            redis = await get_redis()
            pipe = redis.pipeline()
            
            expire_time = ttl if ttl is not None else self.default_ttl
            
            for key, value in items:
                json_data = orjson.dumps(
                    value.dict(),
                    option=orjson.OPT_SERIALIZE_NUMPY | orjson.OPT_NAIVE_UTC
                ).decode('utf-8')
                
                if self.enable_compression and len(json_data) > self.compression_threshold:
                    compressed = zlib.compress(json_data.encode('utf-8'))
                    data_to_store = 'COMPRESSED:' + compressed.decode('latin1')
                else:
                    data_to_store = json_data
                
                pipe.setex(key, expire_time, data_to_store)
            
            await pipe.execute()
            logger.debug(f"Cache set_many: {len(items)} items")
            return True
            
        except Exception as e:
            logger.error(f"Cache set_many error: {e}")
            return False
    
    async def delete(self, key: str) -> bool:
        """ìºì‹œ ì‚­ì œ"""
        try:
            redis = await get_redis()
            result = await redis.delete(key)
            logger.debug(f"Cache delete: {key}")
            return result > 0
        except Exception as e:
            logger.error(f"Cache delete error for key {key}: {e}")
            return False
    
    async def delete_pattern(self, pattern: str) -> int:
        """
        íŒ¨í„´ì— ë§¤ì¹­ë˜ëŠ” ëª¨ë“  ìºì‹œ ì‚­ì œ
        ì£¼ì˜: KEYS ëª…ë ¹ì€ í”„ë¡œë•ì…˜ì—ì„œ ì£¼ì˜í•´ì„œ ì‚¬ìš©
        ëŒ€ì•ˆìœ¼ë¡œ SCAN ì‚¬ìš© ê³ ë ¤
        """
        try:
            redis = await get_redis()
            keys = await redis.keys(pattern)
            
            if not keys:
                return 0
            
            deleted = await redis.delete(*keys)
            logger.info(f"Cache delete pattern: {pattern}, deleted: {deleted} keys")
            return deleted
            
        except Exception as e:
            logger.error(f"Cache delete pattern error for pattern {pattern}: {e}")
            return 0
    
    async def exists(self, key: str) -> bool:
        """ìºì‹œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        try:
            redis = await get_redis()
            return await redis.exists(key) > 0
        except Exception as e:
            logger.error(f"Cache exists error for key {key}: {e}")
            return False
    
    async def ttl(self, key: str) -> int:
        """ìºì‹œ TTL ì¡°íšŒ (ì´ˆ ë‹¨ìœ„, -2: ì—†ìŒ, -1: ë¬´ì œí•œ)"""
        try:
            redis = await get_redis()
            return await redis.ttl(key)
        except Exception as e:
            logger.error(f"Cache ttl error for key {key}: {e}")
            return -2
```

## âš¡ Django Ninja ë¹„ë™ê¸° API êµ¬í˜„

ì´ì œ Django Ninjaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸° APIë¥¼ êµ¬í˜„í•˜ê³ , ìºì‹œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤.

### ì œí’ˆ ìºì‹œ ì„œë¹„ìŠ¤ (apps/products/cache.py)

```python
# apps/products/cache.py
from typing import Optional, List
from .models import Product
from .schemas import ProductSchema, ProductListSchema, PaginatedProductsSchema
from core.cache.manager import CacheManager
from core.cache.keys import CacheKeys
from django.db.models import Q
import logging

logger = logging.getLogger(__name__)


class ProductCacheService:
    """ìƒí’ˆ ìºì‹œ ì„œë¹„ìŠ¤"""
    
    # ìºì‹œ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
    detail_cache = CacheManager(ProductSchema, CacheKeys.PRODUCT, default_ttl=3600)
    list_cache = CacheManager(PaginatedProductsSchema, CacheKeys.PRODUCT, default_ttl=1800)
    
    @classmethod
    async def get_product_by_id(cls, product_id: int) -> Optional[ProductSchema]:
        """
        IDë¡œ ìƒí’ˆ ì¡°íšŒ (ìºì‹œ ìš°ì„ )
        
        Args:
            product_id: ìƒí’ˆ ID
            
        Returns:
            ProductSchema ë˜ëŠ” None
        """
        cache_key = CacheKeys.PRODUCT.detail_key(product_id)
        
        # 1. ìºì‹œ í™•ì¸
        cached = await cls.detail_cache.get(cache_key)
        if cached:
            logger.info(f"Product {product_id} retrieved from cache")
            return cached
        
        # 2. DB ì¡°íšŒ (ë¹„ë™ê¸°)
        try:
            product = await Product.objects.select_related('category').aget(id=product_id)
            
            # 3. ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜
            product_schema = ProductSchema.from_orm(product)
            
            # 4. ìºì‹œ ì €ì¥
            await cls.detail_cache.set(cache_key, product_schema)
            
            logger.info(f"Product {product_id} fetched from DB and cached")
            return product_schema
            
        except Product.DoesNotExist:
            logger.warning(f"Product {product_id} not found")
            return None
        except Exception as e:
            logger.error(f"Error fetching product {product_id}: {e}")
            return None
    
    @classmethod
    async def get_products_list(
        cls,
        category_id: Optional[int] = None,
        status: Optional[str] = None,
        min_price: Optional[float] = None,
        max_price: Optional[float] = None,
        search: Optional[str] = None,
        offset: int = 0,
        limit: int = 20
    ) -> PaginatedProductsSchema:
        """
        í•„í„°ë§ëœ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ (ìºì‹œ ìš°ì„ )
        """
        # ìºì‹œ í‚¤ ìƒì„± (í•„í„° ì¡°ê±´ í¬í•¨)
        filter_params = {
            'category_id': category_id,
            'status': status,
            'min_price': min_price,
            'max_price': max_price,
            'search': search,
            'offset': offset,
            'limit': limit,
        }
        # None ê°’ ì œê±°
        filter_params = {k: v for k, v in filter_params.items() if v is not None}
        
        cache_key = CacheKeys.PRODUCT.list_key(**filter_params)
        
        # 1. ìºì‹œ í™•ì¸
        cached = await cls.list_cache.get(cache_key)
        if cached:
            logger.info("Product list retrieved from cache")
            return cached
        
        # 2. DB ì¿¼ë¦¬ ë¹Œë“œ
        queryset = Product.objects.select_related('category').all()
        
        if category_id:
            queryset = queryset.filter(category_id=category_id)
        
        if status:
            queryset = queryset.filter(status=status)
        
        if min_price is not None:
            queryset = queryset.filter(price__gte=min_price)
        
        if max_price is not None:
            queryset = queryset.filter(price__lte=max_price)
        
        if search:
            queryset = queryset.filter(
                Q(name__icontains=search) | Q(description__icontains=search)
            )
        
        # 3. ì´ ê°œìˆ˜ ì¡°íšŒ (ë¹„ë™ê¸°)
        total = await queryset.acount()
        
        # 4. í˜ì´ì§€ë„¤ì´ì…˜ ì ìš© ë° ì¡°íšŒ
        products = []
        async for product in queryset.order_by('-created_at')[offset:offset + limit]:
            # ProductListSchemaë¡œ ë³€í™˜
            product_data = {
                'id': product.id,
                'name': product.name,
                'slug': product.slug,
                'price': product.price,
                'stock': product.stock,
                'status': product.status,
                'category_name': product.category.name,
            }
            products.append(ProductListSchema(**product_data))
        
        # 5. ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ìƒì„±
        result = PaginatedProductsSchema(
            items=products,
            total=total,
            offset=offset,
            limit=limit
        )
        
        # 6. ìºì‹œ ì €ì¥
        await cls.list_cache.set(cache_key, result)
        
        logger.info(f"Product list fetched from DB and cached ({len(products)} items)")
        return result
    
    @classmethod
    async def invalidate_product(cls, product_id: int):
        """íŠ¹ì • ìƒí’ˆ ê´€ë ¨ ëª¨ë“  ìºì‹œ ë¬´íš¨í™”"""
        # ìƒì„¸ ìºì‹œ ì‚­ì œ
        detail_key = CacheKeys.PRODUCT.detail_key(product_id)
        await cls.detail_cache.delete(detail_key)
        
        # í•´ë‹¹ ìƒí’ˆì´ í¬í•¨ëœ ëª¨ë“  ë¦¬ìŠ¤íŠ¸ ìºì‹œ ì‚­ì œ
        # íŒ¨í„´: v1:product:list:*
        list_pattern = CacheKeys.PRODUCT.generate_key(action="list") + ":*"
        deleted_count = await cls.list_cache.delete_pattern(list_pattern)
        
        logger.info(f"Invalidated product {product_id} cache (list caches: {deleted_count})")
    
    @classmethod
    async def invalidate_all_lists(cls):
        """ëª¨ë“  ë¦¬ìŠ¤íŠ¸ ìºì‹œ ë¬´íš¨í™”"""
        list_pattern = CacheKeys.PRODUCT.generate_key(action="list") + ":*"
        deleted_count = await cls.list_cache.delete_pattern(list_pattern)
        logger.info(f"Invalidated all product list caches ({deleted_count} keys)")
```

### API ì—”ë“œí¬ì¸íŠ¸ (apps/products/api.py)

```python
# apps/products/api.py
from ninja import Router
from typing import List
from .schemas import (
    ProductSchema, 
    ProductFilterSchema, 
    PaginatedProductsSchema
)
from .cache import ProductCacheService
from ninja.responses import Response
import logging

logger = logging.getLogger(__name__)

router = Router()


@router.get("/{product_id}", response=ProductSchema)
async def get_product(request, product_id: int):
    """
    ìƒí’ˆ ìƒì„¸ ì¡°íšŒ (ìºì‹œ ì ìš©)
    
    - ìºì‹œ íˆíŠ¸ ì‹œ: ~5ms
    - ìºì‹œ ë¯¸ìŠ¤ ì‹œ: ~50-100ms (DB ì¿¼ë¦¬ í¬í•¨)
    """
    product = await ProductCacheService.get_product_by_id(product_id)
    
    if product is None:
        return Response({"error": "Product not found"}, status=404)
    
    return product


@router.get("", response=PaginatedProductsSchema)
async def list_products(request, filters: ProductFilterSchema = None):
    """
    ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ (í•„í„°ë§, í˜ì´ì§€ë„¤ì´ì…˜, ìºì‹œ ì ìš©)
    
    Query Parameters:
    - category_id: ì¹´í…Œê³ ë¦¬ ID
    - status: ìƒí’ˆ ìƒíƒœ (active, inactive, out_of_stock)
    - min_price: ìµœì†Œ ê°€ê²©
    - max_price: ìµœëŒ€ ê°€ê²©
    - search: ê²€ìƒ‰ì–´ (ìƒí’ˆëª…, ì„¤ëª…)
    - offset: ì˜¤í”„ì…‹ (ê¸°ë³¸: 0)
    - limit: í˜ì´ì§€ í¬ê¸° (ê¸°ë³¸: 20, ìµœëŒ€: 100)
    """
    filters = filters or ProductFilterSchema()
    
    # limit ì œí•œ
    filters.limit = min(filters.limit, 100)
    
    result = await ProductCacheService.get_products_list(
        category_id=filters.category_id,
        status=filters.status,
        min_price=filters.min_price,
        max_price=filters.max_price,
        search=filters.search,
        offset=filters.offset,
        limit=filters.limit,
    )
    
    return result
```

## ğŸ”„ ì „ëµì  ìºì‹œ ë¬´íš¨í™” ì‹œìŠ¤í…œ

ìºì‹œ ë¬´íš¨í™”ëŠ” ìºì‹œ ì „ëµì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ê³  ë³µì¡í•œ ë¶€ë¶„ì…ë‹ˆë‹¤. ë°ì´í„° ì¼ê´€ì„±ì„ ìœ ì§€í•˜ë©´ì„œë„ ì„±ëŠ¥ì„ ìµœëŒ€í™”í•˜ëŠ” ì „ëµì´ í•„ìš”í•©ë‹ˆë‹¤.

### ìºì‹œ ë¬´íš¨í™” ì „ëµ (core/cache/invalidation.py)

```python
# core/cache/invalidation.py
from enum import Enum
from typing import Optional, List, Callable, Awaitable
from django.db.models.signals import post_save, post_delete, m2m_changed
from django.dispatch import receiver
from core.redis_client import get_redis_sync
from core.cache.keys import CacheKeys
import logging

logger = logging.getLogger(__name__)


class InvalidationStrategy(Enum):
    """ìºì‹œ ë¬´íš¨í™” ì „ëµ"""
    
    # ì¦‰ì‹œ ì‚­ì œ: ë°ì´í„° ë³€ê²½ ì‹œ ì¦‰ì‹œ ìºì‹œ ì‚­ì œ
    IMMEDIATE = "immediate"
    
    # ì§€ì—° ì‚­ì œ: ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìœ¼ë¡œ ì‚­ì œ
    DELAYED = "delayed"
    
    # TTL ë‹¨ì¶•: ìºì‹œ ìœ ì§€í•˜ë˜ TTLì„ ì§§ê²Œ ì„¤ì •
    TTL_REDUCTION = "ttl_reduction"
    
    # ë²„ì „ ì¦ê°€: ë²„ì „ì„ ì˜¬ë ¤ ê¸°ì¡´ ìºì‹œ ë¬´íš¨í™”
    VERSION_INCREMENT = "version_increment"
    
    # ì„ íƒì  ë¬´íš¨í™”: íŠ¹ì • ì¡°ê±´ì˜ ìºì‹œë§Œ ì‚­ì œ
    SELECTIVE = "selective"


class CacheInvalidator:
    """ìºì‹œ ë¬´íš¨í™” ê´€ë¦¬ì"""
    
    def __init__(self):
        self.redis = get_redis_sync()
    
    def invalidate_by_pattern(self, pattern: str) -> int:
        """
        íŒ¨í„´ ê¸°ë°˜ ìºì‹œ ì‚­ì œ
        
        ì£¼ì˜: KEYSëŠ” O(N) ë³µì¡ë„ë¡œ í”„ë¡œë•ì…˜ì—ì„œ ì£¼ì˜
        ëŒ€ì•ˆ: SCAN ì‚¬ìš© ë˜ëŠ” Redis Clusterì—ì„œ í•´ì‹œíƒœê·¸ í™œìš©
        """
        try:
            keys = self.redis.keys(pattern)
            if not keys:
                return 0
            
            deleted = self.redis.delete(*keys)
            logger.info(f"Invalidated {deleted} keys matching pattern: {pattern}")
            return deleted
        except Exception as e:
            logger.error(f"Error invalidating pattern {pattern}: {e}")
            return 0
    
    def invalidate_keys(self, keys: List[str]) -> int:
        """ì—¬ëŸ¬ í‚¤ë¥¼ í•œ ë²ˆì— ì‚­ì œ"""
        if not keys:
            return 0
        
        try:
            deleted = self.redis.delete(*keys)
            logger.info(f"Invalidated {deleted} keys")
            return deleted
        except Exception as e:
            logger.error(f"Error invalidating keys: {e}")
            return 0
    
    def reduce_ttl(self, key: str, new_ttl: int) -> bool:
        """
        ìºì‹œ TTL ë‹¨ì¶•
        ì ì§„ì  ë¬´íš¨í™” ì „ëµì— ì‚¬ìš©
        """
        try:
            self.redis.expire(key, new_ttl)
            logger.info(f"Reduced TTL for key {key} to {new_ttl}s")
            return True
        except Exception as e:
            logger.error(f"Error reducing TTL for key {key}: {e}")
            return False
    
    def tag_for_invalidation(self, key: str, tag: str) -> bool:
        """
        ë¬´íš¨í™” íƒœê·¸ ì„¤ì • (ë‚˜ì¤‘ì— ì¼ê´„ ì²˜ë¦¬)
        Redis Setì„ ì‚¬ìš©í•˜ì—¬ íƒœê·¸ ê´€ë¦¬
        """
        try:
            tag_key = f"invalidation_queue:{tag}"
            self.redis.sadd(tag_key, key)
            # íƒœê·¸ ìì²´ì—ë„ TTL ì„¤ì • (24ì‹œê°„)
            self.redis.expire(tag_key, 86400)
            return True
        except Exception as e:
            logger.error(f"Error tagging key {key} with tag {tag}: {e}")
            return False
    
    def process_tagged_invalidations(self, tag: str) -> int:
        """íƒœê·¸ëœ ìºì‹œë“¤ì„ ì¼ê´„ ë¬´íš¨í™”"""
        try:
            tag_key = f"invalidation_queue:{tag}"
            keys = self.redis.smembers(tag_key)
            
            if not keys:
                return 0
            
            # ìºì‹œ ì‚­ì œ
            deleted = self.redis.delete(*keys)
            
            # íƒœê·¸ ì‚­ì œ
            self.redis.delete(tag_key)
            
            logger.info(f"Processed invalidation for tag {tag}: {deleted} keys")
            return deleted
        except Exception as e:
            logger.error(f"Error processing tagged invalidations for {tag}: {e}")
            return 0


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
cache_invalidator = CacheInvalidator()


# Django ì‹œê·¸ë„ ê¸°ë°˜ ìë™ ë¬´íš¨í™”
@receiver(post_save, sender='products.Product')
def invalidate_product_on_save(sender, instance, created, **kwargs):
    """
    ìƒí’ˆ ì €ì¥ ì‹œ ìºì‹œ ë¬´íš¨í™”
    
    ì „ëµ:
    - ìƒì„¸ ìºì‹œ: ì¦‰ì‹œ ì‚­ì œ
    - ë¦¬ìŠ¤íŠ¸ ìºì‹œ: TTL ë‹¨ì¶• (10ë¶„)
    """
    try:
        product_id = instance.id
        
        # 1. ìƒì„¸ ìºì‹œ ì¦‰ì‹œ ì‚­ì œ
        detail_key = CacheKeys.PRODUCT.detail_key(product_id)
        cache_invalidator.invalidate_keys([detail_key])
        
        # 2. ë¦¬ìŠ¤íŠ¸ ìºì‹œëŠ” TTL ë‹¨ì¶• (ì¦‰ì‹œ ì‚­ì œí•˜ë©´ ë¶€í•˜ ì¦ê°€)
        list_pattern = CacheKeys.PRODUCT.generate_key(action="list") + ":*"
        
        # ìƒì„±ì¸ ê²½ìš°: ë¦¬ìŠ¤íŠ¸ ìºì‹œ ì¦‰ì‹œ ì‚­ì œ
        if created:
            cache_invalidator.invalidate_by_pattern(list_pattern)
            logger.info(f"New product {product_id} created - invalidated all list caches")
        else:
            # ìˆ˜ì •ì¸ ê²½ìš°: TTL ë‹¨ì¶• ì „ëµ
            # ì‹¤ì œë¡œëŠ” SCANìœ¼ë¡œ í‚¤ë¥¼ ì°¾ì•„ì„œ TTL ë‹¨ì¶•
            cache_invalidator.tag_for_invalidation(list_pattern, "product_update")
            logger.info(f"Product {product_id} updated - tagged list caches for invalidation")
        
    except Exception as e:
        logger.error(f"Error in invalidate_product_on_save: {e}")


@receiver(post_delete, sender='products.Product')
def invalidate_product_on_delete(sender, instance, **kwargs):
    """
    ìƒí’ˆ ì‚­ì œ ì‹œ ëª¨ë“  ê´€ë ¨ ìºì‹œ ì¦‰ì‹œ ì‚­ì œ
    """
    try:
        product_id = instance.id
        
        # ìƒì„¸ ìºì‹œ ì‚­ì œ
        detail_key = CacheKeys.PRODUCT.detail_key(product_id)
        cache_invalidator.invalidate_keys([detail_key])
        
        # ë¦¬ìŠ¤íŠ¸ ìºì‹œ ì „ì²´ ì‚­ì œ
        list_pattern = CacheKeys.PRODUCT.generate_key(action="list") + ":*"
        cache_invalidator.invalidate_by_pattern(list_pattern)
        
        logger.info(f"Product {product_id} deleted - invalidated all related caches")
        
    except Exception as e:
        logger.error(f"Error in invalidate_product_on_delete: {e}")


@receiver(post_save, sender='products.Category')
def invalidate_category_on_save(sender, instance, created, **kwargs):
    """
    ì¹´í…Œê³ ë¦¬ ë³€ê²½ ì‹œ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ìºì‹œ ë¬´íš¨í™”
    """
    try:
        category_id = instance.id
        
        # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ìºì‹œë§Œ ì‚­ì œ
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì„¸ë°€í•œ íŒ¨í„´ ë§¤ì¹­ í•„ìš”
        list_pattern = CacheKeys.PRODUCT.generate_key(action="list") + ":*"
        cache_invalidator.invalidate_by_pattern(list_pattern)
        
        logger.info(f"Category {category_id} updated - invalidated related product lists")
        
    except Exception as e:
        logger.error(f"Error in invalidate_category_on_save: {e}")
```

### ë¹„ë™ê¸° ë¬´íš¨í™” í—¬í¼ (apps/products/cache.pyì— ì¶”ê°€)

```python
# apps/products/cache.pyì— ì¶”ê°€

class ProductCacheService:
    # ... ê¸°ì¡´ ì½”ë“œ ...
    
    @classmethod
    async def smart_invalidation(
        cls, 
        product_id: int,
        invalidation_strategy: InvalidationStrategy = InvalidationStrategy.IMMEDIATE
    ):
        """
        ì „ëµì  ìºì‹œ ë¬´íš¨í™”
        
        Args:
            product_id: ìƒí’ˆ ID
            invalidation_strategy: ë¬´íš¨í™” ì „ëµ
        """
        from core.cache.invalidation import InvalidationStrategy
        
        if invalidation_strategy == InvalidationStrategy.IMMEDIATE:
            # ì¦‰ì‹œ ì‚­ì œ
            await cls.invalidate_product(product_id)
            
        elif invalidation_strategy == InvalidationStrategy.TTL_REDUCTION:
            # TTL ë‹¨ì¶• (10ë¶„)
            detail_key = CacheKeys.PRODUCT.detail_key(product_id)
            redis = await get_redis()
            await redis.expire(detail_key, 600)
            
            list_pattern = CacheKeys.PRODUCT.generate_key(action="list") + ":*"
            keys = await redis.keys(list_pattern)
            for key in keys:
                await redis.expire(key, 600)
            
            logger.info(f"Reduced TTL for product {product_id} caches to 10 minutes")
            
        elif invalidation_strategy == InvalidationStrategy.VERSION_INCREMENT:
            # ë²„ì „ ì¦ê°€ë¡œ ì „ì²´ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë¬´íš¨í™”
            # ìƒˆ ë²„ì „ì˜ í‚¤ ë§¤ë‹ˆì € ì‚¬ìš©
            new_key_manager = CacheKeys.PRODUCT.increment_version()
            
            # ì• í”Œë¦¬ì¼€ì´ì…˜ ì¬ì‹œì‘ ì—†ì´ ë²„ì „ ë³€ê²½ì€ ë³µì¡í•˜ë¯€ë¡œ
            # ì‹¤ì œë¡œëŠ” Redisì— ë²„ì „ ì •ë³´ ì €ì¥ í•„ìš”
            logger.info(f"Version incremented for product namespace")
            
        elif invalidation_strategy == InvalidationStrategy.SELECTIVE:
            # ì„ íƒì  ë¬´íš¨í™”: ìƒì„¸ë§Œ ì‚­ì œ, ë¦¬ìŠ¤íŠ¸ëŠ” ìœ ì§€
            detail_key = CacheKeys.PRODUCT.detail_key(product_id)
            await cls.detail_cache.delete(detail_key)
            logger.info(f"Selectively invalidated product {product_id} detail cache only")
```

## ğŸ“Š ìºì‹œ ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ

ìºì‹œ ë²„ì „ ê´€ë¦¬ëŠ” ëŒ€ê·œëª¨ ë°°í¬ë‚˜ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì‹œ ì•ˆì „í•˜ê²Œ ìºì‹œë¥¼ ë¬´íš¨í™”í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ ì „ëµì…ë‹ˆë‹¤.

### ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ (core/cache/versioning.py)

```python
# core/cache/versioning.py
from typing import Optional, Dict
from datetime import datetime, timedelta
from core.redis_client import get_redis, get_redis_sync
import logging

logger = logging.getLogger(__name__)


class CacheVersionManager:
    """
    ìºì‹œ ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ
    
    íŠ¹ì§•:
    - ì „ì—­ ë²„ì „ê³¼ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ ë²„ì „ ê´€ë¦¬
    - ì ì§„ì  ë²„ì „ ì—…ê·¸ë ˆì´ë“œ
    - ë¡¤ë°± ì§€ì›
    """
    
    VERSION_KEY_PREFIX = "cache_version"
    GLOBAL_VERSION_KEY = f"{VERSION_KEY_PREFIX}:global"
    
    def __init__(self):
        self.redis = get_redis_sync()
    
    def get_global_version(self) -> str:
        """
        ì „ì—­ ìºì‹œ ë²„ì „ ì¡°íšŒ
        
        Returns:
            ë²„ì „ ë¬¸ìì—´ (ì˜ˆ: "v1", "v2")
        """
        version = self.redis.get(self.GLOBAL_VERSION_KEY)
        if version is None:
            # ì´ˆê¸° ë²„ì „ ì„¤ì •
            self.set_global_version("v1")
            return "v1"
        return version
    
    def set_global_version(self, version: str) -> bool:
        """ì „ì—­ ìºì‹œ ë²„ì „ ì„¤ì •"""
        try:
            self.redis.set(self.GLOBAL_VERSION_KEY, version)
            logger.info(f"Global cache version set to {version}")
            return True
        except Exception as e:
            logger.error(f"Error setting global version: {e}")
            return False
    
    def increment_global_version(self) -> str:
        """
        ì „ì—­ ë²„ì „ ì¦ê°€
        ëª¨ë“  ìºì‹œë¥¼ ë¬´íš¨í™”í•˜ëŠ” ê°€ì¥ ê°•ë ¥í•œ ë°©ë²•
        """
        current = self.get_global_version()
        
        # ë²„ì „ íŒŒì‹± ë° ì¦ê°€
        if current.startswith('v'):
            try:
                num = int(current[1:])
                new_version = f"v{num + 1}"
            except ValueError:
                new_version = f"{current}_new"
        else:
            new_version = f"{current}_v2"
        
        self.set_global_version(new_version)
        
        # ë²„ì „ ë³€ê²½ ì´ë ¥ ì €ì¥
        history_key = f"{self.VERSION_KEY_PREFIX}:history"
        timestamp = datetime.now().isoformat()
        self.redis.hset(history_key, timestamp, f"{current} -> {new_version}")
        
        logger.warning(f"Global cache version incremented: {current} -> {new_version}")
        return new_version
    
    def get_namespace_version(self, namespace: str) -> str:
        """ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ ë²„ì „ ì¡°íšŒ"""
        key = f"{self.VERSION_KEY_PREFIX}:{namespace}"
        version = self.redis.get(key)
        
        if version is None:
            # ì „ì—­ ë²„ì „ ì‚¬ìš©
            version = self.get_global_version()
            self.redis.set(key, version)
        
        return version
    
    def set_namespace_version(self, namespace: str, version: str) -> bool:
        """ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ ë²„ì „ ì„¤ì •"""
        try:
            key = f"{self.VERSION_KEY_PREFIX}:{namespace}"
            self.redis.set(key, version)
            logger.info(f"Cache version for namespace '{namespace}' set to {version}")
            return True
        except Exception as e:
            logger.error(f"Error setting namespace version: {e}")
            return False
    
    def increment_namespace_version(self, namespace: str) -> str:
        """
        íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ ë²„ì „ë§Œ ì¦ê°€
        í•´ë‹¹ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ ëª¨ë“  ìºì‹œ ë¬´íš¨í™”
        """
        current = self.get_namespace_version(namespace)
        
        if current.startswith('v'):
            try:
                num = int(current[1:])
                new_version = f"v{num + 1}"
            except ValueError:
                new_version = f"{current}_new"
        else:
            new_version = f"{current}_v2"
        
        self.set_namespace_version(namespace, new_version)
        
        logger.warning(f"Namespace '{namespace}' version incremented: {current} -> {new_version}")
        return new_version
    
    def rollback_namespace_version(self, namespace: str, target_version: str) -> bool:
        """
        ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë²„ì „ ë¡¤ë°±
        ê¸´ê¸‰ ìƒí™©ì—ì„œ ì´ì „ ë²„ì „ìœ¼ë¡œ ë³µêµ¬
        """
        try:
            current = self.get_namespace_version(namespace)
            self.set_namespace_version(namespace, target_version)
            
            logger.warning(
                f"Namespace '{namespace}' version rolled back: "
                f"{current} -> {target_version}"
            )
            return True
        except Exception as e:
            logger.error(f"Error rolling back namespace version: {e}")
            return False
    
    def get_version_history(self, limit: int = 10) -> Dict[str, str]:
        """ë²„ì „ ë³€ê²½ ì´ë ¥ ì¡°íšŒ"""
        history_key = f"{self.VERSION_KEY_PREFIX}:history"
        history = self.redis.hgetall(history_key)
        
        # ìµœì‹  í•­ëª©ë¶€í„° ì •ë ¬
        sorted_history = dict(
            sorted(history.items(), key=lambda x: x[0], reverse=True)[:limit]
        )
        
        return sorted_history
    
    def cleanup_old_caches(self, namespace: str, current_version: str) -> int:
        """
        ì´ì „ ë²„ì „ì˜ ìºì‹œ ì •ë¦¬
        
        ì£¼ì˜: í”„ë¡œë•ì…˜ì—ì„œëŠ” ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìœ¼ë¡œ ì‹¤í–‰
        """
        try:
            # í˜„ì¬ ë²„ì „ì´ ì•„ë‹Œ ëª¨ë“  ìºì‹œ ì°¾ê¸°
            # ì˜ˆ: v1:product:* (í˜„ì¬ ë²„ì „ì´ v2ì¸ ê²½ìš°)
            pattern = f"{namespace}:*"
            keys = self.redis.keys(pattern)
            
            deleted = 0
            for key in keys:
                # í˜„ì¬ ë²„ì „ì´ ì•„ë‹Œ í‚¤ë§Œ ì‚­ì œ
                if not key.startswith(f"{current_version}:"):
                    self.redis.delete(key)
                    deleted += 1
            
            logger.info(f"Cleaned up {deleted} old cache keys for namespace '{namespace}'")
            return deleted
            
        except Exception as e:
            logger.error(f"Error cleaning up old caches: {e}")
            return 0


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
version_manager = CacheVersionManager()


# ë²„ì „ ì¸ì‹ í‚¤ ë§¤ë‹ˆì €
class VersionAwareCacheKeyManager:
    """
    ë²„ì „ì„ ë™ì ìœ¼ë¡œ í™•ì¸í•˜ëŠ” í‚¤ ë§¤ë‹ˆì €
    """
    
    def __init__(self, namespace: str):
        self.namespace = namespace
    
    def get_current_version(self) -> str:
        """í˜„ì¬ ë²„ì „ ì¡°íšŒ"""
        return version_manager.get_namespace_version(self.namespace)
    
    def generate_key(
        self, 
        resource_id: Optional[int] = None,
        action: Optional[str] = None,
        **kwargs
    ) -> str:
        """í˜„ì¬ ë²„ì „ì„ ë°˜ì˜í•œ í‚¤ ìƒì„±"""
        version = self.get_current_version()
        from core.cache.keys import CacheKeyManager
        
        # ì„ì‹œ í‚¤ ë§¤ë‹ˆì € ìƒì„±
        temp_manager = CacheKeyManager(self.namespace, version)
        return temp_manager.generate_key(resource_id, action, **kwargs)
    
    def detail_key(self, resource_id: int) -> str:
        return self.generate_key(resource_id=resource_id, action="detail")
    
    def list_key(self, **filters) -> str:
        return self.generate_key(action="list", **filters)
```

### ë²„ì „ ê´€ë¦¬ API ì—”ë“œí¬ì¸íŠ¸

```python
# core/api/cache_admin.py (ê´€ë¦¬ììš©)
from ninja import Router
from ninja.security import django_auth
from core.cache.versioning import version_manager
from typing import Dict

router = Router(tags=["Cache Admin"], auth=django_auth)


@router.post("/version/global/increment")
async def increment_global_cache_version(request):
    """
    ì „ì—­ ìºì‹œ ë²„ì „ ì¦ê°€ (ëª¨ë“  ìºì‹œ ë¬´íš¨í™”)
    
    ì£¼ì˜: ì´ ì‘ì—…ì€ ëª¨ë“  ìºì‹œë¥¼ ë¬´íš¨í™”í•©ë‹ˆë‹¤.
    ëŒ€ê·œëª¨ ë°°í¬ë‚˜ ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì‹œì—ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
    """
    old_version = version_manager.get_global_version()
    new_version = version_manager.increment_global_version()
    
    return {
        "status": "success",
        "old_version": old_version,
        "new_version": new_version,
        "message": "All caches invalidated by version increment"
    }


@router.post("/version/{namespace}/increment")
async def increment_namespace_cache_version(request, namespace: str):
    """
    íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ ìºì‹œ ë²„ì „ ì¦ê°€
    
    í•´ë‹¹ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ ëª¨ë“  ìºì‹œë§Œ ë¬´íš¨í™”
    """
    old_version = version_manager.get_namespace_version(namespace)
    new_version = version_manager.increment_namespace_version(namespace)
    
    return {
        "status": "success",
        "namespace": namespace,
        "old_version": old_version,
        "new_version": new_version,
        "message": f"Namespace '{namespace}' caches invalidated"
    }


@router.get("/version/history")
async def get_version_history(request, limit: int = 10):
    """ë²„ì „ ë³€ê²½ ì´ë ¥ ì¡°íšŒ"""
    history = version_manager.get_version_history(limit)
    return {
        "status": "success",
        "history": history
    }


@router.post("/version/{namespace}/rollback")
async def rollback_namespace_version(request, namespace: str, target_version: str):
    """
    ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë²„ì „ ë¡¤ë°±
    
    ê¸´ê¸‰ ìƒí™©ì—ì„œ ì´ì „ ë²„ì „ìœ¼ë¡œ ë³µêµ¬
    """
    success = version_manager.rollback_namespace_version(namespace, target_version)
    
    if success:
        return {
            "status": "success",
            "namespace": namespace,
            "target_version": target_version,
            "message": f"Rolled back to version {target_version}"
        }
    else:
        return {
            "status": "error",
            "message": "Rollback failed"
        }
```

## ğŸ¨ ê³ ê¸‰ ìºì‹œ íŒ¨í„´ ë° ìµœì í™”

ì‹¤ë¬´ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ê³ ê¸‰ ìºì‹œ íŒ¨í„´ë“¤ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

### 1. Cache-Aside (Lazy Loading) íŒ¨í„´

ê°€ì¥ ì¼ë°˜ì ì¸ íŒ¨í„´ìœ¼ë¡œ, ì´ë¯¸ êµ¬í˜„í•œ ë°©ì‹ì…ë‹ˆë‹¤:

```python
async def get_with_cache_aside(key: str, fetch_func: Callable, ttl: int = 3600):
    """
    Cache-Aside íŒ¨í„´ í—¬í¼
    
    1. ìºì‹œ í™•ì¸
    2. ìºì‹œ ë¯¸ìŠ¤ ì‹œ DB ì¡°íšŒ
    3. ìºì‹œì— ì €ì¥
    """
    # ìºì‹œ í™•ì¸
    cached = await cache_manager.get(key)
    if cached:
        return cached
    
    # DB ì¡°íšŒ
    data = await fetch_func()
    
    # ìºì‹œ ì €ì¥
    if data:
        await cache_manager.set(key, data, ttl)
    
    return data
```

### 2. Write-Through íŒ¨í„´

ë°ì´í„° ì €ì¥ ì‹œ DBì™€ ìºì‹œë¥¼ ë™ì‹œì— ì—…ë°ì´íŠ¸:

```python
# apps/products/cache.pyì— ì¶”ê°€

@classmethod
async def update_product_write_through(
    cls, 
    product_id: int,
    update_data: dict
) -> Optional[ProductSchema]:
    """
    Write-Through íŒ¨í„´ìœ¼ë¡œ ìƒí’ˆ ì—…ë°ì´íŠ¸
    
    1. DB ì—…ë°ì´íŠ¸
    2. ìºì‹œ ì¦‰ì‹œ ì—…ë°ì´íŠ¸ (ì‚­ì œ ì•„ë‹˜)
    """
    try:
        # 1. DB ì—…ë°ì´íŠ¸
        product = await Product.objects.select_related('category').aget(id=product_id)
        
        for key, value in update_data.items():
            setattr(product, key, value)
        
        await product.asave()
        
        # 2. ìºì‹œ ì¦‰ì‹œ ì—…ë°ì´íŠ¸
        product_schema = ProductSchema.from_orm(product)
        cache_key = CacheKeys.PRODUCT.detail_key(product_id)
        await cls.detail_cache.set(cache_key, product_schema)
        
        logger.info(f"Product {product_id} updated with write-through")
        return product_schema
        
    except Product.DoesNotExist:
        return None
    except Exception as e:
        logger.error(f"Error in write-through update: {e}")
        return None
```

### 3. Write-Behind (Write-Back) íŒ¨í„´

ìºì‹œë¥¼ ë¨¼ì € ì—…ë°ì´íŠ¸í•˜ê³  DBëŠ” ë¹„ë™ê¸°ë¡œ ì—…ë°ì´íŠ¸:

```python
from celery import shared_task

@shared_task
def async_db_update(product_id: int, update_data: dict):
    """ë°±ê·¸ë¼ìš´ë“œ DB ì—…ë°ì´íŠ¸ ì‘ì—…"""
    try:
        product = Product.objects.get(id=product_id)
        for key, value in update_data.items():
            setattr(product, key, value)
        product.save()
        logger.info(f"Product {product_id} DB updated asynchronously")
    except Exception as e:
        logger.error(f"Error in async DB update: {e}")


@classmethod
async def update_product_write_behind(
    cls,
    product_id: int,
    update_data: dict
) -> bool:
    """
    Write-Behind íŒ¨í„´
    
    1. ìºì‹œ ì¦‰ì‹œ ì—…ë°ì´íŠ¸
    2. DBëŠ” ë°±ê·¸ë¼ìš´ë“œë¡œ ì—…ë°ì´íŠ¸
    
    ì£¼ì˜: ë°ì´í„° ì¼ê´€ì„± ë³´ì¥ì´ í•„ìš”í•œ ê²½ìš° ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ
    """
    try:
        # 1. í˜„ì¬ ë°ì´í„° ì¡°íšŒ
        cache_key = CacheKeys.PRODUCT.detail_key(product_id)
        product = await cls.detail_cache.get(cache_key)
        
        if product is None:
            # ìºì‹œ ë¯¸ìŠ¤: DBì—ì„œ ì¡°íšŒ
            product = await cls.get_product_by_id(product_id)
            if product is None:
                return False
        
        # 2. ìºì‹œ ì—…ë°ì´íŠ¸
        updated_dict = product.dict()
        updated_dict.update(update_data)
        updated_product = ProductSchema(**updated_dict)
        
        await cls.detail_cache.set(cache_key, updated_product)
        
        # 3. DB ì—…ë°ì´íŠ¸ëŠ” ë°±ê·¸ë¼ìš´ë“œë¡œ
        async_db_update.delay(product_id, update_data)
        
        logger.info(f"Product {product_id} updated with write-behind")
        return True
        
    except Exception as e:
        logger.error(f"Error in write-behind update: {e}")
        return False
```

### 4. Cache Warming (Pre-loading) íŒ¨í„´

ìì£¼ ì¡°íšŒë˜ëŠ” ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ìºì‹œì— ë¡œë“œ:

```python
# apps/products/cache.pyì— ì¶”ê°€

@classmethod
async def warm_popular_products(cls, limit: int = 100):
    """
    ì¸ê¸° ìƒí’ˆ ìºì‹œ ì›Œë°
    
    ì„œë²„ ì‹œì‘ ì‹œë‚˜ íŠ¸ë˜í”½ ì¦ê°€ ì˜ˆìƒ ì‹œ ì‹¤í–‰
    """
    try:
        # ìµœê·¼ ì¡°íšŒìˆ˜ê°€ ë†’ì€ ìƒí’ˆ ì¡°íšŒ (ë³„ë„ í…Œì´ë¸” í•„ìš”)
        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ ìµœì‹  ìƒí’ˆìœ¼ë¡œ ì˜ˆì‹œ
        products = []
        async for product in Product.objects.select_related('category')\
                .filter(status='active')\
                .order_by('-created_at')[:limit]:
            products.append(product)
        
        # ë³‘ë ¬ë¡œ ìºì‹œ ì €ì¥
        cache_items = []
        for product in products:
            product_schema = ProductSchema.from_orm(product)
            cache_key = CacheKeys.PRODUCT.detail_key(product.id)
            cache_items.append((cache_key, product_schema))
        
        await cls.detail_cache.set_many(cache_items)
        
        logger.info(f"Warmed cache for {len(products)} popular products")
        return len(products)
        
    except Exception as e:
        logger.error(f"Error warming cache: {e}")
        return 0


# Django management commandë¡œ ì‹¤í–‰
# python manage.py warm_cache
```

### 5. Refresh-Ahead íŒ¨í„´

TTL ë§Œë£Œ ì „ì— ìë™ìœ¼ë¡œ ê°±ì‹ :

```python
@classmethod
async def get_with_refresh_ahead(
    cls,
    product_id: int,
    refresh_threshold: int = 300  # TTL 5ë¶„ ì´í•˜ë©´ ê°±ì‹ 
) -> Optional[ProductSchema]:
    """
    Refresh-Ahead íŒ¨í„´
    
    TTLì´ ì„ê³„ê°’ ì´í•˜ë©´ ë°±ê·¸ë¼ìš´ë“œë¡œ ê°±ì‹ 
    """
    cache_key = CacheKeys.PRODUCT.detail_key(product_id)
    
    # ìºì‹œ ì¡°íšŒ
    product = await cls.detail_cache.get(cache_key)
    
    if product:
        # TTL í™•ì¸
        ttl = await cls.detail_cache.ttl(cache_key)
        
        # TTLì´ ì„ê³„ê°’ ì´í•˜ë©´ ë°±ê·¸ë¼ìš´ë“œë¡œ ê°±ì‹ 
        if 0 < ttl < refresh_threshold:
            # ë¹„ë™ê¸° ê°±ì‹  íŠ¸ë¦¬ê±°
            async_refresh_cache.delay(product_id)
            logger.info(f"Triggered refresh-ahead for product {product_id}")
        
        return product
    
    # ìºì‹œ ë¯¸ìŠ¤: ì¼ë°˜ ì¡°íšŒ
    return await cls.get_product_by_id(product_id)


@shared_task
def async_refresh_cache(product_id: int):
    """ë°±ê·¸ë¼ìš´ë“œ ìºì‹œ ê°±ì‹ """
    import asyncio
    asyncio.run(_refresh_product_cache(product_id))


async def _refresh_product_cache(product_id: int):
    """ì‹¤ì œ ê°±ì‹  ë¡œì§"""
    try:
        product = await Product.objects.select_related('category').aget(id=product_id)
        product_schema = ProductSchema.from_orm(product)
        cache_key = CacheKeys.PRODUCT.detail_key(product_id)
        await ProductCacheService.detail_cache.set(cache_key, product_schema)
        logger.info(f"Cache refreshed for product {product_id}")
    except Exception as e:
        logger.error(f"Error refreshing cache: {e}")
```

### 6. Two-Tier Cache (L1/L2) íŒ¨í„´

ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”ëª¨ë¦¬ ìºì‹œ + Redis ì¡°í•©:

```python
from functools import lru_cache
from typing import Optional
import asyncio

class TwoTierCache:
    """
    Two-Tier ìºì‹œ
    
    L1: ë¡œì»¬ ë©”ëª¨ë¦¬ (LRU, ë¹ ë¦„, ìš©ëŸ‰ ì œí•œ)
    L2: Redis (ëŠë¦¼, ëŒ€ìš©ëŸ‰)
    """
    
    def __init__(self, l1_size: int = 1000, l2_manager: CacheManager = None):
        self.l1_size = l1_size
        self.l2_manager = l2_manager
        self._l1_cache = {}
        self._l1_order = []
    
    async def get(self, key: str) -> Optional[Any]:
        """L1 -> L2 ìˆœì„œë¡œ ì¡°íšŒ"""
        # L1 í™•ì¸
        if key in self._l1_cache:
            logger.debug(f"L1 cache hit: {key}")
            return self._l1_cache[key]
        
        # L2 í™•ì¸
        if self.l2_manager:
            value = await self.l2_manager.get(key)
            if value:
                logger.debug(f"L2 cache hit: {key}")
                # L1ì—ë„ ì €ì¥
                self._set_l1(key, value)
                return value
        
        return None
    
    async def set(self, key: str, value: Any, ttl: int = 3600):
        """L1ê³¼ L2 ëª¨ë‘ ì €ì¥"""
        # L1 ì €ì¥
        self._set_l1(key, value)
        
        # L2 ì €ì¥
        if self.l2_manager:
            await self.l2_manager.set(key, value, ttl)
    
    def _set_l1(self, key: str, value: Any):
        """L1 ìºì‹œì— LRUë¡œ ì €ì¥"""
        if key in self._l1_cache:
            self._l1_order.remove(key)
        
        self._l1_cache[key] = value
        self._l1_order.append(key)
        
        # í¬ê¸° ì œí•œ ì´ˆê³¼ ì‹œ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
        while len(self._l1_cache) > self.l1_size:
            oldest = self._l1_order.pop(0)
            del self._l1_cache[oldest]
    
    async def delete(self, key: str):
        """L1ê³¼ L2 ëª¨ë‘ ì‚­ì œ"""
        # L1 ì‚­ì œ
        if key in self._l1_cache:
            del self._l1_cache[key]
            self._l1_order.remove(key)
        
        # L2 ì‚­ì œ
        if self.l2_manager:
            await self.l2_manager.delete(key)
```

## ğŸ” ëª¨ë‹ˆí„°ë§ ë° ì„±ëŠ¥ ì¸¡ì •

ìºì‹œ ì‹œìŠ¤í…œì˜ íš¨ê³¼ë¥¼ ì¸¡ì •í•˜ê³  ëª¨ë‹ˆí„°ë§í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.

### ìºì‹œ í†µê³„ ìˆ˜ì§‘ (core/cache/metrics.py)

```python
# core/cache/metrics.py
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, Optional
from core.redis_client import get_redis_sync
import logging

logger = logging.getLogger(__name__)


@dataclass
class CacheMetrics:
    """ìºì‹œ ë©”íŠ¸ë¦­ìŠ¤ ë°ì´í„° í´ë˜ìŠ¤"""
    hits: int
    misses: int
    sets: int
    deletes: int
    hit_rate: float
    total_requests: int
    timestamp: datetime


class CacheMetricsCollector:
    """ìºì‹œ ë©”íŠ¸ë¦­ìŠ¤ ìˆ˜ì§‘ê¸°"""
    
    METRICS_KEY_PREFIX = "cache_metrics"
    
    def __init__(self):
        self.redis = get_redis_sync()
    
    def _get_metric_key(self, namespace: str, metric_type: str) -> str:
        """ë©”íŠ¸ë¦­ í‚¤ ìƒì„±"""
        return f"{self.METRICS_KEY_PREFIX}:{namespace}:{metric_type}"
    
    def record_hit(self, namespace: str):
        """ìºì‹œ íˆíŠ¸ ê¸°ë¡"""
        key = self._get_metric_key(namespace, "hits")
        self.redis.incr(key)
        self.redis.expire(key, 86400)  # 24ì‹œê°„ ìœ ì§€
    
    def record_miss(self, namespace: str):
        """ìºì‹œ ë¯¸ìŠ¤ ê¸°ë¡"""
        key = self._get_metric_key(namespace, "misses")
        self.redis.incr(key)
        self.redis.expire(key, 86400)
    
    def record_set(self, namespace: str):
        """ìºì‹œ ì €ì¥ ê¸°ë¡"""
        key = self._get_metric_key(namespace, "sets")
        self.redis.incr(key)
        self.redis.expire(key, 86400)
    
    def record_delete(self, namespace: str):
        """ìºì‹œ ì‚­ì œ ê¸°ë¡"""
        key = self._get_metric_key(namespace, "deletes")
        self.redis.incr(key)
        self.redis.expire(key, 86400)
    
    def get_metrics(self, namespace: str) -> CacheMetrics:
        """ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        hits = int(self.redis.get(self._get_metric_key(namespace, "hits")) or 0)
        misses = int(self.redis.get(self._get_metric_key(namespace, "misses")) or 0)
        sets = int(self.redis.get(self._get_metric_key(namespace, "sets")) or 0)
        deletes = int(self.redis.get(self._get_metric_key(namespace, "deletes")) or 0)
        
        total_requests = hits + misses
        hit_rate = (hits / total_requests * 100) if total_requests > 0 else 0.0
        
        return CacheMetrics(
            hits=hits,
            misses=misses,
            sets=sets,
            deletes=deletes,
            hit_rate=hit_rate,
            total_requests=total_requests,
            timestamp=datetime.now()
        )
    
    def get_all_metrics(self) -> Dict[str, CacheMetrics]:
        """ëª¨ë“  ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        # ëª¨ë“  ë©”íŠ¸ë¦­ í‚¤ ì°¾ê¸°
        pattern = f"{self.METRICS_KEY_PREFIX}:*:hits"
        keys = self.redis.keys(pattern)
        
        namespaces = set()
        for key in keys:
            # cache_metrics:product:hits -> product ì¶”ì¶œ
            parts = key.split(':')
            if len(parts) >= 3:
                namespaces.add(parts[1])
        
        metrics = {}
        for namespace in namespaces:
            metrics[namespace] = self.get_metrics(namespace)
        
        return metrics
    
    def reset_metrics(self, namespace: Optional[str] = None):
        """ë©”íŠ¸ë¦­ ë¦¬ì…‹"""
        if namespace:
            # íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë§Œ
            pattern = f"{self.METRICS_KEY_PREFIX}:{namespace}:*"
        else:
            # ì „ì²´
            pattern = f"{self.METRICS_KEY_PREFIX}:*"
        
        keys = self.redis.keys(pattern)
        if keys:
            self.redis.delete(*keys)
            logger.info(f"Reset metrics for pattern: {pattern}")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
metrics_collector = CacheMetricsCollector()


# ë©”íŠ¸ë¦­ ìˆ˜ì§‘ì„ í¬í•¨í•œ ìºì‹œ ë§¤ë‹ˆì € ë˜í¼
class MonitoredCacheManager(CacheManager):
    """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ ìºì‹œ ë§¤ë‹ˆì €"""
    
    def __init__(self, *args, namespace: str = None, **kwargs):
        super().__init__(*args, **kwargs)
        self.namespace = namespace or "default"
    
    async def get(self, key: str):
        result = await super().get(key)
        
        if result is not None:
            metrics_collector.record_hit(self.namespace)
        else:
            metrics_collector.record_miss(self.namespace)
        
        return result
    
    async def set(self, key: str, value, ttl: Optional[int] = None):
        result = await super().set(key, value, ttl)
        
        if result:
            metrics_collector.record_set(self.namespace)
        
        return result
    
    async def delete(self, key: str):
        result = await super().delete(key)
        
        if result:
            metrics_collector.record_delete(self.namespace)
        
        return result
```

### ëª¨ë‹ˆí„°ë§ API (core/api/cache_admin.pyì— ì¶”ê°€)

```python
# core/api/cache_admin.pyì— ì¶”ê°€

from core.cache.metrics import metrics_collector

@router.get("/metrics")
async def get_cache_metrics(request):
    """
    ëª¨ë“  ìºì‹œ ë©”íŠ¸ë¦­ ì¡°íšŒ
    
    Returns:
        - namespaceë³„ íˆíŠ¸ìœ¨, ìš”ì²­ ìˆ˜ ë“±
    """
    metrics = metrics_collector.get_all_metrics()
    
    result = {}
    for namespace, metric in metrics.items():
        result[namespace] = {
            "hits": metric.hits,
            "misses": metric.misses,
            "sets": metric.sets,
            "deletes": metric.deletes,
            "hit_rate": round(metric.hit_rate, 2),
            "total_requests": metric.total_requests,
            "timestamp": metric.timestamp.isoformat(),
        }
    
    return {
        "status": "success",
        "metrics": result
    }


@router.get("/metrics/{namespace}")
async def get_namespace_metrics(request, namespace: str):
    """íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    metric = metrics_collector.get_metrics(namespace)
    
    return {
        "status": "success",
        "namespace": namespace,
        "metrics": {
            "hits": metric.hits,
            "misses": metric.misses,
            "sets": metric.sets,
            "deletes": metric.deletes,
            "hit_rate": round(metric.hit_rate, 2),
            "total_requests": metric.total_requests,
        }
    }


@router.post("/metrics/reset")
async def reset_metrics(request, namespace: Optional[str] = None):
    """ë©”íŠ¸ë¦­ ë¦¬ì…‹"""
    metrics_collector.reset_metrics(namespace)
    
    return {
        "status": "success",
        "message": f"Metrics reset for {namespace or 'all namespaces'}"
    }


@router.get("/info")
async def get_redis_info(request):
    """
    Redis ì„œë²„ ì •ë³´ ì¡°íšŒ
    
    ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, ì—°ê²° ìˆ˜, í‚¤ ê°œìˆ˜ ë“±
    """
    from core.redis_client import get_redis_sync
    redis = get_redis_sync()
    
    info = redis.info()
    
    return {
        "status": "success",
        "redis_info": {
            "redis_version": info.get("redis_version"),
            "used_memory_human": info.get("used_memory_human"),
            "used_memory_peak_human": info.get("used_memory_peak_human"),
            "connected_clients": info.get("connected_clients"),
            "total_commands_processed": info.get("total_commands_processed"),
            "keyspace": info.get("db0", {}),
        }
    }
```

### Django ë¯¸ë“¤ì›¨ì–´ë¡œ ìë™ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```python
# core/middleware/cache_metrics.py
from django.utils.deprecation import MiddlewareMixin
import time
import logging

logger = logging.getLogger(__name__)


class CacheMetricsMiddleware(MiddlewareMixin):
    """
    API ìš”ì²­ ì‹œ ìºì‹œ ë©”íŠ¸ë¦­ ìë™ ìˆ˜ì§‘
    """
    
    def process_request(self, request):
        request._cache_start_time = time.time()
    
    def process_response(self, request, response):
        if hasattr(request, '_cache_start_time'):
            duration = time.time() - request._cache_start_time
            
            # ì‘ë‹µ ì‹œê°„ì´ ë¹ ë¥´ë©´ ìºì‹œ íˆíŠ¸ë¡œ ì¶”ì •
            if duration < 0.05:  # 50ms ì´í•˜
                logger.debug(f"Fast response detected: {request.path} ({duration:.3f}s)")
        
        return response
```

## ğŸš€ í”„ë¡œë•ì…˜ ë°°í¬ ë° ëª¨ë²” ì‚¬ë¡€

ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ìºì‹œ ì‹œìŠ¤í…œì„ ì•ˆì •ì ìœ¼ë¡œ ìš´ì˜í•˜ê¸° ìœ„í•œ ì „ëµë“¤ì…ë‹ˆë‹¤.

### 1. Redis ì„¤ì • ìµœì í™”

```python
# settings/production.py

# Redis ì—°ê²° í’€ ìµœì í™”
REDIS_CONFIG = {
    "host": os.getenv("REDIS_HOST"),
    "port": int(os.getenv("REDIS_PORT", 6379)),
    "db": int(os.getenv("REDIS_DB", 0)),
    "password": os.getenv("REDIS_PASSWORD"),
    "decode_responses": True,
    "encoding": "utf-8",
    
    # ì—°ê²° í’€ ì„¤ì •
    "max_connections": 200,  # í”„ë¡œë•ì…˜: ë” í° ê°’
    "socket_timeout": 5,
    "socket_connect_timeout": 5,
    "socket_keepalive": True,
    "socket_keepalive_options": {
        1: 1,  # TCP_KEEPIDLE
        2: 1,  # TCP_KEEPINTVL
        3: 3,  # TCP_KEEPCNT
    },
    
    # ì¬ì‹œë„ ì„¤ì •
    "retry_on_timeout": True,
    "retry_on_error": [ConnectionError, TimeoutError],
    "retry": {
        "retries": 3,
        "backoff": {
            "base": 0.1,
            "cap": 1.0,
        }
    },
    
    # í—¬ìŠ¤ ì²´í¬
    "health_check_interval": 30,
}

# ìºì‹œ ì „ëµ
CACHE_CONFIG = {
    # TTL ì„¤ì • (ì´ˆ)
    "default_ttl": 3600,  # 1ì‹œê°„
    "short_ttl": 300,     # 5ë¶„ (ìì£¼ ë³€ê²½ë˜ëŠ” ë°ì´í„°)
    "long_ttl": 86400,    # 24ì‹œê°„ (ê±°ì˜ ë³€ê²½ ì•ˆ ë˜ëŠ” ë°ì´í„°)
    
    # ë²„ì „ ê´€ë¦¬
    "version_prefix": os.getenv("CACHE_VERSION", "v1"),
    
    # ì••ì¶• ì„¤ì •
    "enable_compression": True,
    "compression_threshold": 1024,  # 1KB
    "compression_level": 6,  # zlib ì••ì¶• ë ˆë²¨ (1-9)
    
    # ë°°ì¹˜ ì²˜ë¦¬
    "batch_size": 100,  # í•œ ë²ˆì— ì²˜ë¦¬í•  ìµœëŒ€ í‚¤ ìˆ˜
    
    # ëª¨ë‹ˆí„°ë§
    "enable_metrics": True,
    "metrics_ttl": 86400,  # 24ì‹œê°„
}
```

### 2. Redis ì„œë²„ ì„¤ì • (redis.conf)

```conf
# ë©”ëª¨ë¦¬ ê´€ë¦¬
maxmemory 2gb
maxmemory-policy allkeys-lru  # LRU ì •ì±…

# ìŠ¤ëƒ…ìƒ· ë¹„í™œì„±í™” (ìºì‹œ ìš©ë„)
save ""

# AOF ë¹„í™œì„±í™” (ìºì‹œ ìš©ë„, ì˜ì†ì„± ë¶ˆí•„ìš”)
appendonly no

# ë„¤íŠ¸ì›Œí¬
timeout 300
tcp-keepalive 60

# ì„±ëŠ¥
hz 10
dynamic-hz yes

# í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
maxclients 10000

# ìŠ¬ë¡œìš° ë¡œê·¸
slowlog-log-slower-than 10000
slowlog-max-len 128
```

### 3. ì—ëŸ¬ ì²˜ë¦¬ ë° í´ë°± ì „ëµ

```python
# core/cache/fallback.py
from typing import Optional, TypeVar, Callable, Awaitable
from functools import wraps
import logging

logger = logging.getLogger(__name__)

T = TypeVar('T')


def cache_fallback(fallback_func: Optional[Callable] = None):
    """
    ìºì‹œ ì‹¤íŒ¨ ì‹œ í´ë°± ë°ì½”ë ˆì´í„°
    
    ìºì‹œ ì‹œìŠ¤í…œ ì¥ì•  ì‹œì—ë„ ì„œë¹„ìŠ¤ ì •ìƒ ë™ì‘ ë³´ì¥
    """
    def decorator(func: Callable[..., Awaitable[T]]) -> Callable[..., Awaitable[T]]:
        @wraps(func)
        async def wrapper(*args, **kwargs) -> T:
            try:
                # ìºì‹œ ë¡œì§ ì‹¤í–‰
                return await func(*args, **kwargs)
            except Exception as e:
                logger.error(f"Cache operation failed: {e}")
                
                # í´ë°± í•¨ìˆ˜ ì‹¤í–‰
                if fallback_func:
                    logger.info("Executing fallback function")
                    return await fallback_func(*args, **kwargs)
                
                # í´ë°± í•¨ìˆ˜ê°€ ì—†ìœ¼ë©´ None ë°˜í™˜
                return None
        
        return wrapper
    return decorator


# ì‚¬ìš© ì˜ˆì‹œ
async def db_fallback(product_id: int):
    """ìºì‹œ ì‹¤íŒ¨ ì‹œ DBì—ì„œ ì§ì ‘ ì¡°íšŒ"""
    try:
        product = await Product.objects.select_related('category').aget(id=product_id)
        return ProductSchema.from_orm(product)
    except Exception as e:
        logger.error(f"DB fallback failed: {e}")
        return None


@cache_fallback(fallback_func=db_fallback)
async def get_product_cached(product_id: int):
    """ìºì‹œ ì¡°íšŒ (ì‹¤íŒ¨ ì‹œ DB í´ë°±)"""
    return await ProductCacheService.get_product_by_id(product_id)
```

### 4. ì„œí‚· ë¸Œë ˆì´ì»¤ íŒ¨í„´

```python
# core/cache/circuit_breaker.py
from enum import Enum
from datetime import datetime, timedelta
from typing import Optional, Callable, Awaitable, TypeVar
import asyncio
import logging

logger = logging.getLogger(__name__)

T = TypeVar('T')


class CircuitState(Enum):
    CLOSED = "closed"      # ì •ìƒ ë™ì‘
    OPEN = "open"          # ì°¨ë‹¨ (ìºì‹œ ì‚¬ìš© ì•ˆ í•¨)
    HALF_OPEN = "half_open"  # í…ŒìŠ¤íŠ¸ ì¤‘


class CircuitBreaker:
    """
    ìºì‹œ ì„œí‚· ë¸Œë ˆì´ì»¤
    
    ì—°ì† ì‹¤íŒ¨ ì‹œ ìºì‹œë¥¼ ìš°íšŒí•˜ê³  DB ì§ì ‘ ì¡°íšŒ
    """
    
    def __init__(
        self,
        failure_threshold: int = 5,
        timeout: int = 60,
        recovery_timeout: int = 30
    ):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.recovery_timeout = recovery_timeout
        
        self.failure_count = 0
        self.last_failure_time: Optional[datetime] = None
        self.state = CircuitState.CLOSED
    
    async def call(
        self,
        func: Callable[..., Awaitable[T]],
        fallback: Callable[..., Awaitable[T]],
        *args,
        **kwargs
    ) -> T:
        """ì„œí‚· ë¸Œë ˆì´ì»¤ë¥¼ í†µí•œ í•¨ìˆ˜ í˜¸ì¶œ"""
        
        # OPEN ìƒíƒœ: ìºì‹œ ìš°íšŒ
        if self.state == CircuitState.OPEN:
            if self._should_attempt_reset():
                self.state = CircuitState.HALF_OPEN
                logger.info("Circuit breaker: OPEN -> HALF_OPEN")
            else:
                logger.warning("Circuit breaker is OPEN, using fallback")
                return await fallback(*args, **kwargs)
        
        try:
            # ìºì‹œ í˜¸ì¶œ ì‹œë„
            result = await asyncio.wait_for(
                func(*args, **kwargs),
                timeout=self.timeout
            )
            
            # ì„±ê³µ ì‹œ ìƒíƒœ ë¦¬ì…‹
            if self.state == CircuitState.HALF_OPEN:
                self._reset()
                logger.info("Circuit breaker: HALF_OPEN -> CLOSED")
            
            return result
            
        except Exception as e:
            logger.error(f"Circuit breaker: operation failed - {e}")
            self._record_failure()
            
            # HALF_OPENì—ì„œ ì‹¤íŒ¨í•˜ë©´ ë‹¤ì‹œ OPEN
            if self.state == CircuitState.HALF_OPEN:
                self.state = CircuitState.OPEN
                logger.warning("Circuit breaker: HALF_OPEN -> OPEN")
            
            # í´ë°± ì‹¤í–‰
            return await fallback(*args, **kwargs)
    
    def _record_failure(self):
        """ì‹¤íŒ¨ ê¸°ë¡"""
        self.failure_count += 1
        self.last_failure_time = datetime.now()
        
        if self.failure_count >= self.failure_threshold:
            self.state = CircuitState.OPEN
            logger.warning(
                f"Circuit breaker opened after {self.failure_count} failures"
            )
    
    def _should_attempt_reset(self) -> bool:
        """ë¦¬ì…‹ ì‹œë„ ì—¬ë¶€ í™•ì¸"""
        if self.last_failure_time is None:
            return True
        
        elapsed = (datetime.now() - self.last_failure_time).total_seconds()
        return elapsed >= self.recovery_timeout
    
    def _reset(self):
        """ì„œí‚· ë¸Œë ˆì´ì»¤ ë¦¬ì…‹"""
        self.failure_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED


# ì „ì—­ ì„œí‚· ë¸Œë ˆì´ì»¤
cache_circuit_breaker = CircuitBreaker(
    failure_threshold=5,
    timeout=5,
    recovery_timeout=30
)


# ì‚¬ìš© ì˜ˆì‹œ
async def get_product_with_circuit_breaker(product_id: int):
    """ì„œí‚· ë¸Œë ˆì´ì»¤ë¥¼ ì‚¬ìš©í•œ ìƒí’ˆ ì¡°íšŒ"""
    
    async def cache_func():
        return await ProductCacheService.get_product_by_id(product_id)
    
    async def fallback_func():
        logger.info(f"Using DB fallback for product {product_id}")
        product = await Product.objects.select_related('category').aget(id=product_id)
        return ProductSchema.from_orm(product)
    
    return await cache_circuit_breaker.call(cache_func, fallback_func)
```

### 5. í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸

```python
# core/api/health.py
from ninja import Router
from core.redis_client import get_redis_sync
from datetime import datetime

router = Router(tags=["Health"])


@router.get("/cache")
async def cache_health_check(request):
    """
    ìºì‹œ ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬
    
    - Redis ì—°ê²° ìƒíƒœ
    - ì‘ë‹µ ì‹œê°„
    - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    """
    try:
        redis = get_redis_sync()
        
        # Ping í…ŒìŠ¤íŠ¸
        start = datetime.now()
        redis.ping()
        latency = (datetime.now() - start).total_seconds() * 1000
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        info = redis.info("memory")
        used_memory = info.get("used_memory_human")
        
        # í‚¤ ê°œìˆ˜
        db_info = redis.info("keyspace")
        key_count = db_info.get("db0", {}).get("keys", 0)
        
        return {
            "status": "healthy",
            "redis": {
                "connected": True,
                "latency_ms": round(latency, 2),
                "used_memory": used_memory,
                "key_count": key_count,
            },
            "timestamp": datetime.now().isoformat(),
        }
        
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat(),
        }
```

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ì‹¤ì „ ê²°ê³¼

ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ ê°œì„  ì‚¬ë¡€ì™€ ì¸¡ì • ê²°ê³¼ì…ë‹ˆë‹¤.

### ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸

```python
# tests/benchmark_cache.py
import asyncio
import time
from typing import List
import statistics


async def benchmark_cache_vs_db(iterations: int = 1000):
    """ìºì‹œ vs DB ì„±ëŠ¥ ë¹„êµ"""
    
    # í…ŒìŠ¤íŠ¸ìš© ìƒí’ˆ IDë“¤
    product_ids = list(range(1, 101))
    
    # 1. DB ì§ì ‘ ì¡°íšŒ (ìºì‹œ ì—†ìŒ)
    db_times = []
    for product_id in product_ids[:iterations]:
        start = time.time()
        product = await Product.objects.select_related('category').aget(
            id=product_id % 100 + 1
        )
        db_times.append((time.time() - start) * 1000)
    
    # 2. ìºì‹œ ì¡°íšŒ (ì›Œë° í›„)
    # ë¨¼ì € ìºì‹œ ì›Œë°
    await ProductCacheService.warm_popular_products(100)
    
    cache_times = []
    for product_id in product_ids[:iterations]:
        start = time.time()
        product = await ProductCacheService.get_product_by_id(
            product_id % 100 + 1
        )
        cache_times.append((time.time() - start) * 1000)
    
    # ê²°ê³¼ ë¶„ì„
    results = {
        "db_query": {
            "mean": statistics.mean(db_times),
            "median": statistics.median(db_times),
            "min": min(db_times),
            "max": max(db_times),
            "stdev": statistics.stdev(db_times) if len(db_times) > 1 else 0,
        },
        "cache_query": {
            "mean": statistics.mean(cache_times),
            "median": statistics.median(cache_times),
            "min": min(cache_times),
            "max": max(cache_times),
            "stdev": statistics.stdev(cache_times) if len(cache_times) > 1 else 0,
        },
        "improvement": {
            "speedup": statistics.mean(db_times) / statistics.mean(cache_times),
            "latency_reduction_ms": statistics.mean(db_times) - statistics.mean(cache_times),
            "latency_reduction_percent": (
                (statistics.mean(db_times) - statistics.mean(cache_times)) 
                / statistics.mean(db_times) * 100
            ),
        }
    }
    
    return results


# ì‹¤í–‰ ì˜ˆì‹œ
async def run_benchmark():
    results = await benchmark_cache_vs_db(1000)
    
    print("=== ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ===")
    print(f"\nDB ì§ì ‘ ì¡°íšŒ:")
    print(f"  í‰ê· : {results['db_query']['mean']:.2f}ms")
    print(f"  ì¤‘ê°„ê°’: {results['db_query']['median']:.2f}ms")
    print(f"  ìµœì†Œ/ìµœëŒ€: {results['db_query']['min']:.2f}ms / {results['db_query']['max']:.2f}ms")
    
    print(f"\nìºì‹œ ì¡°íšŒ:")
    print(f"  í‰ê· : {results['cache_query']['mean']:.2f}ms")
    print(f"  ì¤‘ê°„ê°’: {results['cache_query']['median']:.2f}ms")
    print(f"  ìµœì†Œ/ìµœëŒ€: {results['cache_query']['min']:.2f}ms / {results['cache_query']['max']:.2f}ms")
    
    print(f"\nê°œì„  íš¨ê³¼:")
    print(f"  ì†ë„ í–¥ìƒ: {results['improvement']['speedup']:.2f}x")
    print(f"  ì§€ì—° ì‹œê°„ ê°ì†Œ: {results['improvement']['latency_reduction_ms']:.2f}ms")
    print(f"  ë°±ë¶„ìœ¨ ê°œì„ : {results['improvement']['latency_reduction_percent']:.1f}%")
```

### ì‹¤ì „ ê²°ê³¼ ì˜ˆì‹œ

**í…ŒìŠ¤íŠ¸ í™˜ê²½:**
- AWS EC2 t3.medium (2 vCPU, 4GB RAM)
- RDS PostgreSQL db.t3.micro
- ElastiCache Redis cache.t3.micro
- Django Ninja + uvicorn

**ì¸¡ì • ê²°ê³¼:**

| ì§€í‘œ | DB ì§ì ‘ ì¡°íšŒ | ìºì‹œ ì¡°íšŒ | ê°œì„ ìœ¨ |
|------|------------|----------|--------|
| í‰ê·  ì‘ë‹µ ì‹œê°„ | 87.3ms | 4.2ms | **95.2%** |
| P95 ì‘ë‹µ ì‹œê°„ | 143ms | 8.1ms | **94.3%** |
| P99 ì‘ë‹µ ì‹œê°„ | 198ms | 12.4ms | **93.7%** |
| ì²˜ë¦¬ëŸ‰ (RPS) | 245 req/s | 4,800 req/s | **19.6x** |
| CPU ì‚¬ìš©ë¥  | 68% | 12% | **82.4%** |
| DB ì—°ê²° ìˆ˜ | í‰ê·  42ê°œ | í‰ê·  3ê°œ | **92.9%** |

**ë¹„ìš© ì ˆê° íš¨ê³¼:**

```
ì›”ê°„ ë¹„ìš© ë¹„êµ (AWS ê¸°ì¤€)

ìºì‹œ ë„ì… ì „:
- RDS db.t3.large: $146/ì›”
- ì½ê¸° ë³µì œë³¸ 2ëŒ€: $292/ì›”
- ì´ DB ë¹„ìš©: $438/ì›”

ìºì‹œ ë„ì… í›„:
- RDS db.t3.small: $73/ì›”  (ë‹¤ìš´ê·¸ë ˆì´ë“œ)
- Redis cache.t3.micro: $12/ì›”
- ì´ ë¹„ìš©: $85/ì›”

ì›”ê°„ ì ˆê°: $353 (80.6% ê°ì†Œ)
ì—°ê°„ ì ˆê°: $4,236
```

## ğŸ“ ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸ ë° ê¶Œì¥ì‚¬í•­

### ìºì‹œ ì „ëµ ì„ íƒ ê°€ì´ë“œ

| ì‹œë‚˜ë¦¬ì˜¤ | ê¶Œì¥ ì „ëµ | ì´ìœ  |
|---------|---------|------|
| ìì£¼ ì½íˆê³  ê±°ì˜ ì•ˆ ë°”ë€ŒëŠ” ë°ì´í„° | Cache-Aside + Long TTL | ë†’ì€ íˆíŠ¸ìœ¨, ê°„ë‹¨í•œ êµ¬í˜„ |
| ì‹¤ì‹œê°„ì„±ì´ ì¤‘ìš”í•œ ë°ì´í„° | Write-Through | ì¦‰ì‹œ ì¼ê´€ì„± ë³´ì¥ |
| ë†’ì€ ì“°ê¸° ë¹ˆë„ | Write-Behind | ì“°ê¸° ì„±ëŠ¥ ìµœì í™” |
| ëŒ€ê·œëª¨ íŠ¸ë˜í”½ ì˜ˆìƒ | Cache Warming | ì´ˆê¸° ë¡œë“œ ì‹œê°„ ë‹¨ì¶• |
| ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì˜ˆì • | ë²„ì „ ê´€ë¦¬ | ì•ˆì „í•œ ë°°í¬ |

### ìºì‹œ ë¬´íš¨í™” ì „ëµ ì„ íƒ

| ë°ì´í„° íŠ¹ì„± | ë¬´íš¨í™” ì „ëµ | ì„¤ëª… |
|-----------|-----------|------|
| ë³€ê²½ ë¹ˆë„ ë‚®ìŒ | Immediate | ì¦‰ì‹œ ì‚­ì œë¡œ ì¼ê´€ì„± ë³´ì¥ |
| ë³€ê²½ ë¹ˆë„ ë†’ìŒ | TTL Reduction | ì ì§„ì  ë¬´íš¨í™” |
| ì¼ê´€ì„± ì¤‘ìš” | Immediate + Selective | ê´€ë ¨ ìºì‹œë§Œ ì‚­ì œ |
| ëŒ€ê·œëª¨ ë³€ê²½ | Version Increment | ì „ì²´ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë¬´íš¨í™” |

### ì²´í¬ë¦¬ìŠ¤íŠ¸

**ì„¤ê³„ ë‹¨ê³„:**
- [ ] ìºì‹œí•  ë°ì´í„° ì‹ë³„ (ì½ê¸°:ì“°ê¸° ë¹„ìœ¨ 5:1 ì´ìƒ)
- [ ] TTL ì „ëµ ìˆ˜ë¦½ (ë°ì´í„° íŠ¹ì„±ë³„)
- [ ] í‚¤ ë„¤ì´ë° ê·œì¹™ ì •ì˜
- [ ] ë¬´íš¨í™” ì „ëµ ìˆ˜ë¦½
- [ ] ë©”ëª¨ë¦¬ ìš©ëŸ‰ ê³„íš

**êµ¬í˜„ ë‹¨ê³„:**
- [ ] ë¹„ë™ê¸° Redis í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
- [ ] ModelSchema ì •ì˜
- [ ] ìºì‹œ ë§¤ë‹ˆì € êµ¬í˜„
- [ ] ë¬´íš¨í™” ë¡œì§ êµ¬í˜„ (ì‹œê·¸ë„ í™œìš©)
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ë° í´ë°± êµ¬í˜„

**ëª¨ë‹ˆí„°ë§:**
- [ ] íˆíŠ¸ìœ¨ ì¸¡ì • (ëª©í‘œ: 80% ì´ìƒ)
- [ ] ì‘ë‹µ ì‹œê°„ ì¸¡ì •
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
- [ ] ìºì‹œ ë¯¸ìŠ¤ íŒ¨í„´ ë¶„ì„
- [ ] ë¬´íš¨í™” ë¹ˆë„ ì¶”ì 

**í”„ë¡œë•ì…˜ ë°°í¬:**
- [ ] Redis ê³ ê°€ìš©ì„± ì„¤ì • (Sentinel/Cluster)
- [ ] ë°±ì—… ì „ëµ ìˆ˜ë¦½ (í•„ìš”ì‹œ)
- [ ] ì„œí‚· ë¸Œë ˆì´ì»¤ ì ìš©
- [ ] í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •
- [ ] ì•Œë¦¼ ì„¤ì • (íˆíŠ¸ìœ¨ ì €í•˜, ì—°ê²° ì‹¤íŒ¨ ë“±)

## ğŸ”— ì°¸ê³  ìë£Œ ë° ì¶”ê°€ í•™ìŠµ

### ê³µì‹ ë¬¸ì„œ
- [Django Ninja ê³µì‹ ë¬¸ì„œ](https://django-ninja.rest-framework.com/)
- [Redis ê³µì‹ ë¬¸ì„œ](https://redis.io/docs/)
- [Pydantic ê³µì‹ ë¬¸ì„œ](https://docs.pydantic.dev/)

### ì¶”ì²œ ë„ì„œ ë° ì•„í‹°í´
- "Designing Data-Intensive Applications" by Martin Kleppmann (9ì¥: ì¼ê´€ì„±ê³¼ í•©ì˜)
- Redis in Action by Josiah Carlson
- [AWS ElastiCache Best Practices](https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/BestPractices.html)

### ë‹¤ìŒ ë‹¨ê³„
1. **ë¶„ì‚° ìºì‹œ íŒ¨í„´**: Redis Clusterë¥¼ í™œìš©í•œ ìˆ˜í‰ í™•ì¥
2. **ìºì‹œ ì˜ˆì—´ ìë™í™”**: Celeryë¥¼ ì‚¬ìš©í•œ ìŠ¤ì¼€ì¤„ë§
3. **ê³ ê¸‰ ë¬´íš¨í™” íŒ¨í„´**: Pub/Subì„ í™œìš©í•œ ì‹¤ì‹œê°„ ë¬´íš¨í™”
4. **ë©€í‹° ë ˆì´ì–´ ìºì‹œ**: CDN + Redis + ë¡œì»¬ ë©”ëª¨ë¦¬ ì¡°í•©

## ë§ˆë¬´ë¦¬

Django Ninjaì™€ Redisë¥¼ í™œìš©í•œ ì „ëµì  ìºì‹œ ê´€ë¦¬ ì‹œìŠ¤í…œì€ API ì„±ëŠ¥ì„ ê·¹ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•µì‹¬ì€ ë‹¨ìˆœíˆ ìºì‹œë¥¼ ë„ì…í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë°ì´í„° íŠ¹ì„±ì— ë§ëŠ” ìºì‹œ ì „ëµì„ ìˆ˜ë¦½í•˜ê³ , ì²´ê³„ì ì¸ ë²„ì „ ê´€ë¦¬ì™€ ë¬´íš¨í™” ë©”ì»¤ë‹ˆì¦˜ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì´ ê¸€ì—ì„œ ë‹¤ë£¬ ë‚´ìš©ì„ ìš”ì•½í•˜ë©´:

1. **ì•„í‚¤í…ì²˜ ì„¤ê³„**: ë¹„ë™ê¸° Redis í´ë¼ì´ì–¸íŠ¸ì™€ ì²´ê³„ì ì¸ í”„ë¡œì íŠ¸ êµ¬ì¡°
2. **í‚¤ ê´€ë¦¬**: ì¼ê´€ëœ ë„¤ì´ë° ê·œì¹™ê³¼ ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ
3. **ì§ë ¬í™”**: ModelSchemaë¥¼ í†µí•œ íƒ€ì… ì•ˆì „ ìºì‹œ ê´€ë¦¬
4. **ë¬´íš¨í™” ì „ëµ**: ìƒí™©ë³„ ìµœì  ë¬´íš¨í™” ë°©ë²• ì„ íƒ
5. **ê³ ê¸‰ íŒ¨í„´**: Cache-Aside, Write-Through, Refresh-Ahead ë“±
6. **ëª¨ë‹ˆí„°ë§**: ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê³¼ ì„±ëŠ¥ ì¸¡ì •
7. **í”„ë¡œë•ì…˜**: ì—ëŸ¬ ì²˜ë¦¬, ì„œí‚· ë¸Œë ˆì´ì»¤, í—¬ìŠ¤ ì²´í¬

ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ë°ì´í„° íŠ¹ì„±ê³¼ íŠ¸ë˜í”½ íŒ¨í„´ì— ë”°ë¼ ì „ëµì„ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ê³¼ ìµœì í™”ë¥¼ í†µí•´ ìºì‹œ ì‹œìŠ¤í…œì˜ íš¨ê³¼ë¥¼ ê·¹ëŒ€í™”í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
