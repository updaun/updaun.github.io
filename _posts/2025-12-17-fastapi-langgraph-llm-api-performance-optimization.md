---
layout: post
title: "FastAPI + LangGraph MSA í™˜ê²½ì—ì„œ LLM Streaming API ì„±ëŠ¥ ìµœì í™” ì™„ë²½ ê°€ì´ë“œ"
date: 2025-12-17 09:00:00 +0900
categories: [Backend, Performance, AI]
tags: [FastAPI, LangGraph, LLM, MSA, Redis, RAG, VectorDB, Performance, Streaming, Optimization]
description: "FastAPIì™€ LangGraph ê¸°ë°˜ MSA LLM Streaming APIì˜ ì„±ëŠ¥ì„ íšê¸°ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ëŠ” ì‹¤ì „ ìµœì í™” ê¸°ë²•"
---

## ëª©ì°¨

1. [ì†Œê°œ - ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ê°œìš”](#1-ì†Œê°œ---ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜-ê°œìš”)
2. [í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„ ë° ë³‘ëª© ì§€ì  ì‹ë³„](#2-í˜„ì¬-ì‹œìŠ¤í…œ-ë¶„ì„-ë°-ë³‘ëª©-ì§€ì -ì‹ë³„)
3. [FastAPI ì„±ëŠ¥ ìµœì í™”](#3-fastapi-ì„±ëŠ¥-ìµœì í™”)
4. [LangGraph ì›Œí¬í”Œë¡œìš° ìµœì í™”](#4-langgraph-ì›Œí¬í”Œë¡œìš°-ìµœì í™”)
5. [Redis Checkpoint ìµœì í™”](#5-redis-checkpoint-ìµœì í™”)
6. [Vector DB & RAG ìµœì í™”](#6-vector-db--rag-ìµœì í™”)
7. [Streaming ì‘ë‹µ ìµœì í™”](#7-streaming-ì‘ë‹µ-ìµœì í™”)
8. [MSA ë ˆë²¨ ìµœì í™”](#8-msa-ë ˆë²¨-ìµœì í™”)
9. [ëª¨ë‹ˆí„°ë§ ë° í”„ë¡œíŒŒì¼ë§](#9-ëª¨ë‹ˆí„°ë§-ë°-í”„ë¡œíŒŒì¼ë§)
10. [ê²°ë¡  ë° ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸](#10-ê²°ë¡ -ë°-ìµœì í™”-ì²´í¬ë¦¬ìŠ¤íŠ¸)

---

## 1. ì†Œê°œ - ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ê°œìš”

### 1.1 ê¸°ìˆ  ìŠ¤íƒ ì†Œê°œ

í˜„ëŒ€ì ì¸ LLM ì„œë¹„ìŠ¤ëŠ” ë‹¤ì–‘í•œ ê¸°ìˆ ì˜ ì¡°í•©ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ë³¸ ê°€ì´ë“œì—ì„œ ë‹¤ë£¨ëŠ” ì‹œìŠ¤í…œì€ ë‹¤ìŒê³¼ ê°™ì€ ê¸°ìˆ  ìŠ¤íƒì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Client Layer                          â”‚
â”‚                    (Web/Mobile/API Client)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     API Gateway                              â”‚
â”‚              (Load Balancer + Rate Limiter)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI     â”‚            â”‚   FastAPI      â”‚  â”‚  FastAPI    â”‚
â”‚   Service 1   â”‚            â”‚   Service 2    â”‚  â”‚  Service N  â”‚
â”‚               â”‚            â”‚                â”‚  â”‚             â”‚
â”‚  - LangGraph  â”‚            â”‚  - LangGraph   â”‚  â”‚ - LangGraph â”‚
â”‚  - RAG Logic  â”‚            â”‚  - RAG Logic   â”‚  â”‚ - RAG Logic â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
        â”‚                             â”‚                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Redis     â”‚            â”‚   Vector DB    â”‚  â”‚  LLM API    â”‚
â”‚  (Checkpoint) â”‚            â”‚  (Chroma/Qdrantâ”‚  â”‚  (OpenAI/   â”‚
â”‚               â”‚            â”‚   /Pinecone)   â”‚  â”‚   Anthropic)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•µì‹¬ êµ¬ì„± ìš”ì†Œ:**

1. **FastAPI**: ê³ ì„±ëŠ¥ ë¹„ë™ê¸° Python ì›¹ í”„ë ˆì„ì›Œí¬
2. **LangGraph**: LLM ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (ìƒíƒœ ê´€ë¦¬, ê·¸ë˜í”„ ê¸°ë°˜ ì‹¤í–‰)
3. **Redis**: Checkpoint ì €ì¥ ë° ìºì‹±
4. **Vector DB**: ì„ë² ë”© ê²€ìƒ‰ (RAG)
5. **LLM Provider**: OpenAI, Anthropic, ìì²´ ëª¨ë¸ ë“±

### 1.2 ì‹œìŠ¤í…œ íŠ¹ì„±

**Streaming APIì˜ íŠ¹ì§•:**

```python
# ì¼ë°˜ì ì¸ Streaming API êµ¬ì¡°
@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    async def generate():
        async for chunk in langgraph_workflow.astream(
            {"messages": request.messages},
            config={"configurable": {"thread_id": request.thread_id}}
        ):
            # 1. Redisì—ì„œ checkpoint ë¡œë“œ
            # 2. Vector DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (RAG)
            # 3. LLM API í˜¸ì¶œ
            # 4. ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°
            yield f"data: {json.dumps(chunk)}\n\n"
    
    return StreamingResponse(generate(), media_type="text/event-stream")
```

**ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­:**

| ì§€í‘œ | ëª©í‘œ | í˜„ì¬ (ìµœì í™” ì „) |
|------|------|------------------|
| ì²« í† í° ì‘ë‹µ ì‹œê°„ (TTFT) | < 500ms | 1.2s ~ 2.5s |
| í† í°ë‹¹ ì²˜ë¦¬ ì‹œê°„ | < 50ms | 80ms ~ 150ms |
| ë™ì‹œ ì²˜ë¦¬ ìš”ì²­ | 500+ | 200 ~ 300 |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | < 2GB per instance | 3.5GB ~ 4.5GB |
| Redis ë ˆì´í„´ì‹œ | < 10ms | 25ms ~ 50ms |
| Vector ê²€ìƒ‰ ì†ë„ | < 100ms | 300ms ~ 600ms |

### 1.3 ì„±ëŠ¥ ë³‘ëª©ì˜ ì£¼ìš” ì›ì¸

**ì¼ë°˜ì ì¸ ì„±ëŠ¥ ì €í•˜ ìš”ì¸:**

```python
# âŒ ì•ˆí‹°íŒ¨í„´ ì˜ˆì‹œ
async def process_chat(message: str, thread_id: str):
    # ë¬¸ì œ 1: ë™ê¸° ë¸”ë¡œí‚¹ í˜¸ì¶œ
    checkpoint = redis_client.get(f"checkpoint:{thread_id}")  # ğŸ”´ ë¸”ë¡œí‚¹
    
    # ë¬¸ì œ 2: ë§¤ë²ˆ ì„ë² ë”© ìƒì„±
    embedding = await openai.embeddings.create(input=message)  # ğŸ”´ ì¤‘ë³µ ê³„ì‚°
    
    # ë¬¸ì œ 3: ìˆœì°¨ ì‹¤í–‰
    docs = await vector_db.search(embedding)  # â³ ëŒ€ê¸°
    response = await llm.generate(context=docs)  # â³ ëŒ€ê¸°
    
    # ë¬¸ì œ 4: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹œ ë¸”ë¡œí‚¹
    redis_client.set(f"checkpoint:{thread_id}", checkpoint)  # ğŸ”´ ë¸”ë¡œí‚¹
```

**ìµœì í™” í•„ìš” ì˜ì—­:**

1. ğŸ”´ **I/O ë³‘ëª©**: Redis, Vector DB, LLM API í˜¸ì¶œ
2. ğŸ”´ **ë©”ëª¨ë¦¬ ë³‘ëª©**: ëŒ€ê·œëª¨ ì»¨í…ìŠ¤íŠ¸, ì„ë² ë”© ìºì‹±
3. ğŸ”´ **CPU ë³‘ëª©**: ì„ë² ë”© ìƒì„±, í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
4. ğŸ”´ **ë„¤íŠ¸ì›Œí¬ ë³‘ëª©**: ì„œë¹„ìŠ¤ ê°„ í†µì‹ , ì™¸ë¶€ API í˜¸ì¶œ

### 1.4 ìµœì í™” ì „ëµ ê°œìš”

**ì„±ëŠ¥ ìµœì í™”ëŠ” ë‹¤ìŒ ë ˆë²¨ë¡œ ì§„í–‰ë©ë‹ˆë‹¤:**

```
Level 1: ì½”ë“œ ë ˆë²¨ ìµœì í™”
  â”œâ”€ ë¹„ë™ê¸° I/O ìµœëŒ€ í™œìš©
  â”œâ”€ ë¶ˆí•„ìš”í•œ ì—°ì‚° ì œê±°
  â””â”€ íš¨ìœ¨ì ì¸ ë°ì´í„° êµ¬ì¡°

Level 2: ì¸í”„ë¼ ë ˆë²¨ ìµœì í™”
  â”œâ”€ Redis íŒŒì´í”„ë¼ì´ë‹
  â”œâ”€ Vector DB ì¸ë±ìŠ¤ íŠœë‹
  â””â”€ ì»¤ë„¥ì…˜ í’€ ê´€ë¦¬

Level 3: ì•„í‚¤í…ì²˜ ë ˆë²¨ ìµœì í™”
  â”œâ”€ ìºì‹± ì „ëµ
  â”œâ”€ ë°°ì¹˜ ì²˜ë¦¬
  â””â”€ ë³‘ë ¬ ì‹¤í–‰

Level 4: ì‹œìŠ¤í…œ ë ˆë²¨ ìµœì í™”
  â”œâ”€ ìˆ˜í‰ í™•ì¥ (Scale-out)
  â”œâ”€ ë¡œë“œ ë°¸ëŸ°ì‹±
  â””â”€ CDN ë° ì—£ì§€ ìºì‹±
```

**ìµœì í™” ìš°ì„ ìˆœìœ„ ê²°ì • (Pareto ì›ì¹™):**

```python
# ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ê²°ê³¼ ì˜ˆì‹œ
{
    "total_request_time": "2.5s",
    "breakdown": {
        "vector_search": "0.6s (24%)",      # ğŸ‘ˆ 1ìˆœìœ„ ìµœì í™”
        "llm_api_call": "1.2s (48%)",       # ğŸ‘ˆ 2ìˆœìœ„ (ì™¸ë¶€ ì˜ì¡´)
        "checkpoint_load": "0.3s (12%)",    # ğŸ‘ˆ 3ìˆœìœ„ ìµœì í™”
        "embedding_generation": "0.2s (8%)",
        "preprocessing": "0.2s (8%)"
    }
}
```

> **ğŸ’¡ í•µì‹¬ ì›ì¹™**: ì¸¡ì •í•˜ì§€ ì•Šìœ¼ë©´ ìµœì í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•­ìƒ í”„ë¡œíŒŒì¼ë§ë¶€í„° ì‹œì‘í•˜ì„¸ìš”!

ë‹¤ìŒ ì„¹ì…˜ì—ì„œëŠ” í˜„ì¬ ì‹œìŠ¤í…œì„ ì •í™•íˆ ë¶„ì„í•˜ê³  ë³‘ëª© ì§€ì ì„ ì‹ë³„í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

---

## 2. í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„ ë° ë³‘ëª© ì§€ì  ì‹ë³„

### 2.1 ì„±ëŠ¥ ì¸¡ì • ë„êµ¬ ì„¤ì •

ìµœì í™”ì˜ ì²« ë‹¨ê³„ëŠ” ì •í™•í•œ ì¸¡ì •ì…ë‹ˆë‹¤. ë‹¤ìŒ ë„êµ¬ë“¤ì„ í™œìš©í•˜ì—¬ ì‹œìŠ¤í…œì„ ë¶„ì„í•©ë‹ˆë‹¤.

**ê¸°ë³¸ í”„ë¡œíŒŒì¼ë§ ë¯¸ë“¤ì›¨ì–´:**

```python
# middleware/profiling.py
import time
import logging
from fastapi import Request
from starlette.middleware.base import BaseHTTPMiddleware
from typing import Dict
import json

logger = logging.getLogger(__name__)

class ProfilingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        start_time = time.time()
        
        # ìš”ì²­ ë©”íƒ€ë°ì´í„°
        request.state.timings = {}
        
        # ì‘ë‹µ ì²˜ë¦¬
        response = await call_next(request)
        
        # ì´ ì²˜ë¦¬ ì‹œê°„
        total_time = time.time() - start_time
        
        # ë¡œê¹…
        logger.info(json.dumps({
            "path": request.url.path,
            "method": request.method,
            "total_time": f"{total_time:.3f}s",
            "timings": getattr(request.state, "timings", {}),
            "status_code": response.status_code
        }))
        
        # ì‘ë‹µ í—¤ë”ì— íƒ€ì´ë° ì •ë³´ ì¶”ê°€
        response.headers["X-Process-Time"] = str(total_time)
        
        return response


# íƒ€ì´ë° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
class Timer:
    def __init__(self, request: Request, name: str):
        self.request = request
        self.name = name
        self.start = None
    
    def __enter__(self):
        self.start = time.time()
        return self
    
    def __exit__(self, *args):
        duration = time.time() - self.start
        if hasattr(self.request.state, "timings"):
            self.request.state.timings[self.name] = f"{duration:.3f}s"
```

**ì‚¬ìš© ì˜ˆì‹œ:**

```python
# main.py
from fastapi import FastAPI, Request
from middleware.profiling import ProfilingMiddleware, Timer

app = FastAPI()
app.add_middleware(ProfilingMiddleware)

@app.post("/chat/stream")
async def chat_stream(request: Request, chat_request: ChatRequest):
    # Redis checkpoint ë¡œë“œ ì¸¡ì •
    with Timer(request, "redis_load"):
        checkpoint = await redis_client.get(f"checkpoint:{chat_request.thread_id}")
    
    # Vector ê²€ìƒ‰ ì¸¡ì •
    with Timer(request, "vector_search"):
        docs = await vector_db.search(chat_request.query, k=5)
    
    # LLM í˜¸ì¶œ ì¸¡ì •
    with Timer(request, "llm_generation"):
        async for chunk in langgraph_app.astream(
            {"messages": chat_request.messages},
            config={"configurable": {"thread_id": chat_request.thread_id}}
        ):
            yield chunk
```

### 2.2 ë³‘ëª© ì§€ì  ì‹ë³„

**Python cProfile í™œìš©:**

```python
# profiling/cpu_profiler.py
import cProfile
import pstats
import io
from functools import wraps

def profile_function(func):
    """í•¨ìˆ˜ ë‹¨ìœ„ CPU í”„ë¡œíŒŒì¼ë§"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        profiler = cProfile.Profile()
        profiler.enable()
        
        result = await func(*args, **kwargs)
        
        profiler.disable()
        
        # ê²°ê³¼ ì¶œë ¥
        stream = io.StringIO()
        stats = pstats.Stats(profiler, stream=stream)
        stats.sort_stats('cumulative')
        stats.print_stats(20)  # ìƒìœ„ 20ê°œ í•¨ìˆ˜
        
        print(f"\n=== Profile for {func.__name__} ===")
        print(stream.getvalue())
        
        return result
    
    return wrapper


# ì‚¬ìš© ì˜ˆì‹œ
@profile_function
async def process_rag_query(query: str):
    embedding = await generate_embedding(query)
    results = await vector_db.search(embedding)
    return results
```

**ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ (memory_profiler):**

```python
# requirements.txtì— ì¶”ê°€
# memory-profiler==0.61.0

# profiling/memory_profiler.py
from memory_profiler import profile

@profile
async def load_large_checkpoint(thread_id: str):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì """
    checkpoint = await redis_client.get(f"checkpoint:{thread_id}")
    
    # ëŒ€ìš©ëŸ‰ ì²´í¬í¬ì¸íŠ¸ ì²˜ë¦¬
    parsed = json.loads(checkpoint)  # ğŸ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
    
    return parsed
```

### 2.3 ì‹¤ì œ í”„ë¡œíŒŒì¼ë§ ê²°ê³¼ ë¶„ì„

**Case Study: ì‹¤ì œ ë³‘ëª© ì§€ì  ë°œê²¬**

```python
# í”„ë¡œíŒŒì¼ë§ ê²°ê³¼ ì˜ˆì‹œ
"""
=== Request: POST /chat/stream ===
Total Time: 2.45s

Breakdown:
  redis_load:        0.28s  (11.4%)  ğŸŸ¡ ìµœì í™” ê°€ëŠ¥
  vector_search:     0.65s  (26.5%)  ğŸ”´ ì£¼ìš” ë³‘ëª©
  embedding_gen:     0.18s  (7.3%)   ğŸŸ¢ ì–‘í˜¸
  llm_api_call:      1.15s  (46.9%)  ğŸ”´ ì™¸ë¶€ ì˜ì¡´
  checkpoint_save:   0.19s  (7.8%)   ğŸŸ¡ ìµœì í™” ê°€ëŠ¥

Memory Usage:
  Peak: 3.8 GB
  Checkpoint data: 450 MB  ğŸ”´ ê³¼ë‹¤
  Vector cache: 1.2 GB     ğŸŸ¡ ìµœì í™” ê°€ëŠ¥
  Model context: 2.1 GB    ğŸ”´ ê³¼ë‹¤
"""
```

**ë³‘ëª© ì§€ì  ìš°ì„ ìˆœìœ„:**

```python
# profiling/bottleneck_analyzer.py
from dataclasses import dataclass
from typing import List
import statistics

@dataclass
class BottleneckReport:
    component: str
    avg_time: float
    p95_time: float
    p99_time: float
    percentage: float
    severity: str  # HIGH, MEDIUM, LOW
    
    @property
    def optimization_priority(self) -> int:
        """ìµœì í™” ìš°ì„ ìˆœìœ„ (1-10)"""
        if self.percentage > 20 and self.p95_time > 0.5:
            return 10  # ê¸´ê¸‰
        elif self.percentage > 15 or self.p95_time > 0.3:
            return 7   # ë†’ìŒ
        elif self.percentage > 10:
            return 5   # ì¤‘ê°„
        else:
            return 3   # ë‚®ìŒ


class PerformanceAnalyzer:
    def __init__(self):
        self.metrics = []
    
    def analyze(self, timings: List[Dict]) -> List[BottleneckReport]:
        """íƒ€ì´ë° ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë³‘ëª© ë³´ê³ ì„œ ìƒì„±"""
        reports = []
        
        for component in ["redis_load", "vector_search", "llm_api_call"]:
            times = [t.get(component, 0) for t in timings if component in t]
            
            if not times:
                continue
            
            total_avg = statistics.mean([sum(t.values()) for t in timings])
            
            report = BottleneckReport(
                component=component,
                avg_time=statistics.mean(times),
                p95_time=statistics.quantiles(times, n=20)[18],  # 95th percentile
                p99_time=statistics.quantiles(times, n=100)[98], # 99th percentile
                percentage=(statistics.mean(times) / total_avg) * 100,
                severity="HIGH" if statistics.mean(times) > 0.5 else "MEDIUM"
            )
            
            reports.append(report)
        
        # ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬
        return sorted(reports, key=lambda r: r.optimization_priority, reverse=True)
```

### 2.4 ì£¼ìš” ë³‘ëª© ì§€ì  ì •ë¦¬

**1. Vector Search (26.5% - ìµœìš°ì„  ìµœì í™” ëŒ€ìƒ)**

```python
# ë¬¸ì œ ì½”ë“œ
async def rag_query(query: str):
    # ğŸ”´ ë¬¸ì œ: ë§¤ë²ˆ ì„ë² ë”© ìƒì„±
    embedding = await openai_client.embeddings.create(
        input=query,
        model="text-embedding-ada-002"
    )
    
    # ğŸ”´ ë¬¸ì œ: ë¹„íš¨ìœ¨ì ì¸ ê²€ìƒ‰ (ì¸ë±ìŠ¤ ë¯¸ì‚¬ìš©)
    results = await vector_db.similarity_search(
        embedding.data[0].embedding,
        k=10  # ë„ˆë¬´ ë§ì€ ê²°ê³¼
    )
    
    return results
```

**ìµœì í™” ë°©í–¥:**
- âœ… ì„ë² ë”© ìºì‹± (Redis)
- âœ… ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì¤„ì´ê¸° (k=10 â†’ k=3)
- âœ… HNSW ì¸ë±ìŠ¤ í™œìš©
- âœ… ë°°ì¹˜ ê²€ìƒ‰ êµ¬í˜„

**2. Redis Checkpoint (11.4% - ì¤‘ìš”ë„ ë†’ìŒ)**

```python
# ë¬¸ì œ ì½”ë“œ
async def save_checkpoint(thread_id: str, state: Dict):
    # ğŸ”´ ë¬¸ì œ: ë™ê¸° ë¸”ë¡œí‚¹ í˜¸ì¶œ
    redis_client.set(
        f"checkpoint:{thread_id}",
        json.dumps(state),  # ğŸ”´ ì••ì¶• ì—†ìŒ
        ex=3600
    )
```

**ìµœì í™” ë°©í–¥:**
- âœ… ë¹„ë™ê¸° Redis í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© (redis.asyncio)
- âœ… ë°ì´í„° ì••ì¶• (gzip)
- âœ… íŒŒì´í”„ë¼ì´ë‹ í™œìš©
- âœ… TTL ìµœì í™”

**3. LLM API Call (46.9% - ì™¸ë¶€ ì˜ì¡´)**

```python
# ë¬¸ì œ: ì™¸ë¶€ API ì˜ì¡´ë„ê°€ ë†’ìŒ
# ìµœì í™”ê°€ ì œí•œì ì´ì§€ë§Œ ê°œì„  ê°€ëŠ¥í•œ ë¶€ë¶„:
# - í”„ë¡¬í”„íŠ¸ ìµœì í™” (í† í° ìˆ˜ ê°ì†Œ)
# - Streaming ë²„í¼ë§
# - íƒ€ì„ì•„ì›ƒ ì„¤ì •
```

**ìµœì í™” ë°©í–¥:**
- âœ… í”„ë¡¬í”„íŠ¸ í† í° ìµœì í™” (ì»¨í…ìŠ¤íŠ¸ ì••ì¶•)
- âœ… ë³‘ë ¬ ìš”ì²­ (ì—¬ëŸ¬ ëª¨ë¸ ë™ì‹œ í˜¸ì¶œ)
- âœ… Fallback ì „ëµ (íƒ€ì„ì•„ì›ƒ ì‹œ ìºì‹œëœ ì‘ë‹µ)

### 2.5 ì„±ëŠ¥ ëª©í‘œ ì„¤ì •

**ì¸¡ì • ê²°ê³¼ ê¸°ë°˜ ëª©í‘œ:**

| êµ¬ì„± ìš”ì†Œ | í˜„ì¬ (P95) | ëª©í‘œ (P95) | ê°œì„ ìœ¨ |
|-----------|-----------|-----------|--------|
| Vector Search | 650ms | 150ms | 77% â†“ |
| Redis Load | 280ms | 50ms | 82% â†“ |
| Checkpoint Save | 190ms | 30ms | 84% â†“ |
| Total TTFT | 2.5s | 500ms | 80% â†“ |
| Throughput | 200 req/s | 800 req/s | 300% â†‘ |

**ìµœì í™” ë¡œë“œë§µ:**

```
Week 1-2: ë¹ ë¥¸ ì„±ê³¼ (Quick Wins)
  â”œâ”€ Redis ë¹„ë™ê¸° ì „í™˜
  â”œâ”€ Vector ê²€ìƒ‰ kê°’ ì¡°ì •
  â””â”€ ë¶ˆí•„ìš”í•œ ë¡œê¹… ì œê±°
  ì˜ˆìƒ ê°œì„ : 30-40%

Week 3-4: ì¤‘ê¸‰ ìµœì í™”
  â”œâ”€ ì„ë² ë”© ìºì‹± êµ¬í˜„
  â”œâ”€ Redis íŒŒì´í”„ë¼ì´ë‹
  â””â”€ LangGraph ë…¸ë“œ ë³‘ë ¬í™”
  ì˜ˆìƒ ê°œì„ : ì¶”ê°€ 30-40%

Week 5-6: ê³ ê¸‰ ìµœì í™”
  â”œâ”€ Vector DB ì¸ë±ìŠ¤ íŠœë‹
  â”œâ”€ Checkpoint ì••ì¶•
  â””â”€ MSA ë ˆë²¨ ìºì‹±
  ì˜ˆìƒ ê°œì„ : ì¶”ê°€ 20-30%
```

ë‹¤ìŒ ì„¹ì…˜ì—ì„œëŠ” FastAPI ë ˆë²¨ì—ì„œì˜ êµ¬ì²´ì ì¸ ì„±ëŠ¥ ìµœì í™” ê¸°ë²•ì„ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.

---

## 3. FastAPI ì„±ëŠ¥ ìµœì í™”

### 3.1 ë¹„ë™ê¸° I/O ì™„ì „ í™œìš©

FastAPIëŠ” ë¹„ë™ê¸°ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì§€ì›í•˜ì§€ë§Œ, ì‹¤ìˆ˜ë¡œ ë™ê¸° ì½”ë“œë¥¼ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ì´ í¬ê²Œ ì €í•˜ë©ë‹ˆë‹¤.

**âŒ ì•ˆí‹°íŒ¨í„´ - ë™ê¸° ë¸”ë¡œí‚¹ ì½”ë“œ:**

```python
# BAD: ë™ê¸° Redis í´ë¼ì´ì–¸íŠ¸
import redis

redis_client = redis.Redis(host='localhost', port=6379)

@app.post("/chat")
async def chat(request: ChatRequest):
    # âš ï¸ ë¹„ë™ê¸° í•¨ìˆ˜ì—ì„œ ë™ê¸° í˜¸ì¶œ - ì´ë²¤íŠ¸ ë£¨í”„ ë¸”ë¡œí‚¹!
    checkpoint = redis_client.get(f"checkpoint:{request.thread_id}")
    
    # âš ï¸ ìˆœì°¨ ì‹¤í–‰ - ë¶ˆí•„ìš”í•œ ëŒ€ê¸°
    embedding = await generate_embedding(request.query)
    docs = await vector_search(embedding)
    
    return {"response": "..."}
```

**âœ… ìµœì í™” - ì™„ì „í•œ ë¹„ë™ê¸° êµ¬í˜„:**

```python
# GOOD: ë¹„ë™ê¸° Redis í´ë¼ì´ì–¸íŠ¸
import redis.asyncio as redis
from typing import List, Tuple
import asyncio

# ë¹„ë™ê¸° Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
redis_client = redis.Redis(
    host='localhost',
    port=6379,
    decode_responses=True,
    max_connections=50  # ì»¤ë„¥ì…˜ í’€ í¬ê¸°
)

@app.post("/chat")
async def chat(request: ChatRequest):
    # âœ… ë³‘ë ¬ ì‹¤í–‰ (asyncio.gather)
    checkpoint, embedding = await asyncio.gather(
        redis_client.get(f"checkpoint:{request.thread_id}"),
        generate_embedding(request.query)
    )
    
    # Vector ê²€ìƒ‰ì€ ì„ë² ë”© ê²°ê³¼ê°€ í•„ìš”í•˜ë¯€ë¡œ ìˆœì°¨ ì‹¤í–‰
    docs = await vector_search(embedding)
    
    return {"response": "..."}
```

**ì„±ëŠ¥ ê°œì„  ì¸¡ì •:**

```python
# Before: ìˆœì°¨ ì‹¤í–‰
# redis_get: 50ms + embedding: 150ms = 200ms

# After: ë³‘ë ¬ ì‹¤í–‰
# max(redis_get: 50ms, embedding: 150ms) = 150ms
# ê°œì„ : 25% ë‹¨ì¶•
```

### 3.2 ì»¤ë„¥ì…˜ í’€ ìµœì í™”

**ë¬¸ì œ: ë§¤ ìš”ì²­ë§ˆë‹¤ ì»¤ë„¥ì…˜ ìƒì„±**

```python
# âŒ BAD: ì»¤ë„¥ì…˜ ì¬ì‚¬ìš© ì—†ìŒ
@app.post("/chat")
async def chat(request: ChatRequest):
    # ë§¤ë²ˆ ìƒˆë¡œìš´ Redis ì»¤ë„¥ì…˜ ìƒì„±
    redis_client = await redis.from_url("redis://localhost")
    data = await redis_client.get("key")
    await redis_client.close()  # ì»¤ë„¥ì…˜ ë‹«ê¸°
```

**âœ… ìµœì í™”: ì»¤ë„¥ì…˜ í’€ ì‚¬ìš©**

```python
# GOOD: ì „ì—­ ì»¤ë„¥ì…˜ í’€
from contextlib import asynccontextmanager

# ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì‹œì‘ ì‹œ ì»¤ë„¥ì…˜ í’€ ì´ˆê¸°í™”
    app.state.redis = redis.Redis(
        host='localhost',
        port=6379,
        max_connections=100,  # ìµœëŒ€ ì»¤ë„¥ì…˜ ìˆ˜
        socket_keepalive=True,
        socket_connect_timeout=5,
        retry_on_timeout=True
    )
    
    # Vector DB ì»¤ë„¥ì…˜ë„ ì´ˆê¸°í™”
    app.state.vector_db = await init_vector_db()
    
    yield
    
    # ì¢…ë£Œ ì‹œ ì •ë¦¬
    await app.state.redis.close()
    await app.state.vector_db.close()

app = FastAPI(lifespan=lifespan)

@app.post("/chat")
async def chat(request: Request, chat_request: ChatRequest):
    # ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì»¤ë„¥ì…˜ ì‚¬ìš©
    redis_client = request.app.state.redis
    data = await redis_client.get("key")
```

**ì™¸ë¶€ API ì»¤ë„¥ì…˜ í’€ (httpx):**

```python
# httpxë¡œ LLM API í˜¸ì¶œ ì‹œ ì»¤ë„¥ì…˜ í’€
import httpx

@asynccontextmanager
async def lifespan(app: FastAPI):
    # HTTP í´ë¼ì´ì–¸íŠ¸ í’€
    app.state.http_client = httpx.AsyncClient(
        limits=httpx.Limits(
            max_connections=100,
            max_keepalive_connections=20
        ),
        timeout=httpx.Timeout(30.0)
    )
    
    yield
    
    await app.state.http_client.aclose()

# LLM API í˜¸ì¶œ
async def call_llm_api(prompt: str, http_client: httpx.AsyncClient):
    response = await http_client.post(
        "https://api.openai.com/v1/chat/completions",
        json={"model": "gpt-4", "messages": [{"role": "user", "content": prompt}]},
        headers={"Authorization": f"Bearer {OPENAI_API_KEY}"}
    )
    return response.json()
```

### 3.3 ì‘ë‹µ ìºì‹± ì „ëµ

**API ì‘ë‹µ ìºì‹± (Redis + ë°ì½”ë ˆì´í„°):**

```python
# utils/cache.py
from functools import wraps
import json
import hashlib
from typing import Callable, Any

def cache_response(
    ttl: int = 300,  # 5ë¶„
    key_prefix: str = "cache"
):
    """API ì‘ë‹µ ìºì‹± ë°ì½”ë ˆì´í„°"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(request: Request, *args, **kwargs):
            redis_client = request.app.state.redis
            
            # ìºì‹œ í‚¤ ìƒì„± (ìš”ì²­ íŒŒë¼ë¯¸í„° ê¸°ë°˜)
            cache_key_data = {
                "path": str(request.url),
                "args": str(args),
                "kwargs": str(kwargs)
            }
            cache_key = f"{key_prefix}:{hashlib.md5(json.dumps(cache_key_data).encode()).hexdigest()}"
            
            # ìºì‹œ í™•ì¸
            cached = await redis_client.get(cache_key)
            if cached:
                return json.loads(cached)
            
            # ìºì‹œ ë¯¸ìŠ¤ - ì‹¤ì œ í•¨ìˆ˜ ì‹¤í–‰
            result = await func(request, *args, **kwargs)
            
            # ê²°ê³¼ ìºì‹±
            await redis_client.setex(
                cache_key,
                ttl,
                json.dumps(result)
            )
            
            return result
        
        return wrapper
    return decorator


# ì‚¬ìš© ì˜ˆì‹œ
@app.get("/popular-queries")
@cache_response(ttl=3600, key_prefix="popular")  # 1ì‹œê°„ ìºì‹±
async def get_popular_queries(request: Request):
    # ë¬´ê±°ìš´ ì¿¼ë¦¬
    results = await db.execute("SELECT ... FROM ... GROUP BY ...")
    return {"queries": results}
```

**ì¡°ê±´ë¶€ ìºì‹± (ì‚¬ìš©ìë³„):**

```python
from fastapi import Depends

def get_cache_key(user_id: str, query: str) -> str:
    """ì‚¬ìš©ìë³„ ìºì‹œ í‚¤ ìƒì„±"""
    return f"user:{user_id}:query:{hashlib.md5(query.encode()).hexdigest()}"

@app.post("/chat")
async def chat(
    request: Request,
    chat_request: ChatRequest,
    user_id: str = Depends(get_current_user)
):
    redis_client = request.app.state.redis
    cache_key = get_cache_key(user_id, chat_request.query)
    
    # ë™ì¼í•œ ì§ˆë¬¸ì— ëŒ€í•œ ìºì‹œ í™•ì¸
    cached_response = await redis_client.get(cache_key)
    if cached_response and not chat_request.force_new:
        return json.loads(cached_response)
    
    # ìƒˆë¡œìš´ ì‘ë‹µ ìƒì„±
    response = await generate_response(chat_request)
    
    # ìºì‹± (5ë¶„)
    await redis_client.setex(cache_key, 300, json.dumps(response))
    
    return response
```

### 3.4 ë¯¸ë“¤ì›¨ì–´ ìµœì í™”

**âŒ ë¹„íš¨ìœ¨ì ì¸ ë¯¸ë“¤ì›¨ì–´:**

```python
# BAD: ë™ê¸° ë¯¸ë“¤ì›¨ì–´ê°€ ëª¨ë“  ìš”ì²­ì„ ë¸”ë¡œí‚¹
class SlowMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        # âš ï¸ ë™ê¸° ë¡œê¹… - íŒŒì¼ I/O ë¸”ë¡œí‚¹
        with open("access.log", "a") as f:
            f.write(f"{request.url}\n")
        
        response = await call_next(request)
        
        # âš ï¸ ë™ê¸° DB ì¿¼ë¦¬
        db.execute("INSERT INTO access_log ...")
        
        return response
```

**âœ… ìµœì í™”ëœ ë¯¸ë“¤ì›¨ì–´:**

```python
# GOOD: ë¹„ë™ê¸° + ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…
from fastapi import BackgroundTasks
import aiofiles

class OptimizedMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        # ë¹ ë¥¸ ì‘ë‹µ ë¨¼ì €
        response = await call_next(request)
        
        # ë¡œê¹…ì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ
        asyncio.create_task(self.log_async(request))
        
        return response
    
    async def log_async(self, request: Request):
        """ë¹„ë™ê¸° ë¡œê¹…"""
        async with aiofiles.open("access.log", "a") as f:
            await f.write(f"{request.url}\n")
```

**ì¡°ê±´ë¶€ ë¯¸ë“¤ì›¨ì–´ ì ìš©:**

```python
# íŠ¹ì • ê²½ë¡œì—ë§Œ ë¯¸ë“¤ì›¨ì–´ ì ìš©
from starlette.middleware.base import BaseHTTPMiddleware

class ConditionalMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        # /admin ê²½ë¡œì—ë§Œ ì¸ì¦ ì²´í¬
        if request.url.path.startswith("/admin"):
            # ì¸ì¦ ë¡œì§
            pass
        
        return await call_next(request)
```

### 3.5 ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… í™œìš©

**ë¬´ê±°ìš´ ì‘ì—…ì„ ë°±ê·¸ë¼ìš´ë“œë¡œ:**

```python
from fastapi import BackgroundTasks

async def send_analytics(user_id: str, action: str):
    """ë¶„ì„ ë°ì´í„° ì „ì†¡ (ë¬´ê±°ìš´ ì‘ì—…)"""
    await asyncio.sleep(2)  # ì™¸ë¶€ API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
    print(f"Analytics sent for {user_id}")

@app.post("/chat")
async def chat(
    request: ChatRequest,
    background_tasks: BackgroundTasks
):
    # ì¦‰ì‹œ ì‘ë‹µ ìƒì„±
    response = await generate_response(request.query)
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¶„ì„ ë°ì´í„° ì „ì†¡
    background_tasks.add_task(send_analytics, request.user_id, "chat")
    
    # ë¡œê¹…ë„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ
    background_tasks.add_task(log_conversation, request, response)
    
    return response
```

### 3.6 Request Body í¬ê¸° ì œí•œ

**ëŒ€ìš©ëŸ‰ ìš”ì²­ìœ¼ë¡œ ì¸í•œ ë©”ëª¨ë¦¬ í­ë°œ ë°©ì§€:**

```python
from fastapi import Request, HTTPException
from starlette.middleware.base import BaseHTTPMiddleware

class RequestSizeLimitMiddleware(BaseHTTPMiddleware):
    def __init__(self, app, max_size: int = 1024 * 1024):  # 1MB
        super().__init__(app)
        self.max_size = max_size
    
    async def dispatch(self, request: Request, call_next):
        # Content-Length í™•ì¸
        content_length = request.headers.get("content-length")
        if content_length and int(content_length) > self.max_size:
            raise HTTPException(
                status_code=413,
                detail=f"Request body too large. Max size: {self.max_size} bytes"
            )
        
        return await call_next(request)

app.add_middleware(RequestSizeLimitMiddleware, max_size=5 * 1024 * 1024)  # 5MB
```

### 3.7 Response ì••ì¶•

**Gzip ì••ì¶•ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ ì „ì†¡ ìµœì í™”:**

```python
from fastapi.middleware.gzip import GZipMiddleware

# Gzip ì••ì¶• í™œì„±í™”
app.add_middleware(GZipMiddleware, minimum_size=1000)  # 1KB ì´ìƒë§Œ ì••ì¶•

# ì••ì¶• íš¨ê³¼ ì¸¡ì •
"""
Before:
  Response size: 250 KB
  Transfer time: 500ms (on 4Mbps)

After (gzip):
  Compressed size: 45 KB (82% reduction)
  Transfer time: 90ms (on 4Mbps)
  
ì´ ê°œì„ : 410ms (82% ë‹¨ì¶•)
"""
```

### 3.8 Worker ì„¤ì • ìµœì í™”

**Uvicorn ì„¤ì •:**

```python
# main.py
if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        workers=4,  # CPU ì½”ì–´ ìˆ˜ì— ë§ì¶° ì¡°ì •
        loop="uvloop",  # ë” ë¹ ë¥¸ ì´ë²¤íŠ¸ ë£¨í”„
        http="httptools",  # ë” ë¹ ë¥¸ HTTP íŒŒì„œ
        log_level="warning",  # í”„ë¡œë•ì…˜ì—ì„œëŠ” warning ì´ìƒë§Œ
        access_log=False,  # ì•¡ì„¸ìŠ¤ ë¡œê·¸ ë¹„í™œì„±í™” (ì„±ëŠ¥ í–¥ìƒ)
        limit_concurrency=1000,  # ìµœëŒ€ ë™ì‹œ ì—°ê²° ìˆ˜
        limit_max_requests=10000,  # Worker ì¬ì‹œì‘ ì „ ì²˜ë¦¬í•  ìµœëŒ€ ìš”ì²­ ìˆ˜
        timeout_keep_alive=5  # Keep-alive íƒ€ì„ì•„ì›ƒ
    )
```

**Gunicorn + Uvicorn Worker:**

```bash
# í”„ë¡œë•ì…˜ ë°°í¬ ì‹œ
gunicorn main:app \
    --workers 4 \
    --worker-class uvicorn.workers.UvicornWorker \
    --bind 0.0.0.0:8000 \
    --timeout 120 \
    --graceful-timeout 30 \
    --keep-alive 5 \
    --max-requests 10000 \
    --max-requests-jitter 1000
```

### 3.9 ì„±ëŠ¥ ê°œì„  ìš”ì•½

**FastAPI ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸:**

```python
# âœ… ì ìš©ëœ ìµœì í™”
optimizations = {
    "async_io": {
        "status": "ì™„ë£Œ",
        "improvement": "25-30%",
        "notes": "ëª¨ë“  I/Oë¥¼ ë¹„ë™ê¸°ë¡œ ì „í™˜"
    },
    "connection_pooling": {
        "status": "ì™„ë£Œ",
        "improvement": "15-20%",
        "notes": "Redis, HTTP, DB ì»¤ë„¥ì…˜ í’€"
    },
    "response_caching": {
        "status": "ì™„ë£Œ",
        "improvement": "40-60% (ìºì‹œ íˆíŠ¸ ì‹œ)",
        "notes": "ìì£¼ ìš”ì²­ë˜ëŠ” ë°ì´í„° ìºì‹±"
    },
    "middleware_optimization": {
        "status": "ì™„ë£Œ",
        "improvement": "10-15%",
        "notes": "ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… í™œìš©"
    },
    "compression": {
        "status": "ì™„ë£Œ",
        "improvement": "80% (ì „ì†¡ ì‹œê°„)",
        "notes": "Gzip ì••ì¶•"
    }
}

# ì˜ˆìƒ ì´ ê°œì„ : 50-70% (ìºì‹œ ë¯¸ìŠ¤ ì‹œ), 80-90% (ìºì‹œ íˆíŠ¸ ì‹œ)
```

ë‹¤ìŒ ì„¹ì…˜ì—ì„œëŠ” LangGraph ì›Œí¬í”Œë¡œìš° ìµœì í™”ë¥¼ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.

---

## 4. LangGraph ì›Œí¬í”Œë¡œìš° ìµœì í™”

### 4.1 LangGraph êµ¬ì¡° ì´í•´

LangGraphëŠ” LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ ìƒíƒœ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

**ê¸°ë³¸ LangGraph êµ¬ì¡°:**

```python
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from typing import TypedDict, Annotated
import operator

# ìƒíƒœ ì •ì˜
class AgentState(TypedDict):
    messages: Annotated[list, operator.add]
    context: str
    retrieved_docs: list
    final_response: str

# ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
async def retrieve_node(state: AgentState):
    """ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ"""
    query = state["messages"][-1]
    docs = await vector_db.search(query, k=3)
    return {"retrieved_docs": docs}

async def generate_node(state: AgentState):
    """LLM ìƒì„± ë…¸ë“œ"""
    context = "\n".join(state["retrieved_docs"])
    response = await llm.ainvoke(state["messages"], context=context)
    return {"final_response": response}

# ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(AgentState)
workflow.add_node("retrieve", retrieve_node)
workflow.add_node("generate", generate_node)
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", END)

# Checkpoint ì„¤ì •
checkpointer = MemorySaver()
app = workflow.compile(checkpointer=checkpointer)
```

### 4.2 ë…¸ë“œ ë³‘ë ¬ ì‹¤í–‰

**âŒ ìˆœì°¨ ì‹¤í–‰ (ëŠë¦¼):**

```python
# BAD: ë…ë¦½ì ì¸ ì‘ì—…ì„ ìˆœì°¨ ì‹¤í–‰
async def process_node(state: AgentState):
    # 300ms
    user_context = await load_user_context(state["user_id"])
    
    # 400ms
    relevant_docs = await vector_search(state["query"])
    
    # 200ms
    recent_history = await load_conversation_history(state["thread_id"])
    
    # ì´ ì‹œê°„: 900ms
    return {
        "context": user_context,
        "docs": relevant_docs,
        "history": recent_history
    }
```

**âœ… ë³‘ë ¬ ì‹¤í–‰ (ë¹ ë¦„):**

```python
# GOOD: ë…ë¦½ì ì¸ ì‘ì—…ì„ ë³‘ë ¬ ì‹¤í–‰
import asyncio

async def process_node_optimized(state: AgentState):
    # ë³‘ë ¬ ì‹¤í–‰
    user_context, relevant_docs, recent_history = await asyncio.gather(
        load_user_context(state["user_id"]),
        vector_search(state["query"]),
        load_conversation_history(state["thread_id"])
    )
    
    # ì´ ì‹œê°„: max(300ms, 400ms, 200ms) = 400ms
    # ê°œì„ : 55% ë‹¨ì¶• (900ms â†’ 400ms)
    
    return {
        "context": user_context,
        "docs": relevant_docs,
        "history": recent_history
    }
```

### 4.3 ì¡°ê±´ë¶€ ì‹¤í–‰ (ë¶ˆí•„ìš”í•œ ë…¸ë“œ ìŠ¤í‚µ)

**ìŠ¤ë§ˆíŠ¸ ë¼ìš°íŒ…ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ë…¸ë“œ ì œê±°:**

```python
from langgraph.graph import StateGraph, END

# ì¡°ê±´ í•¨ìˆ˜
def should_retrieve(state: AgentState) -> str:
    """RAGê°€ í•„ìš”í•œì§€ íŒë‹¨"""
    query = state["messages"][-1].content
    
    # ê°„ë‹¨í•œ ì§ˆë¬¸ì€ RAG ìŠ¤í‚µ
    simple_patterns = ["ì•ˆë…•", "ê°ì‚¬", "hi", "hello", "thanks"]
    if any(pattern in query.lower() for pattern in simple_patterns):
        return "skip_retrieval"
    
    # ìºì‹œ í™•ì¸
    cached = check_cache(query)
    if cached:
        return "use_cache"
    
    return "do_retrieval"

# ê·¸ë˜í”„ì— ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€
workflow = StateGraph(AgentState)
workflow.add_node("classify", classify_query)
workflow.add_node("retrieve", retrieve_docs)
workflow.add_node("use_cache", load_from_cache)
workflow.add_node("generate", generate_response)

workflow.set_entry_point("classify")

# ì¡°ê±´ë¶€ ë¼ìš°íŒ…
workflow.add_conditional_edges(
    "classify",
    should_retrieve,
    {
        "skip_retrieval": "generate",  # RAG ì—†ì´ ë°”ë¡œ ìƒì„±
        "use_cache": "use_cache",      # ìºì‹œ ì‚¬ìš©
        "do_retrieval": "retrieve"     # ì •ìƒ RAG í”Œë¡œìš°
    }
)

workflow.add_edge("retrieve", "generate")
workflow.add_edge("use_cache", "generate")
workflow.add_edge("generate", END)
```

**ì„±ëŠ¥ ê°œì„ :**

```python
# Case 1: ê°„ë‹¨í•œ ì¸ì‚¬ (RAG ìŠ¤í‚µ)
# Before: classify(50ms) + retrieve(400ms) + generate(800ms) = 1250ms
# After:  classify(50ms) + generate(800ms) = 850ms
# ê°œì„ : 32% ë‹¨ì¶•

# Case 2: ìºì‹œëœ ì¿¼ë¦¬
# Before: 1250ms
# After:  classify(50ms) + cache(20ms) + generate(200ms) = 270ms
# ê°œì„ : 78% ë‹¨ì¶•
```

### 4.4 Checkpoint ìµœì í™”

**Redis ê¸°ë°˜ Checkpoint ìµœì í™”:**

```python
from langgraph.checkpoint.redis import RedisSaver
import redis.asyncio as redis
import pickle
import gzip

class OptimizedRedisSaver(RedisSaver):
    """ìµœì í™”ëœ Redis Checkpoint Saver"""
    
    def __init__(self, redis_client: redis.Redis):
        super().__init__(redis_client)
        self.compression_threshold = 1024  # 1KB ì´ìƒë§Œ ì••ì¶•
    
    async def aput(self, config, checkpoint, metadata):
        """Checkpoint ì €ì¥ (ì••ì¶• + ë¹„ë™ê¸°)"""
        thread_id = config["configurable"]["thread_id"]
        key = f"checkpoint:{thread_id}"
        
        # ì§ë ¬í™”
        data = pickle.dumps(checkpoint)
        
        # ì••ì¶• (1KB ì´ìƒì¼ ë•Œë§Œ)
        if len(data) > self.compression_threshold:
            data = gzip.compress(data, compresslevel=6)
            key += ":compressed"
        
        # ë¹„ë™ê¸° ì €ì¥ (TTL ì„¤ì •)
        await self.redis_client.setex(key, 3600, data)  # 1ì‹œê°„
        
        # ë©”íƒ€ë°ì´í„°ë„ ì €ì¥
        await self.redis_client.hset(
            f"checkpoint_meta:{thread_id}",
            mapping={
                "created_at": metadata.get("created_at"),
                "size": len(data),
                "compressed": len(data) > self.compression_threshold
            }
        )
    
    async def aget(self, config):
        """Checkpoint ë¡œë“œ (ì••ì¶• í•´ì œ + ë¹„ë™ê¸°)"""
        thread_id = config["configurable"]["thread_id"]
        
        # ì••ì¶•ëœ ë°ì´í„° í™•ì¸
        compressed_key = f"checkpoint:{thread_id}:compressed"
        normal_key = f"checkpoint:{thread_id}"
        
        data = await self.redis_client.get(compressed_key)
        if data:
            # ì••ì¶• í•´ì œ
            data = gzip.decompress(data)
        else:
            data = await self.redis_client.get(normal_key)
        
        if not data:
            return None
        
        # ì—­ì§ë ¬í™”
        checkpoint = pickle.loads(data)
        return checkpoint

# ì‚¬ìš©
redis_client = redis.Redis(host='localhost', port=6379)
checkpointer = OptimizedRedisSaver(redis_client)
app = workflow.compile(checkpointer=checkpointer)
```

**ë©”ëª¨ë¦¬ ì ˆê° íš¨ê³¼:**

```python
# Before: ë¹„ì••ì¶•
checkpoint_size = 4.5 MB

# After: gzip ì••ì¶•
compressed_size = 850 KB  # 81% ê°ì†Œ

# Redis ë©”ëª¨ë¦¬ ì ˆì•½: 100ê°œ ì„¸ì…˜ ê¸°ì¤€
# Before: 4.5 MB Ã— 100 = 450 MB
# After:  850 KB Ã— 100 = 85 MB
# ì ˆì•½: 365 MB (81%)
```

### 4.5 ìƒíƒœ í¬ê¸° ìµœì†Œí™”

**âŒ ë¶ˆí•„ìš”í•œ ë°ì´í„°ë¥¼ ìƒíƒœì— ì €ì¥:**

```python
# BAD: ëª¨ë“  ê²ƒì„ ìƒíƒœì— ì €ì¥
class AgentState(TypedDict):
    messages: list  # ì „ì²´ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ (ìˆ˜ë°± ê°œ)
    all_retrieved_docs: list  # ëª¨ë“  ê²€ìƒ‰ ë¬¸ì„œ (ìˆ˜ MB)
    raw_embeddings: list  # ì›ë³¸ ì„ë² ë”© (ë¶ˆí•„ìš”)
    intermediate_results: list  # ì¤‘ê°„ ê²°ê³¼ ì „ë¶€
    debug_info: dict  # ë””ë²„ê·¸ ì •ë³´ (í”„ë¡œë•ì…˜ì— ë¶ˆí•„ìš”)
```

**âœ… í•„ìˆ˜ ë°ì´í„°ë§Œ ìƒíƒœì— ì €ì¥:**

```python
# GOOD: í•„ìš”í•œ ìµœì†Œí•œì˜ ë°ì´í„°ë§Œ
from typing import TypedDict, Annotated
import operator

class OptimizedAgentState(TypedDict):
    # ìµœê·¼ Nê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€
    messages: Annotated[list, operator.add]
    
    # ë¬¸ì„œ IDë§Œ ì €ì¥ (ì „ì²´ ë‚´ìš© ì œì™¸)
    doc_ids: list[str]
    
    # ìµœì¢… ì‘ë‹µë§Œ
    response: str
    
    # ë©”íƒ€ë°ì´í„°
    user_id: str
    thread_id: str

# ë©”ì‹œì§€ ì œí•œ
def limit_messages(messages: list, max_messages: int = 10):
    """ìµœê·¼ Nê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€"""
    return messages[-max_messages:]

async def generate_node(state: OptimizedAgentState):
    # ë©”ì‹œì§€ ê°œìˆ˜ ì œí•œ
    limited_messages = limit_messages(state["messages"])
    
    # ë¬¸ì„œëŠ” IDë¡œë§Œ ì°¸ì¡° (í•„ìš” ì‹œ Redisì—ì„œ ë¡œë“œ)
    docs = await load_docs_by_ids(state["doc_ids"])
    
    response = await llm.ainvoke(limited_messages, context=docs)
    return {"response": response}
```

**ë©”ëª¨ë¦¬ ì ˆê°:**

```python
# Before: ì „ì²´ ë°ì´í„° ì €ì¥
state_size = {
    "messages": 2.5 MB,      # ì „ì²´ íˆìŠ¤í† ë¦¬
    "docs": 3.8 MB,          # ì „ì²´ ë¬¸ì„œ ë‚´ìš©
    "embeddings": 1.2 MB,    # ì„ë² ë”©
    "debug": 0.5 MB          # ë””ë²„ê·¸ ì •ë³´
}
total = 8.0 MB

# After: í•„ìˆ˜ ë°ì´í„°ë§Œ
optimized_size = {
    "messages": 150 KB,      # ìµœê·¼ 10ê°œë§Œ
    "doc_ids": 5 KB,         # IDë§Œ
    "response": 50 KB        # ìµœì¢… ì‘ë‹µë§Œ
}
optimized_total = 205 KB

# ê°œì„ : 97.4% ê°ì†Œ (8.0 MB â†’ 205 KB)
```

### 4.6 LLM í˜¸ì¶œ ìµœì í™”

**ìŠ¤íŠ¸ë¦¬ë° + ì¡°ê¸° ì¢…ë£Œ:**

```python
from openai import AsyncOpenAI

client = AsyncOpenAI()

async def generate_with_streaming(messages: list, max_tokens: int = 500):
    """ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µ ìƒì„± + ì¡°ê¸° ì¢…ë£Œ"""
    collected_messages = []
    
    stream = await client.chat.completions.create(
        model="gpt-4",
        messages=messages,
        max_tokens=max_tokens,
        stream=True,  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
        temperature=0.7
    )
    
    async for chunk in stream:
        if chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            collected_messages.append(content)
            
            # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ (ì˜ˆ: ì¶©ë¶„í•œ ë‹µë³€)
            full_text = "".join(collected_messages)
            if len(full_text) > 200 and full_text.endswith((".", "!", "?")):
                # ë” ì´ìƒ ìƒì„±í•˜ì§€ ì•Šê³  ì¢…ë£Œ
                break
            
            # ì‹¤ì‹œê°„ìœ¼ë¡œ ì²­í¬ ë°˜í™˜ (í´ë¼ì´ì–¸íŠ¸ë¡œ)
            yield content
    
    return "".join(collected_messages)
```

**ë°°ì¹˜ ì²˜ë¦¬ (ì—¬ëŸ¬ ìš”ì²­ ë™ì‹œ ì²˜ë¦¬):**

```python
async def process_batch(queries: list[str]) -> list[str]:
    """ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬"""
    # ë³‘ë ¬ë¡œ LLM í˜¸ì¶œ
    tasks = [
        client.chat.completions.create(
            model="gpt-3.5-turbo",  # ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
            messages=[{"role": "user", "content": q}],
            max_tokens=100
        )
        for q in queries
    ]
    
    responses = await asyncio.gather(*tasks)
    return [r.choices[0].message.content for r in responses]

# ì‚¬ìš© ì˜ˆì‹œ
queries = ["ì§ˆë¬¸ 1", "ì§ˆë¬¸ 2", "ì§ˆë¬¸ 3"]
results = await process_batch(queries)  # ë³‘ë ¬ ì²˜ë¦¬

# Before: ìˆœì°¨ ì²˜ë¦¬
# Time: 800ms Ã— 3 = 2400ms

# After: ë³‘ë ¬ ì²˜ë¦¬
# Time: max(800ms) = 800ms
# ê°œì„ : 66% ë‹¨ì¶•
```

### 4.7 ê·¸ë˜í”„ êµ¬ì¡° ìµœì í™”

**ë³µì¡í•œ ê·¸ë˜í”„ vs ë‹¨ìˆœí•œ ê·¸ë˜í”„:**

```python
# âŒ ë„ˆë¬´ ë³µì¡í•œ ê·¸ë˜í”„ (ë¶ˆí•„ìš”í•œ ë…¸ë“œ)
workflow = StateGraph(AgentState)
workflow.add_node("input_validation", validate_input)      # 50ms
workflow.add_node("preprocessing", preprocess)             # 30ms
workflow.add_node("intent_classification", classify)       # 100ms
workflow.add_node("entity_extraction", extract)            # 80ms
workflow.add_node("context_loading", load_context)         # 200ms
workflow.add_node("retrieval", retrieve)                   # 400ms
workflow.add_node("reranking", rerank)                     # 150ms
workflow.add_node("synthesis", synthesize)                 # 100ms
workflow.add_node("generation", generate)                  # 800ms
workflow.add_node("postprocessing", postprocess)           # 50ms
workflow.add_node("validation", validate_output)           # 40ms

# ì´ ì‹œê°„: 2000ms+

# âœ… ë‹¨ìˆœí•˜ê³  íš¨ìœ¨ì ì¸ ê·¸ë˜í”„
workflow = StateGraph(AgentState)
workflow.add_node("prepare", prepare_context)   # í†µí•©: 280ms
workflow.add_node("retrieve", smart_retrieve)   # í†µí•©: 450ms
workflow.add_node("generate", generate)         # 800ms

# ì´ ì‹œê°„: 1530ms
# ê°œì„ : 23% ë‹¨ì¶•
```

### 4.8 ì‹¤ì „ ì˜ˆì œ: ìµœì í™”ëœ LangGraph ì•±

```python
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.redis import RedisSaver
import redis.asyncio as redis
from typing import TypedDict, Annotated, Literal
import operator
import asyncio

# ìµœì í™”ëœ ìƒíƒœ ì •ì˜
class OptimizedState(TypedDict):
    messages: Annotated[list, operator.add]
    query: str
    doc_ids: list[str]
    response: str
    cache_key: str

# ë…¸ë“œ 1: ë³‘ë ¬ ì¤€ë¹„
async def prepare_node(state: OptimizedState):
    """ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (ë³‘ë ¬ ì‹¤í–‰)"""
    query = state["messages"][-1].content
    
    # ë³‘ë ¬ ì‹¤í–‰: ìºì‹œ í™•ì¸ + ì„ë² ë”© ìƒì„±
    cache_result, embedding = await asyncio.gather(
        check_query_cache(query),
        generate_embedding_cached(query)  # ì„ë² ë”©ë„ ìºì‹±
    )
    
    if cache_result:
        return {
            "response": cache_result["response"],
            "cache_key": cache_result["key"]
        }
    
    return {"query": query, "embedding": embedding}

# ë…¸ë“œ 2: ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰
async def retrieve_node(state: OptimizedState):
    """ì¡°ê±´ë¶€ ê²€ìƒ‰ (í•„ìš”ì‹œì—ë§Œ)"""
    if state.get("response"):  # ìºì‹œ íˆíŠ¸ ì‹œ ìŠ¤í‚µ
        return {}
    
    # íš¨ìœ¨ì ì¸ ê²€ìƒ‰ (k=3ë§Œ)
    doc_ids = await vector_db.search_ids(
        state["embedding"],
        k=3,
        use_index=True  # HNSW ì¸ë±ìŠ¤ ì‚¬ìš©
    )
    
    return {"doc_ids": doc_ids}

# ë…¸ë“œ 3: ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
async def generate_node(state: OptimizedState):
    """ìŠ¤íŠ¸ë¦¬ë° ìƒì„±"""
    if state.get("response"):  # ìºì‹œ íˆíŠ¸ ì‹œ ìŠ¤í‚µ
        return {}
    
    # ë¬¸ì„œ ë¡œë“œ (IDë§Œ ì „ë‹¬ë°›ì•˜ìœ¼ë¯€ë¡œ)
    docs = await load_docs_by_ids(state["doc_ids"])
    
    # ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
    response = await generate_with_streaming(
        state["messages"],
        context=docs,
        max_tokens=500
    )
    
    # ìºì‹œ ì €ì¥ (ë°±ê·¸ë¼ìš´ë“œ)
    asyncio.create_task(save_to_cache(state["query"], response))
    
    return {"response": response}

# ì¡°ê±´ í•¨ìˆ˜
def route_after_prepare(state: OptimizedState) -> Literal["generate", "retrieve"]:
    """ìºì‹œ íˆíŠ¸ ì—¬ë¶€ì— ë”°ë¼ ë¼ìš°íŒ…"""
    if state.get("response"):
        return "generate"  # ìºì‹œ íˆíŠ¸: ë°”ë¡œ ì‘ë‹µ
    return "retrieve"  # ìºì‹œ ë¯¸ìŠ¤: ê²€ìƒ‰ í•„ìš”

# ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(OptimizedState)
workflow.add_node("prepare", prepare_node)
workflow.add_node("retrieve", retrieve_node)
workflow.add_node("generate", generate_node)

workflow.set_entry_point("prepare")
workflow.add_conditional_edges(
    "prepare",
    route_after_prepare,
    {"retrieve": "retrieve", "generate": "generate"}
)
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", END)

# ìµœì í™”ëœ Checkpoint
redis_client = redis.Redis(host='localhost', port=6379)
checkpointer = OptimizedRedisSaver(redis_client)

app = workflow.compile(checkpointer=checkpointer)
```

**ì„±ëŠ¥ ë¹„êµ:**

```python
# Before: ë¹„ìµœì í™”
{
    "avg_time": "2.45s",
    "p95_time": "3.2s",
    "cache_miss": "2.8s",
    "memory": "3.8 GB"
}

# After: ìµœì í™”
{
    "avg_time": "0.85s",      # 65% ê°œì„ 
    "p95_time": "1.1s",       # 66% ê°œì„ 
    "cache_hit": "0.25s",     # 91% ê°œì„ 
    "cache_miss": "0.95s",    # 66% ê°œì„ 
    "memory": "1.2 GB"        # 68% ê°ì†Œ
}
```

ë‹¤ìŒ ì„¹ì…˜ì—ì„œëŠ” Redis Checkpoint ìµœì í™”ë¥¼ ì‹¬í™”í•˜ì—¬ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.

---

## 5. Redis Checkpoint ìµœì í™”

### 5.1 íŒŒì´í”„ë¼ì´ë‹ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ ì™•ë³µ íšŸìˆ˜ ì¤„ì´ê¸°

**âŒ ê°œë³„ ëª…ë ¹ (ëŠë¦¼):**

```python
# BAD: 4ë²ˆì˜ ë„¤íŠ¸ì›Œí¬ ì™•ë³µ
async def save_conversation(thread_id: str, messages: list, metadata: dict):
    await redis_client.set(f"messages:{thread_id}", json.dumps(messages))
    await redis_client.set(f"metadata:{thread_id}", json.dumps(metadata))
    await redis_client.incr(f"msg_count:{thread_id}")
    await redis_client.expire(f"messages:{thread_id}", 3600)
    
    # ì´ ë ˆì´í„´ì‹œ: 4 Ã— 10ms = 40ms
```

**âœ… íŒŒì´í”„ë¼ì´ë‹ (ë¹ ë¦„):**

```python
# GOOD: 1ë²ˆì˜ ë„¤íŠ¸ì›Œí¬ ì™•ë³µ
async def save_conversation_optimized(thread_id: str, messages: list, metadata: dict):
    async with redis_client.pipeline(transaction=True) as pipe:
        pipe.set(f"messages:{thread_id}", json.dumps(messages))
        pipe.set(f"metadata:{thread_id}", json.dumps(metadata))
        pipe.incr(f"msg_count:{thread_id}")
        pipe.expire(f"messages:{thread_id}", 3600)
        await pipe.execute()
    
    # ì´ ë ˆì´í„´ì‹œ: 1 Ã— 10ms = 10ms
    # ê°œì„ : 75% ë‹¨ì¶•
```

### 5.2 ë°ì´í„° ì••ì¶•

```python
import gzip
import pickle
from typing import Any

class CompressedRedisClient:
    def __init__(self, redis_client):
        self.client = redis_client
        self.compression_threshold = 1024  # 1KB
    
    async def set_compressed(self, key: str, value: Any, ex: int = None):
        """ì••ì¶•í•˜ì—¬ ì €ì¥"""
        # ì§ë ¬í™”
        data = pickle.dumps(value)
        original_size = len(data)
        
        # ì••ì¶• (1KB ì´ìƒë§Œ)
        if original_size > self.compression_threshold:
            compressed = gzip.compress(data, compresslevel=6)
            compression_ratio = len(compressed) / original_size
            
            # ì••ì¶• íš¨ê³¼ê°€ ìˆì„ ë•Œë§Œ ì••ì¶• ë°ì´í„° ì €ì¥
            if compression_ratio < 0.9:
                await self.client.set(f"{key}:c", compressed, ex=ex)
                return {
                    "compressed": True,
                    "original": original_size,
                    "final": len(compressed),
                    "ratio": compression_ratio
                }
        
        # ì••ì¶•í•˜ì§€ ì•ŠìŒ
        await self.client.set(key, data, ex=ex)
        return {"compressed": False, "size": original_size}
    
    async def get_compressed(self, key: str) -> Any:
        """ì••ì¶• í•´ì œí•˜ì—¬ ë¡œë“œ"""
        # ì••ì¶•ëœ ë°ì´í„° í™•ì¸
        data = await self.client.get(f"{key}:c")
        if data:
            # ì••ì¶• í•´ì œ
            data = gzip.decompress(data)
        else:
            # ì¼ë°˜ ë°ì´í„°
            data = await self.client.get(key)
        
        if not data:
            return None
        
        return pickle.loads(data)

# ì‚¬ìš© ì˜ˆì‹œ
compressed_client = CompressedRedisClient(redis_client)

# ì €ì¥
state = {"messages": [...], "context": "..."}  # 4.5 MB
result = await compressed_client.set_compressed("checkpoint:123", state, ex=3600)
print(result)  # {'compressed': True, 'original': 4718592, 'final': 856000, 'ratio': 0.18}

# ë¡œë“œ
loaded_state = await compressed_client.get_compressed("checkpoint:123")
```

**ì••ì¶• íš¨ê³¼:**

```
Checkpoint ë°ì´í„°: 4.5 MB
  â”œâ”€ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬: 2.8 MB
  â”œâ”€ ì»¨í…ìŠ¤íŠ¸: 1.5 MB
  â””â”€ ë©”íƒ€ë°ì´í„°: 0.2 MB

ì••ì¶• í›„ (gzip level 6):
  ì´ í¬ê¸°: 850 KB (81% ê°ì†Œ)
  ì••ì¶• ì‹œê°„: 15ms
  í•´ì œ ì‹œê°„: 8ms

ë©”ëª¨ë¦¬ ì ˆì•½: 100ê°œ ì„¸ì…˜ ê¸°ì¤€
  Before: 450 MB
  After: 85 MB (81% ì ˆì•½)
```

### 5.3 TTL ì „ëµ ìµœì í™”

```python
class SmartTTLManager:
    """ì‚¬ìš© ë¹ˆë„ì— ë”°ë¥¸ TTL ê´€ë¦¬"""
    
    def __init__(self, redis_client):
        self.client = redis_client
        self.base_ttl = 3600  # 1ì‹œê°„
        self.max_ttl = 86400  # 24ì‹œê°„
    
    async def set_with_smart_ttl(self, key: str, value: Any, access_count: int = 0):
        """ì ‘ê·¼ ë¹ˆë„ì— ë”°ë¼ TTL ì¡°ì •"""
        # ì ‘ê·¼ ë¹ˆë„ê°€ ë†’ìœ¼ë©´ TTL ì—°ì¥
        if access_count > 10:
            ttl = self.max_ttl  # 24ì‹œê°„
        elif access_count > 5:
            ttl = self.base_ttl * 4  # 4ì‹œê°„
        elif access_count > 2:
            ttl = self.base_ttl * 2  # 2ì‹œê°„
        else:
            ttl = self.base_ttl  # 1ì‹œê°„
        
        await self.client.set(key, value, ex=ttl)
        
        # ì ‘ê·¼ íšŸìˆ˜ ê¸°ë¡
        await self.client.incr(f"{key}:access_count")
        await self.client.expire(f"{key}:access_count", ttl)
    
    async def get_and_update_ttl(self, key: str):
        """ë¡œë“œ ì‹œ TTL ê°±ì‹ """
        # ë°ì´í„° ë¡œë“œ
        value = await self.client.get(key)
        
        if value:
            # ì ‘ê·¼ íšŸìˆ˜ ì¦ê°€
            access_count = await self.client.incr(f"{key}:access_count")
            
            # TTL ê°±ì‹  (ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°)
            if access_count > 5:
                await self.client.expire(key, self.max_ttl)
        
        return value
```

### 5.4 ë°°ì¹˜ ë¡œë“œ/ì €ì¥

```python
async def batch_load_checkpoints(thread_ids: list[str]) -> dict:
    """ì—¬ëŸ¬ checkpointë¥¼ í•œ ë²ˆì— ë¡œë“œ"""
    async with redis_client.pipeline(transaction=False) as pipe:
        for thread_id in thread_ids:
            pipe.get(f"checkpoint:{thread_id}")
        
        results = await pipe.execute()
    
    return {
        thread_id: pickle.loads(data) if data else None
        for thread_id, data in zip(thread_ids, results)
    }

# ì‚¬ìš© ì˜ˆì‹œ
thread_ids = ["thread_1", "thread_2", "thread_3"]
checkpoints = await batch_load_checkpoints(thread_ids)

# Before: ìˆœì°¨ ë¡œë“œ
# Time: 3 Ã— 15ms = 45ms

# After: ë°°ì¹˜ ë¡œë“œ
# Time: 1 Ã— 15ms = 15ms
# ê°œì„ : 66% ë‹¨ì¶•
```

---

## 6. Vector DB & RAG ìµœì í™”

### 6.1 ì„ë² ë”© ìºì‹±

```python
class EmbeddingCache:
    """ì„ë² ë”© ìºì‹± ë ˆì´ì–´"""
    
    def __init__(self, redis_client, ttl: int = 86400):
        self.redis = redis_client
        self.ttl = ttl
    
    def _get_cache_key(self, text: str, model: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        import hashlib
        text_hash = hashlib.md5(text.encode()).hexdigest()
        return f"embedding:{model}:{text_hash}"
    
    async def get_embedding(self, text: str, model: str = "text-embedding-ada-002"):
        """ìºì‹œëœ ì„ë² ë”© ì¡°íšŒ ë˜ëŠ” ìƒì„±"""
        cache_key = self._get_cache_key(text, model)
        
        # ìºì‹œ í™•ì¸
        cached = await self.redis.get(cache_key)
        if cached:
            return pickle.loads(cached)
        
        # ìºì‹œ ë¯¸ìŠ¤: ì„ë² ë”© ìƒì„±
        embedding = await openai_client.embeddings.create(
            input=text,
            model=model
        )
        
        vector = embedding.data[0].embedding
        
        # ìºì‹± (24ì‹œê°„)
        await self.redis.setex(cache_key, self.ttl, pickle.dumps(vector))
        
        return vector

# ì‚¬ìš© ì˜ˆì‹œ
embedding_cache = EmbeddingCache(redis_client)

# ì²« ìš”ì²­: OpenAI API í˜¸ì¶œ (150ms)
embedding1 = await embedding_cache.get_embedding("ì‚¬ìš©ì ì§ˆë¬¸")

# ê°™ì€ ì§ˆë¬¸: ìºì‹œì—ì„œ ë¡œë“œ (5ms)
embedding2 = await embedding_cache.get_embedding("ì‚¬ìš©ì ì§ˆë¬¸")

# ê°œì„ : 96% ë‹¨ì¶• (150ms â†’ 5ms)
```

### 6.2 Vector ê²€ìƒ‰ ìµœì í™”

**HNSW ì¸ë±ìŠ¤ í™œìš© (Qdrant ì˜ˆì‹œ):**

```python
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, SearchParams

# Qdrant í´ë¼ì´ì–¸íŠ¸
qdrant = QdrantClient(host="localhost", port=6333)

# ì»¬ë ‰ì…˜ ìƒì„± (HNSW ì¸ë±ìŠ¤)
qdrant.create_collection(
    collection_name="documents",
    vectors_config=VectorParams(
        size=1536,  # OpenAI embedding ì°¨ì›
        distance=Distance.COSINE
    ),
    hnsw_config={
        "m": 16,  # ì—°ê²° ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
        "ef_construct": 100,  # êµ¬ì¶• ì‹œ íƒìƒ‰ ë²”ìœ„
    }
)

async def optimized_vector_search(
    query_embedding: list[float],
    k: int = 3,
    score_threshold: float = 0.7
):
    """ìµœì í™”ëœ Vector ê²€ìƒ‰"""
    results = qdrant.search(
        collection_name="documents",
        query_vector=query_embedding,
        limit=k,
        search_params=SearchParams(
            hnsw_ef=64,  # ê²€ìƒ‰ ì‹œ íƒìƒ‰ ë²”ìœ„ (ë‚®ì„ìˆ˜ë¡ ë¹ ë¦„)
            exact=False  # ê·¼ì‚¬ ê²€ìƒ‰ (ë¹ ë¦„)
        ),
        score_threshold=score_threshold,  # ì„ê³„ê°’ ì´í•˜ ì œì™¸
        with_payload=False,  # ë©”íƒ€ë°ì´í„°ë§Œ (ì „ì²´ ë‚´ìš© ì œì™¸)
        with_vectors=False   # ë²¡í„° ì œì™¸
    )
    
    # í•„ìš”í•œ ë¬¸ì„œë§Œ ë³„ë„ë¡œ ë¡œë“œ
    doc_ids = [hit.id for hit in results]
    documents = await load_docs_by_ids(doc_ids)
    
    return documents

# ì„±ëŠ¥ ë¹„êµ
"""
Before (ì „ì²´ ìŠ¤ìº”):
  ê²€ìƒ‰ ì‹œê°„: 600ms
  ë©”ëª¨ë¦¬: 2.5 GB ë¡œë“œ

After (HNSW + í˜ì´ë¡œë“œ ì œì™¸):
  ê²€ìƒ‰ ì‹œê°„: 85ms (85% ë‹¨ì¶•)
  ë©”ëª¨ë¦¬: 50 MB ë¡œë“œ (98% ê°ì†Œ)
"""
```

### 6.3 ì²­í¬ í¬ê¸° ìµœì í™”

```python
def optimize_chunk_size(text: str, chunk_size: int = 512, overlap: int = 50):
    """ìµœì í™”ëœ ì²­í¬ ë¶„í• """
    # ì‘ì€ ì²­í¬ (512 í† í°): ë¹ ë¥¸ ê²€ìƒ‰, ì •í™•í•œ ë§¤ì¹­
    # í° ì²­í¬ (2048 í† í°): ëŠë¦° ê²€ìƒ‰, ì»¨í…ìŠ¤íŠ¸ í’ë¶€
    
    # í† í°í™” (ëŒ€ëµì )
    tokens = text.split()
    
    chunks = []
    for i in range(0, len(tokens), chunk_size - overlap):
        chunk = " ".join(tokens[i:i + chunk_size])
        chunks.append(chunk)
    
    return chunks

# ì²­í¬ í¬ê¸°ë³„ ì„±ëŠ¥
"""
Chunk Size: 256 tokens
  - ê²€ìƒ‰ ì†ë„: ë§¤ìš° ë¹ ë¦„ (40ms)
  - ì •í™•ë„: ì¤‘ê°„
  - ì»¨í…ìŠ¤íŠ¸: ë¶€ì¡±

Chunk Size: 512 tokens âœ… ê¶Œì¥
  - ê²€ìƒ‰ ì†ë„: ë¹ ë¦„ (85ms)
  - ì •í™•ë„: ë†’ìŒ
  - ì»¨í…ìŠ¤íŠ¸: ì¶©ë¶„

Chunk Size: 2048 tokens
  - ê²€ìƒ‰ ì†ë„: ëŠë¦¼ (250ms)
  - ì •í™•ë„: ë†’ìŒ
  - ì»¨í…ìŠ¤íŠ¸: í’ë¶€ (ê³¼ë„)
"""
```

### 6.4 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰

```python
async def hybrid_search(
    query: str,
    k: int = 5
) -> list[dict]:
    """Vector + Keyword í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
    # ë³‘ë ¬ ì‹¤í–‰: Vector ê²€ìƒ‰ + í‚¤ì›Œë“œ ê²€ìƒ‰
    vector_results, keyword_results = await asyncio.gather(
        vector_search(query, k=10),
        keyword_search(query, k=10)
    )
    
    # ì ìˆ˜ ê¸°ë°˜ ë³‘í•© (Reciprocal Rank Fusion)
    combined = {}
    
    for rank, doc in enumerate(vector_results):
        doc_id = doc["id"]
        combined[doc_id] = combined.get(doc_id, 0) + 1 / (rank + 60)
    
    for rank, doc in enumerate(keyword_results):
        doc_id = doc["id"]
        combined[doc_id] = combined.get(doc_id, 0) + 1 / (rank + 60)
    
    # ìƒìœ„ kê°œ ë°˜í™˜
    sorted_docs = sorted(combined.items(), key=lambda x: x[1], reverse=True)
    top_doc_ids = [doc_id for doc_id, score in sorted_docs[:k]]
    
    return await load_docs_by_ids(top_doc_ids)
```

---

## 7. Streaming ì‘ë‹µ ìµœì í™”

### 7.1 ë°±í”„ë ˆì…” ì²˜ë¦¬

```python
from asyncio import Queue

async def streaming_with_backpressure(
    messages: list,
    max_queue_size: int = 10
):
    """ë°±í”„ë ˆì…”ê°€ ì ìš©ëœ ìŠ¤íŠ¸ë¦¬ë°"""
    queue = Queue(maxsize=max_queue_size)
    
    async def producer():
        """LLM ì‘ë‹µ ìƒì„± (í”„ë¡œë“€ì„œ)"""
        async for chunk in llm_stream(messages):
            # íê°€ ê°€ë“ ì°¨ë©´ ëŒ€ê¸° (ë°±í”„ë ˆì…”)
            await queue.put(chunk)
        
        # ì¢…ë£Œ ì‹ í˜¸
        await queue.put(None)
    
    async def consumer():
        """í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ì†¡ (ì»¨ìŠˆë¨¸)"""
        while True:
            chunk = await queue.get()
            if chunk is None:
                break
            
            yield f"data: {json.dumps(chunk)}\n\n"
    
    # í”„ë¡œë“€ì„œì™€ ì»¨ìŠˆë¨¸ ë™ì‹œ ì‹¤í–‰
    producer_task = asyncio.create_task(producer())
    
    async for data in consumer():
        yield data
    
    await producer_task
```

### 7.2 ë²„í¼ ìµœì í™”

```python
class StreamingBuffer:
    """ìŠ¤íŠ¸ë¦¬ë° ë²„í¼"""
    
    def __init__(self, min_buffer_size: int = 10):
        self.buffer = []
        self.min_buffer_size = min_buffer_size
    
    async def add(self, chunk: str):
        """ì²­í¬ ì¶”ê°€"""
        self.buffer.append(chunk)
        
        # ë²„í¼ê°€ ìµœì†Œ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ í”ŒëŸ¬ì‹œ
        if len(self.buffer) >= self.min_buffer_size:
            return await self.flush()
        
        return None
    
    async def flush(self):
        """ë²„í¼ í”ŒëŸ¬ì‹œ"""
        if not self.buffer:
            return None
        
        combined = "".join(self.buffer)
        self.buffer = []
        return combined

async def buffered_streaming(messages: list):
    """ë²„í¼ë§ëœ ìŠ¤íŠ¸ë¦¬ë°"""
    buffer = StreamingBuffer(min_buffer_size=10)
    
    async for chunk in llm_stream(messages):
        result = await buffer.add(chunk)
        
        if result:
            yield f"data: {result}\n\n"
    
    # ë‚¨ì€ ë²„í¼ í”ŒëŸ¬ì‹œ
    final = await buffer.flush()
    if final:
        yield f"data: {final}\n\n"

# ì„±ëŠ¥ ê°œì„ 
"""
Before (ì²­í¬ë§ˆë‹¤ ì „ì†¡):
  - ì²­í¬ ìˆ˜: 500ê°œ
  - ë„¤íŠ¸ì›Œí¬ ì™•ë³µ: 500íšŒ
  - ì˜¤ë²„í—¤ë“œ: ë†’ìŒ

After (ë²„í¼ë§):
  - ì²­í¬ ìˆ˜: 50ê°œ (10ê°œì”© ë¬¶ìŒ)
  - ë„¤íŠ¸ì›Œí¬ ì™•ë³µ: 50íšŒ
  - ì˜¤ë²„í—¤ë“œ: 90% ê°ì†Œ
"""
```

---

## 8. MSA ë ˆë²¨ ìµœì í™”

### 8.1 ì„œë¹„ìŠ¤ ê°„ í†µì‹  ìµœì í™”

```python
# gRPCë¡œ ì„œë¹„ìŠ¤ ê°„ í†µì‹  (HTTP RESTë³´ë‹¤ ë¹ ë¦„)
import grpc
from concurrent import futures

# Protobuf ì •ì˜ (embedding_service.proto)
"""
syntax = "proto3";

service EmbeddingService {
  rpc GetEmbedding(EmbeddingRequest) returns (EmbeddingResponse);
  rpc BatchGetEmbeddings(BatchEmbeddingRequest) returns (BatchEmbeddingResponse);
}

message EmbeddingRequest {
  string text = 1;
  string model = 2;
}

message EmbeddingResponse {
  repeated float vector = 1;
}
"""

# gRPC ì„œë²„
class EmbeddingServicer:
    async def GetEmbedding(self, request, context):
        embedding = await generate_embedding(request.text, request.model)
        return EmbeddingResponse(vector=embedding)

# gRPC í´ë¼ì´ì–¸íŠ¸
async def call_embedding_service_grpc(text: str):
    async with grpc.aio.insecure_channel('embedding-service:50051') as channel:
        stub = EmbeddingServiceStub(channel)
        response = await stub.GetEmbedding(EmbeddingRequest(text=text))
        return list(response.vector)

# ì„±ëŠ¥ ë¹„êµ
"""
HTTP REST:
  - ë ˆì´í„´ì‹œ: 25ms
  - ì˜¤ë²„í—¤ë“œ: JSON ì§ë ¬í™”

gRPC:
  - ë ˆì´í„´ì‹œ: 8ms (68% ë‹¨ì¶•)
  - ì˜¤ë²„í—¤ë“œ: Protobuf (ë°”ì´ë„ˆë¦¬)
"""
```

### 8.2 ë¡œë“œ ë°¸ëŸ°ì‹±

```python
from random import choice

class LoadBalancer:
    """ê°„ë‹¨í•œ ë¼ìš´ë“œ ë¡œë¹ˆ ë¡œë“œ ë°¸ëŸ°ì„œ"""
    
    def __init__(self, endpoints: list[str]):
        self.endpoints = endpoints
        self.current = 0
    
    def get_next_endpoint(self) -> str:
        """ë‹¤ìŒ ì—”ë“œí¬ì¸íŠ¸ ì„ íƒ"""
        endpoint = self.endpoints[self.current]
        self.current = (self.current + 1) % len(self.endpoints)
        return endpoint

# ì‚¬ìš©
balancer = LoadBalancer([
    "http://llm-service-1:8000",
    "http://llm-service-2:8000",
    "http://llm-service-3:8000"
])

async def call_llm_service(prompt: str):
    endpoint = balancer.get_next_endpoint()
    response = await httpx_client.post(f"{endpoint}/generate", json={"prompt": prompt})
    return response.json()
```

### 8.3 ì„œí‚· ë¸Œë ˆì´ì»¤

```python
from datetime import datetime, timedelta

class CircuitBreaker:
    """ì„œí‚· ë¸Œë ˆì´ì»¤ íŒ¨í„´"""
    
    def __init__(self, failure_threshold: int = 5, timeout: int = 60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failures = 0
        self.last_failure_time = None
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
    
    async def call(self, func, *args, **kwargs):
        """ì„œí‚· ë¸Œë ˆì´ì»¤ë¥¼ í†µí•œ í˜¸ì¶œ"""
        # OPEN ìƒíƒœ: ë¹ ë¥¸ ì‹¤íŒ¨
        if self.state == "OPEN":
            if datetime.now() - self.last_failure_time > timedelta(seconds=self.timeout):
                self.state = "HALF_OPEN"
            else:
                raise Exception("Circuit breaker is OPEN")
        
        try:
            result = await func(*args, **kwargs)
            
            # ì„±ê³µ ì‹œ ë¦¬ì…‹
            if self.state == "HALF_OPEN":
                self.state = "CLOSED"
                self.failures = 0
            
            return result
        
        except Exception as e:
            self.failures += 1
            self.last_failure_time = datetime.now()
            
            # ì„ê³„ê°’ ì´ˆê³¼ ì‹œ OPEN
            if self.failures >= self.failure_threshold:
                self.state = "OPEN"
            
            raise e

# ì‚¬ìš©
circuit_breaker = CircuitBreaker(failure_threshold=3, timeout=30)

async def call_external_api_with_circuit_breaker(data):
    return await circuit_breaker.call(external_api_call, data)
```

---

## 9. ëª¨ë‹ˆí„°ë§ ë° í”„ë¡œíŒŒì¼ë§

### 9.1 Prometheus ë©”íŠ¸ë¦­

```python
from prometheus_client import Counter, Histogram, Gauge
import time

# ë©”íŠ¸ë¦­ ì •ì˜
request_count = Counter('api_requests_total', 'Total API requests', ['endpoint', 'status'])
request_duration = Histogram('api_request_duration_seconds', 'Request duration', ['endpoint'])
active_connections = Gauge('active_connections', 'Active connections')

@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    start_time = time.time()
    active_connections.inc()
    
    try:
        response = await call_next(request)
        status = response.status_code
    except Exception as e:
        status = 500
        raise
    finally:
        duration = time.time() - start_time
        request_duration.labels(endpoint=request.url.path).observe(duration)
        request_count.labels(endpoint=request.url.path, status=status).inc()
        active_connections.dec()
    
    return response
```

### 9.2 ë¶„ì‚° íŠ¸ë ˆì´ì‹± (OpenTelemetry)

```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.jaeger.thrift import JaegerExporter

# Tracer ì„¤ì •
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

jaeger_exporter = JaegerExporter(
    agent_host_name="localhost",
    agent_port=6831,
)

trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(jaeger_exporter)
)

# ì‚¬ìš©
async def process_request(query: str):
    with tracer.start_as_current_span("process_request"):
        with tracer.start_as_current_span("embedding_generation"):
            embedding = await generate_embedding(query)
        
        with tracer.start_as_current_span("vector_search"):
            docs = await vector_search(embedding)
        
        with tracer.start_as_current_span("llm_generation"):
            response = await llm_generate(docs)
        
        return response
```

---

## 10. ê²°ë¡  ë° ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸

### 10.1 í•µì‹¬ ìš”ì•½

**ì ìš©ëœ ìµœì í™” ê¸°ë²•:**

| ë ˆì´ì–´ | ìµœì í™” | ê°œì„  íš¨ê³¼ |
|--------|--------|----------|
| FastAPI | ë¹„ë™ê¸° I/O, ì»¤ë„¥ì…˜ í’€ | 25-30% |
| FastAPI | ì‘ë‹µ ìºì‹±, ì••ì¶• | 40-80% |
| LangGraph | ë…¸ë“œ ë³‘ë ¬ ì‹¤í–‰ | 30-40% |
| LangGraph | ì¡°ê±´ë¶€ ì‹¤í–‰, ìƒíƒœ ìµœì†Œí™” | 20-30% |
| Redis | íŒŒì´í”„ë¼ì´ë‹, ì••ì¶• | 75-80% |
| Vector DB | HNSW ì¸ë±ìŠ¤, ì„ë² ë”© ìºì‹± | 85-96% |
| Streaming | ë°±í”„ë ˆì…”, ë²„í¼ë§ | 90% |
| MSA | gRPC, ë¡œë“œ ë°¸ëŸ°ì‹± | 68% |

**ì¢…í•© ì„±ëŠ¥ ê°œì„ :**

```python
# Before (ìµœì í™” ì „)
{
    "TTFT (First Token)": "2.5s",
    "P95 Latency": "3.2s",
    "Throughput": "200 req/s",
    "Memory": "3.8 GB per instance"
}

# After (ìµœì í™” í›„)
{
    "TTFT (First Token)": "450ms",    # 82% ê°œì„  â¬‡ï¸
    "P95 Latency": "850ms",           # 73% ê°œì„  â¬‡ï¸
    "Throughput": "950 req/s",        # 375% ì¦ê°€ â¬†ï¸
    "Memory": "1.1 GB per instance"   # 71% ê°ì†Œ â¬‡ï¸
}

# ëª©í‘œ ë‹¬ì„±ë„
- âœ… TTFT < 500ms: ë‹¬ì„± (450ms)
- âœ… Throughput > 800 req/s: ë‹¬ì„± (950 req/s)
- âœ… Memory < 2GB: ë‹¬ì„± (1.1 GB)
```

### 10.2 ë‹¨ê³„ë³„ ì ìš© ê°€ì´ë“œ

**Week 1-2: Quick Wins (ì¦‰ì‹œ ì ìš© ê°€ëŠ¥)**

```python
quick_wins_checklist = [
    "âœ… Redis ë™ê¸° â†’ ë¹„ë™ê¸° ì „í™˜",
    "âœ… Vector ê²€ìƒ‰ kê°’ ì¡°ì • (10 â†’ 3)",
    "âœ… ë¶ˆí•„ìš”í•œ ë¡œê¹… ì œê±°",
    "âœ… Gzip ì••ì¶• í™œì„±í™”",
    "âœ… Uvicorn Worker ìˆ˜ ì¦ê°€",
    "âœ… ì„ë² ë”© ìºì‹± (Redis)",
]

# ì˜ˆìƒ ê°œì„ : 30-40%
```

**Week 3-4: ì¤‘ê¸‰ ìµœì í™”**

```python
intermediate_checklist = [
    "âœ… LangGraph ë…¸ë“œ ë³‘ë ¬í™”",
    "âœ… Redis íŒŒì´í”„ë¼ì´ë‹",
    "âœ… Checkpoint ì••ì¶•",
    "âœ… ì¡°ê±´ë¶€ RAG (ê°„ë‹¨í•œ ì§ˆë¬¸ ìŠ¤í‚µ)",
    "âœ… ì»¤ë„¥ì…˜ í’€ ìµœì í™”",
    "âœ… ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… í™œìš©",
]

# ì˜ˆìƒ ì¶”ê°€ ê°œì„ : 30-40%
```

**Week 5-6: ê³ ê¸‰ ìµœì í™”**

```python
advanced_checklist = [
    "âœ… Vector DB HNSW ì¸ë±ìŠ¤ íŠœë‹",
    "âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ êµ¬í˜„",
    "âœ… gRPC ë„ì…",
    "âœ… ì„œí‚· ë¸Œë ˆì´ì»¤ íŒ¨í„´",
    "âœ… ë¶„ì‚° íŠ¸ë ˆì´ì‹±",
    "âœ… ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ",
]

# ì˜ˆìƒ ì¶”ê°€ ê°œì„ : 20-30%
```

### 10.3 ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸

```python
final_checklist = {
    "FastAPI": {
        "ë¹„ë™ê¸° I/O": "âœ…",
        "ì»¤ë„¥ì…˜ í’€": "âœ…",
        "ì‘ë‹µ ìºì‹±": "âœ…",
        "ë¯¸ë“¤ì›¨ì–´ ìµœì í™”": "âœ…",
        "Gzip ì••ì¶•": "âœ…",
        "Worker ì„¤ì •": "âœ…"
    },
    "LangGraph": {
        "ë…¸ë“œ ë³‘ë ¬ ì‹¤í–‰": "âœ…",
        "ì¡°ê±´ë¶€ ë¼ìš°íŒ…": "âœ…",
        "ìƒíƒœ í¬ê¸° ìµœì†Œí™”": "âœ…",
        "Checkpoint ì••ì¶•": "âœ…",
        "ìŠ¤ë§ˆíŠ¸ ìºì‹±": "âœ…"
    },
    "Redis": {
        "ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸": "âœ…",
        "íŒŒì´í”„ë¼ì´ë‹": "âœ…",
        "ë°ì´í„° ì••ì¶•": "âœ…",
        "TTL ìµœì í™”": "âœ…",
        "ë°°ì¹˜ ì‘ì—…": "âœ…"
    },
    "Vector DB": {
        "HNSW ì¸ë±ìŠ¤": "âœ…",
        "ì„ë² ë”© ìºì‹±": "âœ…",
        "ì²­í¬ í¬ê¸° ìµœì í™”": "âœ…",
        "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰": "âœ…"
    },
    "Streaming": {
        "ë°±í”„ë ˆì…”": "âœ…",
        "ë²„í¼ë§": "âœ…",
        "ì¡°ê¸° ì¢…ë£Œ": "âœ…"
    },
    "MSA": {
        "gRPC": "âœ…",
        "ë¡œë“œ ë°¸ëŸ°ì‹±": "âœ…",
        "ì„œí‚· ë¸Œë ˆì´ì»¤": "âœ…"
    },
    "ëª¨ë‹ˆí„°ë§": {
        "Prometheus": "âœ…",
        "ë¶„ì‚° íŠ¸ë ˆì´ì‹±": "âœ…",
        "ëŒ€ì‹œë³´ë“œ": "âœ…"
    }
}
```

### 10.4 ì°¸ê³  ìë£Œ

**ê³µì‹ ë¬¸ì„œ:**

- [FastAPI Performance](https://fastapi.tiangolo.com/deployment/server-workers/)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [Redis Performance Optimization](https://redis.io/docs/management/optimization/)
- [Qdrant HNSW Index](https://qdrant.tech/documentation/indexing/)

**ìœ ìš©í•œ ë„êµ¬:**

```python
monitoring_tools = [
    "Prometheus + Grafana",
    "Jaeger (ë¶„ì‚° íŠ¸ë ˆì´ì‹±)",
    "cProfile (Python í”„ë¡œíŒŒì¼ë§)",
    "memory_profiler",
    "py-spy (í”„ë¡œë•ì…˜ í”„ë¡œíŒŒì¼ë§)"
]
```

**ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤:**

1. **ì¸¡ì • ìš°ì„ **: í•­ìƒ í”„ë¡œíŒŒì¼ë§ë¶€í„° ì‹œì‘
2. **ì ì§„ì  ì ìš©**: Quick Wins â†’ ì¤‘ê¸‰ â†’ ê³ ê¸‰ ìˆœì„œë¡œ
3. **A/B í…ŒìŠ¤íŠ¸**: ìµœì í™” ì „í›„ ì„±ëŠ¥ ë¹„êµ
4. **ëª¨ë‹ˆí„°ë§ í•„ìˆ˜**: ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¶”ì 
5. **ë¬¸ì„œí™”**: ì ìš©í•œ ìµœì í™” ê¸°ë¡ ìœ ì§€

---

## ë§ˆì¹˜ë©°

FastAPI + LangGraph ê¸°ë°˜ MSA LLM Streaming APIì˜ ì„±ëŠ¥ì„ **80% ì´ìƒ ê°œì„ **í•˜ëŠ” ì¢…í•©ì ì¸ ìµœì í™” ë°©ë²•ì„ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤.

**í•µì‹¬ í¬ì¸íŠ¸:**

- ğŸš€ **ë¹„ë™ê¸° I/O**: ëª¨ë“  I/Oë¥¼ ë¹„ë™ê¸°ë¡œ ì „í™˜
- ğŸ’¾ **ìºì‹±**: Redisë¥¼ í™œìš©í•œ ë‹¤ì¸µ ìºì‹± ì „ëµ
- ğŸ“¦ **ì••ì¶•**: Checkpoint ë° ë„¤íŠ¸ì›Œí¬ ì „ì†¡ ì••ì¶•
- ğŸ” **ì¸ë±ì‹±**: Vector DB HNSW ì¸ë±ìŠ¤ í™œìš©
- âš¡ **ë³‘ë ¬ ì‹¤í–‰**: ë…ë¦½ì ì¸ ì‘ì—… ë³‘ë ¬í™”
- ğŸ“Š **ëª¨ë‹ˆí„°ë§**: ì§€ì†ì ì¸ ì„±ëŠ¥ ì¶”ì 

ì´ì œ ì—¬ëŸ¬ë¶„ì˜ LLM API ì„±ëŠ¥ì„ íšê¸°ì ìœ¼ë¡œ í–¥ìƒì‹œì¼œë³´ì„¸ìš”! ğŸ‰

**ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±**ì€ ëŒ“ê¸€ë¡œ ë‚¨ê²¨ì£¼ì„¸ìš”! ğŸ˜Š

