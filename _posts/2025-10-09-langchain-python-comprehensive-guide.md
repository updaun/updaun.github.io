---
layout: post
title: "LangChain Python ì™„ë²½ ê°€ì´ë“œ: ì‹¤ì „ ì˜ˆì œë¡œ ë°°ìš°ëŠ” LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ"
date: 2025-10-09 14:00:00 +0900
categories: [LangChain, Python, LLM, AI]
tags: [LangChain, Python, OpenAI, GPT, LLM, AI, Chain, Agent, Vector Store, RAG]
author: "updaun"
image: "/assets/img/posts/2025-10-09-langchain-python-comprehensive-guide.webp"
---

LangChainì€ LLM(Large Language Model)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ ê°•ë ¥í•œ Python í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” LangChainì˜ í•µì‹¬ ê°œë…ë¶€í„° ì‹¤ì „ í™œìš©ë²•ê¹Œì§€ ë‹¨ê³„ë³„ë¡œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

## ğŸš€ LangChainì´ë€?

LangChainì€ ì–¸ì–´ ëª¨ë¸ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì£¼ìš” íŠ¹ì§•:

- **ëª¨ë“ˆì„±**: ë‹¤ì–‘í•œ ì»´í¬ë„ŒíŠ¸ë¥¼ ì¡°í•©í•˜ì—¬ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° êµ¬ì„±
- **ì²´ì´ë‹**: ì—¬ëŸ¬ ë‹¨ê³„ì˜ ì²˜ë¦¬ë¥¼ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì‘ì—… ìˆ˜í–‰
- **ì—ì´ì „íŠ¸**: ììœ¨ì ìœ¼ë¡œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” AI ì—ì´ì „íŠ¸ êµ¬í˜„
- **ë©”ëª¨ë¦¬**: ëŒ€í™” ê¸°ë¡ê³¼ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬

## ğŸ“¦ ì„¤ì¹˜ ë° ê¸°ë³¸ ì„¤ì •

### 1. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# ê¸°ë³¸ LangChain ì„¤ì¹˜
pip install langchain

# OpenAI í†µí•©
pip install langchain-openai

# ë²¡í„° ìŠ¤í† ì–´ (Chroma)
pip install langchain-chroma

# ì¶”ê°€ ìœ í‹¸ë¦¬í‹°
pip install langchain-community
pip install langchain-experimental
```

### 2. í™˜ê²½ ì„¤ì •

```python
import os
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
```

## ğŸ”§ í•µì‹¬ ì»´í¬ë„ŒíŠ¸

### 1. LLMsì™€ Chat Models

```python
from langchain_openai import OpenAI, ChatOpenAI

# ê¸°ë³¸ LLM
llm = OpenAI(temperature=0.7)
response = llm.invoke("Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€?")
print(response)

# Chat Model (ëŒ€í™”í˜•)
chat = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
messages = [
    ("system", "ë‹¹ì‹ ì€ ì¹œì ˆí•œ Python íŠœí„°ì…ë‹ˆë‹¤."),
    ("human", "Python ë°ì½”ë ˆì´í„°ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.")
]
response = chat.invoke(messages)
print(response.content)
```

### 2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

```python
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate

# ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = PromptTemplate(
    input_variables=["topic", "level"],
    template="""
    {topic}ì— ëŒ€í•´ {level} ìˆ˜ì¤€ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    êµ¬ì²´ì ì¸ ì˜ˆì œì™€ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì‹œë©´ ì¢‹ê² ìŠµë‹ˆë‹¤.
    """
)

# ì‚¬ìš©ë²•
formatted_prompt = prompt.format(topic="ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°", level="ì´ˆê¸‰ì")

# Chat í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ ì „ë¬¸ í”„ë¡œê·¸ë˜ë° ê°•ì‚¬ì…ë‹ˆë‹¤."),
    ("human", "{topic}ì— ëŒ€í•´ {level} ìˆ˜ì¤€ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.")
])

# ì²´ì¸ê³¼ ê²°í•©
chain = chat_prompt | chat
response = chain.invoke({
    "topic": "í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë°", 
    "level": "ì¤‘ê¸‰ì"
})
```

### 3. ì¶œë ¥ íŒŒì„œ

```python
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field

# JSON ì¶œë ¥ íŒŒì„œ
class CodeReview(BaseModel):
    strengths: list = Field(description="ì½”ë“œì˜ ì¥ì ë“¤")
    weaknesses: list = Field(description="ê°œì„ í•  ì ë“¤")
    suggestions: list = Field(description="ê°œì„  ì œì•ˆì‚¬í•­")

parser = JsonOutputParser(pydantic_object=CodeReview)

prompt = PromptTemplate(
    template="""
    ë‹¤ìŒ Python ì½”ë“œë¥¼ ë¦¬ë·°í•´ì£¼ì„¸ìš”:
    {code}
    
    {format_instructions}
    """,
    input_variables=["code"],
    partial_variables={"format_instructions": parser.get_format_instructions()}
)

chain = prompt | llm | parser

code_sample = """
def calculate_average(numbers):
    return sum(numbers) / len(numbers)
"""

result = chain.invoke({"code": code_sample})
print(result)
```

## ğŸ”— ì²´ì¸(Chains) í™œìš©

### 1. ê¸°ë³¸ ì²´ì¸

```python
from langchain_core.runnables import RunnablePassthrough

# ê°„ë‹¨í•œ ì²´ì¸
simple_chain = prompt | llm | StrOutputParser()

# ë³µì¡í•œ ì²´ì¸
def analyze_code(inputs):
    code = inputs["code"]
    # ì½”ë“œ ë¶„ì„ ë¡œì§
    return {"analysis": f"ë¶„ì„ ê²°ê³¼: {code}", "code": code}

complex_chain = (
    RunnablePassthrough.assign(analysis=analyze_code) |
    prompt |
    llm |
    parser
)
```

### 2. ì¡°ê±´ë¶€ ì²´ì¸

```python
from langchain_core.runnables import RunnableBranch

def route_question(inputs):
    question = inputs["question"]
    if "ë””ë²„ê¹…" in question:
        return "debug_chain"
    elif "ìµœì í™”" in question:
        return "optimize_chain"
    else:
        return "general_chain"

debug_prompt = PromptTemplate.from_template(
    "ë””ë²„ê¹… ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ì„¸ìš”: {question}"
)

optimize_prompt = PromptTemplate.from_template(
    "ì„±ëŠ¥ ìµœì í™” ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒì„ ê°œì„ í•´ì£¼ì„¸ìš”: {question}"
)

general_prompt = PromptTemplate.from_template(
    "Python ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”: {question}"
)

branch_chain = RunnableBranch(
    (lambda x: "ë””ë²„ê¹…" in x["question"], debug_prompt | llm),
    (lambda x: "ìµœì í™”" in x["question"], optimize_prompt | llm),
    general_prompt | llm
)
```

## ğŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬

### 1. ëŒ€í™” ê¸°ë¡ ê´€ë¦¬

```python
from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory
from langchain.chains import ConversationChain

# ê¸°ë³¸ ë©”ëª¨ë¦¬
memory = ConversationBufferMemory()

conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

# ëŒ€í™” ì‹œì‘
response1 = conversation.predict(input="ì•ˆë…•í•˜ì„¸ìš”, Python íŠœí„°ë‹˜!")
response2 = conversation.predict(input="ë°©ê¸ˆ ì „ì— ë­ë¼ê³  ì¸ì‚¬í–ˆì£ ?")

# ìš”ì•½ ë©”ëª¨ë¦¬ (ê¸´ ëŒ€í™”ìš©)
summary_memory = ConversationSummaryMemory(llm=llm)
```

### 2. ì»¤ìŠ¤í…€ ë©”ëª¨ë¦¬

```python
from langchain.schema import BaseMemory

class CodeHistoryMemory(BaseMemory):
    """ì½”ë“œ ì‹¤í–‰ ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” ì»¤ìŠ¤í…€ ë©”ëª¨ë¦¬"""
    
    def __init__(self):
        self.code_history = []
        
    @property
    def memory_variables(self):
        return ["code_history"]
    
    def load_memory_variables(self, inputs):
        return {"code_history": "\n".join(self.code_history)}
    
    def save_context(self, inputs, outputs):
        if "code" in inputs:
            self.code_history.append(inputs["code"])
    
    def clear(self):
        self.code_history = []
```

## ğŸ¤– ì—ì´ì „íŠ¸(Agents) êµ¬í˜„

### 1. ê¸°ë³¸ ì—ì´ì „íŠ¸

```python
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain_community.tools import PythonREPLTool
from langchain_core.tools import tool

# ì»¤ìŠ¤í…€ ë„êµ¬ ì •ì˜
@tool
def code_analyzer(code: str) -> str:
    """Python ì½”ë“œë¥¼ ë¶„ì„í•˜ê³  ë³µì¡ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    lines = len(code.split('\n'))
    functions = code.count('def ')
    classes = code.count('class ')
    
    complexity = "ë‚®ìŒ" if lines < 20 else "ë³´í†µ" if lines < 50 else "ë†’ìŒ"
    
    return f"""
    ì½”ë“œ ë¶„ì„ ê²°ê³¼:
    - ë¼ì¸ ìˆ˜: {lines}
    - í•¨ìˆ˜ ìˆ˜: {functions}
    - í´ë˜ìŠ¤ ìˆ˜: {classes}
    - ë³µì¡ë„: {complexity}
    """

# ë„êµ¬ ëª©ë¡
tools = [
    PythonREPLTool(),
    code_analyzer
]

# ì—ì´ì „íŠ¸ í”„ë¡¬í”„íŠ¸
agent_prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ Python ê°œë°œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    ì½”ë“œ ì‘ì„±, ì‹¤í–‰, ë¶„ì„ì„ ë„ì™€ì£¼ì„¸ìš”.
    
    ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:
    - Python REPL: ì½”ë“œ ì‹¤í–‰
    - code_analyzer: ì½”ë“œ ë³µì¡ë„ ë¶„ì„
    """),
    ("placeholder", "{chat_history}"),
    ("human", "{input}"),
    ("placeholder", "{agent_scratchpad}")
])

# ì—ì´ì „íŠ¸ ìƒì„±
agent = create_openai_functions_agent(
    llm=chat,
    tools=tools,
    prompt=agent_prompt
)

agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    handle_parsing_errors=True
)

# ì—ì´ì „íŠ¸ ì‹¤í–‰
result = agent_executor.invoke({
    "input": "í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ êµ¬í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ê³  ì‹¤í–‰í•´ì„œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”."
})
```

### 2. ë¦¬ì•¡íŠ¸(ReAct) ì—ì´ì „íŠ¸

```python
from langchain.agents import create_react_agent

react_prompt = """
ë‹¤ìŒ ë„êµ¬ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:

{tools}

ë‹¤ìŒ í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”:

Question: ë‹µí•´ì•¼ í•  ì§ˆë¬¸
Thought: ë¬´ì—‡ì„ í•´ì•¼ í•˜ëŠ”ì§€ ìƒê°
Action: ìˆ˜í–‰í•  ì•¡ì…˜ [{tool_names} ì¤‘ í•˜ë‚˜]
Action Input: ì•¡ì…˜ì— ëŒ€í•œ ì…ë ¥
Observation: ì•¡ì…˜ì˜ ê²°ê³¼
... (ì´ Thought/Action/Action Input/Observationì„ í•„ìš”í•œ ë§Œí¼ ë°˜ë³µ)
Thought: ì´ì œ ìµœì¢… ë‹µì„ ì•Œì•˜ìŠµë‹ˆë‹¤
Final Answer: ì›ë˜ ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ë‹µ

ì‹œì‘í•˜ì„¸ìš”!

Question: {input}
Thought: {agent_scratchpad}
"""

react_agent = create_react_agent(
    llm=llm,
    tools=tools,
    prompt=PromptTemplate.from_template(react_prompt)
)
```

## ğŸ“š ë²¡í„° ìŠ¤í† ì–´ì™€ RAG

### 1. ë¬¸ì„œ ë¡œë”©ê³¼ ì²˜ë¦¬

```python
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

# ë¬¸ì„œ ë¡œë”©
loader = TextLoader("python_docs.txt")
documents = loader.load()

# í…ìŠ¤íŠ¸ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len
)
texts = text_splitter.split_documents(documents)

# ì„ë² ë”© ìƒì„±
embeddings = OpenAIEmbeddings()

# ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
vectorstore = Chroma.from_documents(
    documents=texts,
    embedding=embeddings,
    persist_directory="./chroma_db"
)
```

### 2. RAG ì²´ì¸ êµ¬í˜„

```python
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# ê²€ìƒ‰ê¸° ìƒì„±
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)

# RAG í”„ë¡¬í”„íŠ¸
rag_prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ Python ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
    ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
    
    ì»¨í…ìŠ¤íŠ¸:
    {context}
    """),
    ("human", "{input}")
])

# ë¬¸ì„œ ê²°í•© ì²´ì¸
document_chain = create_stuff_documents_chain(
    llm=chat,
    prompt=rag_prompt
)

# ê²€ìƒ‰ ì²´ì¸
retrieval_chain = create_retrieval_chain(
    retriever=retriever,
    combine_docs_chain=document_chain
)

# ì§ˆë¬¸í•˜ê¸°
response = retrieval_chain.invoke({
    "input": "Pythonì—ì„œ ì œë„ˆë ˆì´í„°ì™€ ì´í„°ë ˆì´í„°ì˜ ì°¨ì´ì ì€?"
})
print(response["answer"])
```

## ğŸ”„ ìŠ¤íŠ¸ë¦¬ë°ê³¼ ë¹„ë™ê¸°

### 1. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```python
# ìŠ¤íŠ¸ë¦¬ë° ì²´ì¸
streaming_chain = prompt | chat

for chunk in streaming_chain.stream({"topic": "ë¨¸ì‹ ëŸ¬ë‹", "level": "ì¤‘ê¸‰"}):
    print(chunk.content, end="", flush=True)
```

### 2. ë¹„ë™ê¸° ì²˜ë¦¬

```python
import asyncio

async def async_chain_example():
    # ë¹„ë™ê¸° ì²´ì¸ ì‹¤í–‰
    result = await chat.ainvoke([
        ("human", "ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì˜ ì¥ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.")
    ])
    return result.content

# ì‹¤í–‰
result = asyncio.run(async_chain_example())
```

## ğŸ› ï¸ ì‹¤ì „ ì˜ˆì œ: ì½”ë“œ ë¦¬ë·° ë´‡

```python
class CodeReviewBot:
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4", temperature=0.1)
        self.memory = ConversationBufferMemory(return_messages=True)
        
        # ì½”ë“œ ë¦¬ë·° í”„ë¡¬í”„íŠ¸
        self.review_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ Python ê°œë°œìì…ë‹ˆë‹¤.
            ì½”ë“œë¥¼ ë¦¬ë·°í•˜ê³  ë‹¤ìŒ ê´€ì ì—ì„œ í”¼ë“œë°±ì„ ì œê³µí•˜ì„¸ìš”:
            1. ì½”ë“œ í’ˆì§ˆ
            2. ì„±ëŠ¥
            3. ë³´ì•ˆ
            4. ê°€ë…ì„±
            5. ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤
            """),
            ("human", """
            ë‹¤ìŒ ì½”ë“œë¥¼ ë¦¬ë·°í•´ì£¼ì„¸ìš”:
            
            ```python
            {code}
            ```
            
            ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸: {context}
            """)
        ])
        
        self.chain = self.review_prompt | self.llm | JsonOutputParser()
    
    def review_code(self, code: str, context: str = ""):
        """ì½”ë“œë¥¼ ë¦¬ë·°í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            result = self.chain.invoke({
                "code": code,
                "context": context
            })
            return result
        except Exception as e:
            return {"error": f"ë¦¬ë·° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
    
    def get_suggestions(self, code: str):
        """ì½”ë“œ ê°œì„  ì œì•ˆì„ ì œê³µí•©ë‹ˆë‹¤."""
        suggestion_prompt = ChatPromptTemplate.from_messages([
            ("human", f"""
            ë‹¤ìŒ ì½”ë“œë¥¼ ê°œì„ í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•´ì£¼ì„¸ìš”:
            
            ```python
            {code}
            ```
            
            ê°œì„ ëœ ì½”ë“œì™€ ì„¤ëª…ì„ í•¨ê»˜ ì œê³µí•´ì£¼ì„¸ìš”.
            """)
        ])
        
        chain = suggestion_prompt | self.llm
        return chain.invoke({}).content

# ì‚¬ìš© ì˜ˆì œ
bot = CodeReviewBot()

sample_code = """
def get_user_data(user_id):
    import sqlite3
    conn = sqlite3.connect('users.db')
    cursor = conn.cursor()
    cursor.execute(f"SELECT * FROM users WHERE id = {user_id}")
    result = cursor.fetchone()
    conn.close()
    return result
"""

review_result = bot.review_code(sample_code, "ì‚¬ìš©ì ë°ì´í„° ì¡°íšŒ í•¨ìˆ˜")
suggestions = bot.get_suggestions(sample_code)

print("ë¦¬ë·° ê²°ê³¼:", review_result)
print("\nê°œì„  ì œì•ˆ:", suggestions)
```

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### 1. ìºì‹±

```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

# ìºì‹œ ì„¤ì •
set_llm_cache(InMemoryCache())

# ì´ì œ ë™ì¼í•œ ì§ˆë¬¸ì— ëŒ€í•´ ìºì‹œëœ ì‘ë‹µ ì‚¬ìš©
```

### 2. ë°°ì¹˜ ì²˜ë¦¬

```python
# ì—¬ëŸ¬ ì…ë ¥ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬
inputs = [
    {"topic": "í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë°"},
    {"topic": "ê°ì²´ì§€í–¥ í”„ë¡œê·¸ë˜ë°"},
    {"topic": "ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°"}
]

results = chain.batch(inputs)
```

## ğŸš¨ ì˜¤ë¥˜ ì²˜ë¦¬ì™€ ì¬ì‹œë„

```python
from langchain_core.runnables import RunnableRetry
from tenacity import retry, stop_after_attempt, wait_exponential

# ì¬ì‹œë„ ë¡œì§ì´ ìˆëŠ” ì²´ì¸
retry_chain = RunnableRetry(
    bound=chat,
    retry=retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
)

# ì»¤ìŠ¤í…€ ì˜¤ë¥˜ ì²˜ë¦¬
def handle_chain_error(error):
    print(f"ì²´ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}")
    return "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

try:
    result = chain.invoke({"input": "ë³µì¡í•œ ì§ˆë¬¸"})
except Exception as e:
    result = handle_chain_error(e)
```

## ğŸ¯ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 1. í”„ë¡¬í”„íŠ¸ ì„¤ê³„
- ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì§€ì‹œì‚¬í•­
- ì˜ˆì œ í¬í•¨
- ì¶œë ¥ í˜•ì‹ ëª…ì‹œ

### 2. ì²´ì¸ êµ¬ì„±
- ëª¨ë“ˆí™”ëœ ì»´í¬ë„ŒíŠ¸
- ì¬ì‚¬ìš© ê°€ëŠ¥í•œ êµ¬ì¡°
- ì˜¤ë¥˜ ì²˜ë¦¬ í¬í•¨

### 3. ì„±ëŠ¥ ê³ ë ¤ì‚¬í•­
- ì ì ˆí•œ ìºì‹± ì „ëµ
- ë°°ì¹˜ ì²˜ë¦¬ í™œìš©
- ë¹„ë™ê¸° ì²˜ë¦¬ ê³ ë ¤

### 4. ë³´ì•ˆ
- API í‚¤ ì•ˆì „í•œ ê´€ë¦¬
- ì…ë ¥ ê²€ì¦
- ì¶œë ¥ í•„í„°ë§

## ğŸ“ ë§ˆë¬´ë¦¬

LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œ ë‹¤ë£¬ ë‚´ìš©ë“¤ì„ í™œìš©í•˜ì—¬:

1. **ì‹œì‘í•˜ê¸°**: ê¸°ë³¸ ì»´í¬ë„ŒíŠ¸ë¶€í„° ì°¨ê·¼ì°¨ê·¼
2. **ì‹¤í—˜í•˜ê¸°**: ë‹¤ì–‘í•œ ì²´ì¸ê³¼ ì—ì´ì „íŠ¸ ì¡°í•©
3. **ìµœì í™”í•˜ê¸°**: ì„±ëŠ¥ê³¼ ì•ˆì •ì„± í–¥ìƒ
4. **í™•ì¥í•˜ê¸°**: ì‹¤ì œ ì„œë¹„ìŠ¤ì— ì ìš©

LangChainì˜ í’ë¶€í•œ ìƒíƒœê³„ë¥¼ í™œìš©í•˜ì—¬ í˜ì‹ ì ì¸ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”!

---

*ì´ í¬ìŠ¤íŠ¸ê°€ ë„ì›€ì´ ë˜ì…¨ë‹¤ë©´ ê³µìœ í•´ì£¼ì„¸ìš”! LangChainì— ëŒ€í•œ ë” ìì„¸í•œ ë‚´ìš©ì´ë‚˜ íŠ¹ì • ì‚¬ìš© ì‚¬ë¡€ì— ëŒ€í•œ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ëŒ“ê¸€ë¡œ ë‚¨ê²¨ì£¼ì„¸ìš”.*