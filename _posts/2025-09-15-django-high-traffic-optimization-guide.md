---
layout: post
title: "Django ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ ì²˜ë¦¬ ì™„ì „ ê°€ì´ë“œ: ì´ˆë‹¹ 10ë§Œ ìš”ì²­ì„ ê°ë‹¹í•˜ëŠ” ì•„í‚¤í…ì²˜"
date: 2025-09-15 14:00:00 +0900
categories: [Django, Performance, Scalability, Architecture]
tags: [Django, High Traffic, Performance Optimization, Scalability, Load Balancing, Caching, Database Optimization, ASGI, Microservices]
image: "/assets/img/posts/2025-09-15-django-high-traffic-optimization-guide.webp"
---

Djangoë¡œ êµ¬ì¶•í•œ ì„œë¹„ìŠ¤ê°€ ì„±ì¥í•˜ë©´ì„œ **ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½**ì„ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ” ìƒí™©ì— ì§ë©´í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ˆë‹¹ ìˆ˜ì²œ, ìˆ˜ë§Œ ê±´ì˜ ìš”ì²­ì„ ì•ˆì •ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë©´ì„œë„ ë¹ ë¥¸ ì‘ë‹µ ì‹œê°„ì„ ìœ ì§€í•˜ëŠ” ê²ƒì€ ì‰½ì§€ ì•Šì€ ë„ì „ì…ë‹ˆë‹¤. ì´ ê¸€ì—ì„œëŠ” Django ì• í”Œë¦¬ì¼€ì´ì…˜ì„ **ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ì— ìµœì í™”**í•˜ëŠ” ì¢…í•©ì ì¸ ì „ëµì„ ì‹¤ì „ ì¤‘ì‹¬ìœ¼ë¡œ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.

## ğŸ¯ ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ ì²˜ë¦¬ì˜ í•µì‹¬ ì›ì¹™

### ì„±ëŠ¥ ëª©í‘œ ì„¤ì •

ì‹¤ì œ ëŒ€ìš©ëŸ‰ ì„œë¹„ìŠ¤ì—ì„œ ëª©í‘œë¡œ í•˜ëŠ” ì„±ëŠ¥ ì§€í‘œë“¤ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

```python
# ì„±ëŠ¥ ëª©í‘œ ì˜ˆì‹œ
PERFORMANCE_TARGETS = {
    'ì‘ë‹µ_ì‹œê°„': {
        'P95': '200ms',  # 95%ì˜ ìš”ì²­ì´ 200ms ì´ë‚´ ì‘ë‹µ
        'P99': '500ms',  # 99%ì˜ ìš”ì²­ì´ 500ms ì´ë‚´ ì‘ë‹µ
        'í‰ê· ': '100ms'   # í‰ê·  ì‘ë‹µ ì‹œê°„
    },
    'ì²˜ë¦¬ëŸ‰': {
        'ì´ˆë‹¹_ìš”ì²­ìˆ˜': 50000,  # 50K RPS
        'ì¼ì¼_ìš”ì²­ìˆ˜': 4_000_000_000,  # 40ì–µ ìš”ì²­/ì¼
        'ë™ì‹œ_ì‚¬ìš©ì': 100000  # 10ë§Œ ë™ì‹œ ì‚¬ìš©ì
    },
    'ê°€ìš©ì„±': {
        'SLA': '99.9%',  # ì—°ê°„ 8.76ì‹œê°„ ë‹¤ìš´íƒ€ì„
        'MTTR': '5ë¶„',   # í‰ê·  ë³µêµ¬ ì‹œê°„
        'MTBF': '30ì¼'   # í‰ê·  ì¥ì•  ê°„ê²©
    },
    'ë¦¬ì†ŒìŠ¤': {
        'CPU_ì‚¬ìš©ë¥ ': '70%',  # í‰ê·  CPU ì‚¬ìš©ë¥ 
        'ë©”ëª¨ë¦¬_ì‚¬ìš©ë¥ ': '80%',  # í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
        'DB_ì»¤ë„¥ì…˜í’€': '80%'   # DB ì—°ê²° ì‚¬ìš©ë¥ 
    }
}
```

### í™•ì¥ì„± íŒ¨í„´ ì´í•´

```python
# í™•ì¥ì„±ì˜ ë‘ ê°€ì§€ ë°©í–¥
SCALABILITY_PATTERNS = {
    'ìˆ˜ì§ì _í™•ì¥': {
        'ì„¤ëª…': 'Scale Up - ì„œë²„ ì‚¬ì–‘ ì—…ê·¸ë ˆì´ë“œ',
        'ì¥ì ': ['êµ¬í˜„ ë‹¨ìˆœ', 'ë°ì´í„° ì¼ê´€ì„±'],
        'ë‹¨ì ': ['ë¹„ìš© ì¦ê°€', 'ë¬¼ë¦¬ì  í•œê³„'],
        'ì ìš©_ì˜ˆ': 'CPU/ë©”ëª¨ë¦¬ ì¦ì„¤, SSD ì—…ê·¸ë ˆì´ë“œ'
    },
    'ìˆ˜í‰ì _í™•ì¥': {
        'ì„¤ëª…': 'Scale Out - ì„œë²„ ëŒ€ìˆ˜ ì¦ê°€',
        'ì¥ì ': ['ë¬´ì œí•œ í™•ì¥', 'ì¥ì•  ê²©ë¦¬'],
        'ë‹¨ì ': ['ë³µì¡ì„± ì¦ê°€', 'ë°ì´í„° ë™ê¸°í™”'],
        'ì ìš©_ì˜ˆ': 'ë¡œë“œ ë°¸ëŸ°ì„œ, ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤'
    }
}
```

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ìµœì í™” ì „ëµ

### 1. ASGI vs WSGI: ë¹„ë™ê¸° ì²˜ë¦¬ì˜ ìœ„ë ¥

```python
# WSGI ê¸°ë°˜ ì „í†µì  êµ¬ì¡°
# gunicorn settings.py
bind = "0.0.0.0:8000"
workers = 16  # CPU ì½”ì–´ ìˆ˜ * 2
worker_class = "sync"
worker_connections = 1000
max_requests = 1000
max_requests_jitter = 100
keepalive = 2
timeout = 30

# ë™ì‹œ ì²˜ë¦¬ ê°€ëŠ¥ ìš”ì²­: 16 * 1 = 16ê°œ (ë¸”ë¡œí‚¹ I/O)

# ASGI ê¸°ë°˜ ë¹„ë™ê¸° êµ¬ì¡°
# uvicorn settings
import uvicorn

if __name__ == "__main__":
    uvicorn.run(
        "myproject.asgi:application",
        host="0.0.0.0",
        port=8000,
        workers=4,  # CPU ì½”ì–´ ìˆ˜
        loop="uvloop",  # ê³ ì„±ëŠ¥ ì´ë²¤íŠ¸ ë£¨í”„
        http="httptools",  # ê³ ì„±ëŠ¥ HTTP íŒŒì„œ
        access_log=False,  # ìš´ì˜í™˜ê²½ì—ì„œëŠ” ë¹„í™œì„±í™”
        server_header=False
    )

# ë™ì‹œ ì²˜ë¦¬ ê°€ëŠ¥ ìš”ì²­: ìˆ˜ì²œ ê°œ (ë¹„ë¸”ë¡œí‚¹ I/O)
```

### 2. Django ì„¤ì • ìµœì í™”

```python
# settings/production.py
import os
from .base import *

# ë³´ì•ˆ ì„¤ì •
DEBUG = False
ALLOWED_HOSTS = ['*.yourdomain.com', 'yourdomain.com']
SECRET_KEY = os.environ['SECRET_KEY']

# ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': os.environ['DB_NAME'],
        'USER': os.environ['DB_USER'],
        'PASSWORD': os.environ['DB_PASSWORD'],
        'HOST': os.environ['DB_HOST'],
        'PORT': os.environ['DB_PORT'],
        'CONN_MAX_AGE': 600,  # ì—°ê²° ì¬ì‚¬ìš©
        'OPTIONS': {
            'MAX_CONNS': 20,  # ìµœëŒ€ ì—°ê²° ìˆ˜
            'connect_timeout': 10,
            'options': '-c default_transaction_isolation=read_committed'
        }
    },
    # ì½ê¸° ì „ìš© ë³µì œë³¸
    'read_replica': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': os.environ['DB_NAME'],
        'USER': os.environ['DB_READONLY_USER'],
        'PASSWORD': os.environ['DB_READONLY_PASSWORD'],
        'HOST': os.environ['DB_READONLY_HOST'],
        'PORT': os.environ['DB_PORT'],
        'CONN_MAX_AGE': 600,
        'OPTIONS': {
            'MAX_CONNS': 30,
            'connect_timeout': 10,
        }
    }
}

# ë°ì´í„°ë² ì´ìŠ¤ ë¼ìš°íŒ…
DATABASE_ROUTERS = ['myproject.routers.DatabaseRouter']

# ìºì‹œ ì„¤ì •
CACHES = {
    'default': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': f"redis://{os.environ['REDIS_HOST']}:6379/0",
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
            'CONNECTION_POOL_KWARGS': {
                'max_connections': 50,
                'retry_on_timeout': True,
            },
            'SERIALIZER': 'django_redis.serializers.json.JSONSerializer',
            'COMPRESSOR': 'django_redis.compressors.zlib.ZlibCompressor',
        },
        'TIMEOUT': 300,
        'KEY_PREFIX': 'myproject',
        'VERSION': 1,
    },
    'sessions': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': f"redis://{os.environ['REDIS_HOST']}:6379/1",
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
            'CONNECTION_POOL_KWARGS': {'max_connections': 30},
        },
        'TIMEOUT': 86400,  # 24ì‹œê°„
    }
}

# ì„¸ì…˜ ìµœì í™”
SESSION_ENGINE = 'django.contrib.sessions.backends.cache'
SESSION_CACHE_ALIAS = 'sessions'
SESSION_COOKIE_AGE = 86400  # 24ì‹œê°„
SESSION_SAVE_EVERY_REQUEST = False  # ì„±ëŠ¥ ìµœì í™”

# ì •ì  íŒŒì¼ ìµœì í™”
STATIC_URL = f"https://{os.environ['CDN_DOMAIN']}/static/"
MEDIA_URL = f"https://{os.environ['CDN_DOMAIN']}/media/"

# ì••ì¶• ë° ìµœì í™”
STATICFILES_STORAGE = 'whitenoise.storage.CompressedManifestStaticFilesStorage'
```

## ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì „ëµ

ë°ì´í„°ë² ì´ìŠ¤ëŠ” ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ ì²˜ë¦¬ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë³‘ëª©ì  ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. íš¨ê³¼ì ì¸ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”ë¥¼ í†µí•´ ì„±ëŠ¥ì„ ê·¹ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 1. ë°ì´í„°ë² ì´ìŠ¤ ë¼ìš°íŒ…ê³¼ ì½ê¸° ë³µì œë³¸ í™œìš©

```python
# myproject/routers.py
class DatabaseRouter:
    """
    ì½ê¸°/ì“°ê¸° ë¶„ì‚°ì„ ìœ„í•œ ë°ì´í„°ë² ì´ìŠ¤ ë¼ìš°í„°
    ì½ê¸° ì‘ì—…ì€ ë³µì œë³¸ìœ¼ë¡œ, ì“°ê¸° ì‘ì—…ì€ ë§ˆìŠ¤í„°ë¡œ ë¼ìš°íŒ…
    """
    
    READ_ONLY_MODELS = {
        'analytics', 'reports', 'logs'  # ì½ê¸° ì „ìš© ëª¨ë¸ë“¤
    }
    
    def db_for_read(self, model, **hints):
        """ì½ê¸° ì‘ì—… ë¼ìš°íŒ…"""
        
        # íŠ¹ì • ëª¨ë¸ì€ í•­ìƒ ì½ê¸° ì „ìš© DB ì‚¬ìš©
        if model._meta.app_label in self.READ_ONLY_MODELS:
            return 'read_replica'
        
        # í˜„ì¬ ìŠ¤ë ˆë“œê°€ íŠ¸ëœì­ì…˜ ì¤‘ì´ë©´ ë§ˆìŠ¤í„° DB ì‚¬ìš©
        from django.db import transaction
        if transaction.get_connection().in_atomic_block:
            return 'default'
        
        # ì¼ë°˜ì ì¸ ì½ê¸° ì‘ì—…ì€ ë³µì œë³¸ìœ¼ë¡œ
        return 'read_replica'
    
    def db_for_write(self, model, **hints):
        """ì“°ê¸° ì‘ì—…ì€ í•­ìƒ ë§ˆìŠ¤í„° DB"""
        return 'default'
    
    def allow_relation(self, obj1, obj2, **hints):
        """ê´€ê³„ í—ˆìš© ì—¬ë¶€"""
        db_set = {'default', 'read_replica'}
        if obj1._state.db in db_set and obj2._state.db in db_set:
            return True
        return None
    
    def allow_migrate(self, db, app_label, model_name=None, **hints):
        """ë§ˆì´ê·¸ë ˆì´ì…˜ í—ˆìš© ì—¬ë¶€"""
        return db == 'default'

# ìˆ˜ë™ ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒì„ ìœ„í•œ ë§¤ë‹ˆì €
class ReadOnlyManager(models.Manager):
    """ì½ê¸° ì „ìš© ë§¤ë‹ˆì €"""
    
    def get_queryset(self):
        return super().get_queryset().using('read_replica')

class OptimizedQueryManager(models.Manager):
    """ìµœì í™”ëœ ì¿¼ë¦¬ ë§¤ë‹ˆì €"""
    
    def get_queryset(self):
        return super().get_queryset().select_related().prefetch_related()
    
    def high_traffic_filter(self, **kwargs):
        """ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ìš© í•„í„°"""
        return self.get_queryset().filter(**kwargs).only(
            'id', 'name', 'status'  # í•„ìš”í•œ í•„ë“œë§Œ ì¡°íšŒ
        )
    
    def with_cache(self, cache_key, timeout=300):
        """ìºì‹œì™€ í•¨ê»˜ ì¡°íšŒ"""
        from django.core.cache import cache
        
        cached_result = cache.get(cache_key)
        if cached_result is not None:
            return cached_result
        
        result = list(self.get_queryset())
        cache.set(cache_key, result, timeout)
        return result

# ëª¨ë¸ ì˜ˆì œ
class Product(models.Model):
    name = models.CharField(max_length=200, db_index=True)
    price = models.DecimalField(max_digits=10, decimal_places=2)
    category = models.ForeignKey('Category', on_delete=models.CASCADE)
    status = models.CharField(max_length=20, default='active', db_index=True)
    created_at = models.DateTimeField(auto_now_add=True, db_index=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    objects = models.Manager()  # ê¸°ë³¸ ë§¤ë‹ˆì €
    readonly = ReadOnlyManager()  # ì½ê¸° ì „ìš© ë§¤ë‹ˆì €
    optimized = OptimizedQueryManager()  # ìµœì í™”ëœ ë§¤ë‹ˆì €
    
    class Meta:
        indexes = [
            models.Index(fields=['status', 'created_at']),  # ë³µí•© ì¸ë±ìŠ¤
            models.Index(fields=['category', 'price']),
            models.Index(fields=['-created_at']),  # ì •ë ¬ìš© ì¸ë±ìŠ¤
        ]
        db_table = 'products'
```

### 2. ì—°ê²° í’€ë§ê³¼ ì»¤ë„¥ì…˜ ìµœì í™”

```python
# PostgreSQL ì—°ê²° í’€ ìµœì í™”
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': os.environ['DB_NAME'],
        'USER': os.environ['DB_USER'],
        'PASSWORD': os.environ['DB_PASSWORD'],
        'HOST': os.environ['DB_HOST'],
        'PORT': os.environ['DB_PORT'],
        'CONN_MAX_AGE': 600,  # 10ë¶„ê°„ ì—°ê²° ì¬ì‚¬ìš©
        'OPTIONS': {
            'MAX_CONNS': 20,  # í”„ë¡œì„¸ìŠ¤ë‹¹ ìµœëŒ€ ì—°ê²° ìˆ˜
            'MIN_CONNS': 5,   # ìµœì†Œ ì—°ê²° ìˆ˜ ìœ ì§€
            'connect_timeout': 10,
            'options': '-c default_transaction_isolation=read_committed'
        },
        'TEST': {
            'NAME': 'test_' + os.environ['DB_NAME'],
        }
    }
}

# ì»¤ìŠ¤í…€ ë°ì´í„°ë² ì´ìŠ¤ ë˜í¼
class DatabaseConnectionManager:
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬"""
    
    def __init__(self):
        self.connection_stats = {
            'total_queries': 0,
            'slow_queries': 0,
            'connection_errors': 0,
            'active_connections': 0
        }
    
    def execute_query(self, sql, params=None, using='default'):
        """ì¿¼ë¦¬ ì‹¤í–‰ with ëª¨ë‹ˆí„°ë§"""
        import time
        from django.db import connections
        
        start_time = time.time()
        
        try:
            connection = connections[using]
            with connection.cursor() as cursor:
                cursor.execute(sql, params or [])
                result = cursor.fetchall()
            
            execution_time = time.time() - start_time
            self.connection_stats['total_queries'] += 1
            
            # ëŠë¦° ì¿¼ë¦¬ ê°ì§€
            if execution_time > 1.0:
                self.connection_stats['slow_queries'] += 1
                self.log_slow_query(sql, execution_time, params)
            
            return result
            
        except Exception as e:
            self.connection_stats['connection_errors'] += 1
            raise e
    
    def log_slow_query(self, sql, execution_time, params):
        """ëŠë¦° ì¿¼ë¦¬ ë¡œê¹…"""
        import logging
        logger = logging.getLogger('slow_queries')
        
        logger.warning(f"Slow query detected: {execution_time:.3f}s")
        logger.warning(f"SQL: {sql}")
        logger.warning(f"Params: {params}")
    
    def get_connection_stats(self):
        """ì—°ê²° í†µê³„ ë°˜í™˜"""
        return self.connection_stats

# ì „ì—­ ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì €
db_manager = DatabaseConnectionManager()
```

### 3. ì¸ë±ìŠ¤ ìµœì í™”ì™€ ì¿¼ë¦¬ ì„±ëŠ¥ í–¥ìƒ

```python
# ì¸ë±ìŠ¤ ì „ëµ
class IndexOptimizedModel(models.Model):
    """ì¸ë±ìŠ¤ ìµœì í™”ëœ ëª¨ë¸ ì˜ˆì œ"""
    
    # ê¸°ë³¸ í•„ë“œë“¤
    name = models.CharField(max_length=200)
    status = models.CharField(max_length=20, choices=[
        ('active', 'Active'),
        ('inactive', 'Inactive'),
        ('pending', 'Pending')
    ])
    category_id = models.IntegerField()
    price = models.DecimalField(max_digits=10, decimal_places=2)
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    class Meta:
        indexes = [
            # 1. ë‹¨ì¼ í•„ë“œ ì¸ë±ìŠ¤
            models.Index(fields=['status']),
            models.Index(fields=['category_id']),
            models.Index(fields=['-created_at']),  # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ìš©
            
            # 2. ë³µí•© ì¸ë±ìŠ¤ (ìˆœì„œ ì¤‘ìš”!)
            models.Index(fields=['status', 'category_id']),
            models.Index(fields=['status', 'created_at']),
            models.Index(fields=['category_id', 'price']),
            
            # 3. ë¶€ë¶„ ì¸ë±ìŠ¤ (PostgreSQL)
            models.Index(
                fields=['name'],
                name='idx_active_products_name',
                condition=models.Q(status='active')
            ),
        ]

# ì¿¼ë¦¬ ìµœì í™” ìœ í‹¸ë¦¬í‹°
class QueryOptimizer:
    """ì¿¼ë¦¬ ìµœì í™” ë„êµ¬"""
    
    @staticmethod
    def optimize_select_related(queryset, depth=2):
        """ìë™ìœ¼ë¡œ select_related ìµœì í™”"""
        model = queryset.model
        select_fields = []
        
        for field in model._meta.get_fields():
            if hasattr(field, 'related_model') and depth > 0:
                if field.one_to_one or field.many_to_one:
                    select_fields.append(field.name)
        
        return queryset.select_related(*select_fields)
    
    @staticmethod
    def bulk_create_optimized(model_class, objects, batch_size=1000):
        """ìµœì í™”ëœ bulk_create"""
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(objects), batch_size):
            batch = objects[i:i + batch_size]
            model_class.objects.bulk_create(
                batch,
                batch_size=batch_size,
                ignore_conflicts=True  # ì¤‘ë³µ ë¬´ì‹œ
            )

# ì‚¬ìš© ì˜ˆì œ
def get_products_optimized():
    """ìµœì í™”ëœ ìƒí’ˆ ì¡°íšŒ"""
    
    queryset = Product.objects.all()
    
    # ìë™ ìµœì í™” ì ìš©
    queryset = QueryOptimizer.optimize_select_related(queryset)
    
    # í•„ìš”í•œ í•„ë“œë§Œ ì„ íƒ
    queryset = queryset.only('id', 'name', 'price', 'status')
    
    # ì¡°ê±´ ìµœì í™”
    queryset = queryset.filter(
        status='active'
    ).order_by('-created_at')
    
    return queryset[:100]  # í˜ì´ì§€ë„¤ì´ì…˜
```

## ğŸ¯ ìºì‹± ì „ëµ: ì„±ëŠ¥ì˜ í•µì‹¬

ìºì‹±ì€ ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ ì²˜ë¦¬ì—ì„œ ê°€ì¥ ì¦‰ê°ì ì´ê³  íš¨ê³¼ì ì¸ ì„±ëŠ¥ í–¥ìƒ ë°©ë²•ì…ë‹ˆë‹¤. DjangoëŠ” ë‹¤ì–‘í•œ ë ˆë²¨ì˜ ìºì‹±ì„ ì§€ì›í•©ë‹ˆë‹¤.

### 1. ë‹¤ì¸µ ìºì‹± ì•„í‚¤í…ì²˜

```python
# ìºì‹± ê³„ì¸µ êµ¬ì¡°
CACHING_LAYERS = {
    'L1_Browser': {
        'location': 'Client Browser',
        'ttl': '1 hour',
        'scope': 'Static assets, API responses'
    },
    'L2_CDN': {
        'location': 'CloudFlare/AWS CloudFront',
        'ttl': '24 hours',
        'scope': 'Static files, Public content'
    },
    'L3_Reverse_Proxy': {
        'location': 'Nginx/Varnish',
        'ttl': '10 minutes',
        'scope': 'Full page cache'
    },
    'L4_Application': {
        'location': 'Django Cache Framework',
        'ttl': '5 minutes',
        'scope': 'View cache, Template fragments'
    },
    'L5_Database': {
        'location': 'Redis/Memcached',
        'ttl': '1 hour',
        'scope': 'Query results, Session data'
    }
}

# Redis í´ëŸ¬ìŠ¤í„° ì„¤ì •
CACHES = {
    'default': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': [
            f"redis://{os.environ['REDIS_NODE1']}:6379/0",
            f"redis://{os.environ['REDIS_NODE2']}:6379/0",
            f"redis://{os.environ['REDIS_NODE3']}:6379/0",
        ],
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.ShardClient',
            'CONNECTION_POOL_KWARGS': {
                'max_connections': 100,
                'retry_on_timeout': True,
            },
            'SERIALIZER': 'django_redis.serializers.msgpack.MSGPackSerializer',
            'COMPRESSOR': 'django_redis.compressors.zlib.ZlibCompressor',
        },
        'TIMEOUT': 300,
        'KEY_PREFIX': 'myproject',
    },
    'sessions': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': f"redis://{os.environ['REDIS_SESSION']}:6379/1",
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
            'CONNECTION_POOL_KWARGS': {'max_connections': 50},
        },
        'TIMEOUT': 86400,
    },
    'long_term': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': f"redis://{os.environ['REDIS_LONGTERM']}:6379/2",
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
            'CONNECTION_POOL_KWARGS': {'max_connections': 30},
        },
        'TIMEOUT': 3600,  # 1ì‹œê°„
    }
}
```

### 2. ìŠ¤ë§ˆíŠ¸ ìºì‹± ë§¤ë‹ˆì €

```python
import hashlib
import pickle
import time
from django.core.cache import cache, caches
from django.utils.decorators import method_decorator
from django.views.decorators.cache import cache_page
from django.views.decorators.vary import vary_on_headers
import logging

logger = logging.getLogger(__name__)

class SmartCacheManager:
    """ì§€ëŠ¥í˜• ìºì‹± ê´€ë¦¬ì"""
    
    def __init__(self):
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'sets': 0,
            'deletes': 0
        }
        self.cache_strategies = {
            'hot_data': {'timeout': 60, 'alias': 'default'},
            'warm_data': {'timeout': 300, 'alias': 'default'},
            'cold_data': {'timeout': 3600, 'alias': 'long_term'},
            'session_data': {'timeout': 86400, 'alias': 'sessions'}
        }
    
    def generate_cache_key(self, prefix, *args, **kwargs):
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_data = f"{prefix}:{':'.join(map(str, args))}:{':'.join(f'{k}={v}' for k, v in sorted(kwargs.items()))}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def get_cached_data(self, cache_key, strategy='warm_data'):
        """ìºì‹œëœ ë°ì´í„° ì¡°íšŒ"""
        cache_config = self.cache_strategies.get(strategy, self.cache_strategies['warm_data'])
        cache_instance = caches[cache_config['alias']]
        
        try:
            data = cache_instance.get(cache_key)
            if data is not None:
                self.cache_stats['hits'] += 1
                return data
            else:
                self.cache_stats['misses'] += 1
                return None
        except Exception as e:
            logger.error(f"Cache get error: {e}")
            self.cache_stats['misses'] += 1
            return None
    
    def set_cached_data(self, cache_key, data, strategy='warm_data'):
        """ë°ì´í„° ìºì‹±"""
        cache_config = self.cache_strategies.get(strategy, self.cache_strategies['warm_data'])
        cache_instance = caches[cache_config['alias']]
        
        try:
            cache_instance.set(cache_key, data, cache_config['timeout'])
            self.cache_stats['sets'] += 1
            return True
        except Exception as e:
            logger.error(f"Cache set error: {e}")
            return False
    
    def delete_cached_data(self, cache_key, strategy='warm_data'):
        """ìºì‹œ ì‚­ì œ"""
        cache_config = self.cache_strategies.get(strategy, self.cache_strategies['warm_data'])
        cache_instance = caches[cache_config['alias']]
        
        try:
            cache_instance.delete(cache_key)
            self.cache_stats['deletes'] += 1
            return True
        except Exception as e:
            logger.error(f"Cache delete error: {e}")
            return False
    
    def cached_function(self, timeout=300, strategy='warm_data', key_prefix=None):
        """í•¨ìˆ˜ ê²°ê³¼ ìºì‹± ë°ì½”ë ˆì´í„°"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                # ìºì‹œ í‚¤ ìƒì„±
                if key_prefix:
                    cache_key = self.generate_cache_key(key_prefix, func.__name__, *args, **kwargs)
                else:
                    cache_key = self.generate_cache_key(func.__module__, func.__name__, *args, **kwargs)
                
                # ìºì‹œì—ì„œ í™•ì¸
                cached_result = self.get_cached_data(cache_key, strategy)
                if cached_result is not None:
                    return cached_result
                
                # í•¨ìˆ˜ ì‹¤í–‰
                result = func(*args, **kwargs)
                
                # ê²°ê³¼ ìºì‹±
                self.set_cached_data(cache_key, result, strategy)
                
                return result
            return wrapper
        return decorator
    
    def invalidate_pattern(self, pattern):
        """íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ìºì‹œ ë¬´íš¨í™”"""
        from django_redis import get_redis_connection
        
        for alias in ['default', 'long_term', 'sessions']:
            try:
                redis_conn = get_redis_connection(alias)
                for key in redis_conn.scan_iter(match=pattern):
                    redis_conn.delete(key)
            except Exception as e:
                logger.error(f"Pattern invalidation error for {alias}: {e}")
    
    def get_cache_statistics(self):
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total_requests = self.cache_stats['hits'] + self.cache_stats['misses']
        hit_rate = (self.cache_stats['hits'] / total_requests * 100) if total_requests > 0 else 0
        
        return {
            'hit_rate': f"{hit_rate:.2f}%",
            'total_requests': total_requests,
            **self.cache_stats
        }

# ì „ì—­ ìºì‹œ ë§¤ë‹ˆì €
cache_manager = SmartCacheManager()
```

### 3. ë·° ë ˆë²¨ ìºì‹±

```python
from django.views.generic import ListView, DetailView
from django.utils.decorators import method_decorator
from django.views.decorators.cache import cache_page
from django.views.decorators.vary import vary_on_headers, vary_on_cookie

# í˜ì´ì§€ ì „ì²´ ìºì‹±
@method_decorator(cache_page(60 * 5), name='dispatch')  # 5ë¶„ ìºì‹±
@method_decorator(vary_on_headers('User-Agent', 'Accept-Language'), name='dispatch')
class ProductListView(ListView):
    """ìƒí’ˆ ëª©ë¡ ë·° (ì „ì²´ í˜ì´ì§€ ìºì‹±)"""
    
    model = Product
    template_name = 'products/list.html'
    paginate_by = 20
    
    def get_queryset(self):
        # ì´ë¯¸ ìºì‹±ë˜ë¯€ë¡œ ë³µì¡í•œ ì¿¼ë¦¬ë„ OK
        return Product.objects.select_related('category').prefetch_related('tags').filter(
            status='active'
        ).order_by('-created_at')

# ì¡°ê±´ë¶€ ìºì‹±
class ConditionalCacheView(DetailView):
    """ì¡°ê±´ë¶€ ìºì‹± ë·°"""
    
    model = Product
    template_name = 'products/detail.html'
    
    @cache_manager.cached_function(timeout=600, strategy='warm_data', key_prefix='product_detail')
    def get_object(self):
        """ê°ì²´ ì¡°íšŒ (ìºì‹±ë¨)"""
        return super().get_object()
    
    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        
        # ì‚¬ìš©ìë³„ ë‹¤ë¥¸ ë°ì´í„°ëŠ” ë³„ë„ ì²˜ë¦¬
        if self.request.user.is_authenticated:
            context['user_favorites'] = self.get_user_favorites()
            context['recommendations'] = self.get_recommendations()
        
        return context
    
    @cache_manager.cached_function(timeout=1800, strategy='cold_data')
    def get_recommendations(self):
        """ì¶”ì²œ ìƒí’ˆ (30ë¶„ ìºì‹±)"""
        return Product.objects.filter(
            category=self.object.category
        ).exclude(
            id=self.object.id
        ).order_by('?')[:5]

# API ì‘ë‹µ ìºì‹±
from rest_framework.views import APIView
from rest_framework.response import Response
from django.core.cache import cache

class CachedAPIView(APIView):
    """ìºì‹œëœ API ë·°"""
    
    def get(self, request, *args, **kwargs):
        # ìºì‹œ í‚¤ ìƒì„± (ì‚¬ìš©ì, ë§¤ê°œë³€ìˆ˜ í¬í•¨)
        cache_key = f"api_response:{request.user.id}:{request.GET.urlencode()}"
        
        # ìºì‹œ í™•ì¸
        cached_response = cache.get(cache_key)
        if cached_response:
            return Response(cached_response)
        
        # ë°ì´í„° ìƒì„±
        data = self.generate_response_data()
        
        # ì‘ë‹µ ìºì‹± (5ë¶„)
        cache.set(cache_key, data, 300)
        
        return Response(data)
    
    def generate_response_data(self):
        """ì‘ë‹µ ë°ì´í„° ìƒì„±"""
        return {
            'products': list(Product.objects.values('id', 'name', 'price')[:100]),
            'timestamp': time.time()
        }
```

### 4. í…œí”Œë¦¿ í”„ë˜ê·¸ë¨¼íŠ¸ ìºì‹±

{% raw %}
```html
<!-- templates/products/detail.html -->
{% load cache %}

<div class="product-detail">
    <!-- ê¸°ë³¸ ìƒí’ˆ ì •ë³´ (ìì£¼ ë³€ê²½ë˜ì§€ ì•ŠìŒ) -->
    {% cache 3600 product_basic product.id %}
    <div class="product-info">
        <h1>{{ product.name }}</h1>
        <p class="price">${{ product.price }}</p>
        <div class="description">{{ product.description }}</div>
    </div>
    {% endcache %}
    
    <!-- ë¦¬ë·° ì„¹ì…˜ (ì¤‘ê°„ ì •ë„ ìºì‹±) -->
    {% cache 600 product_reviews product.id %}
    <div class="reviews">
        <h3>ê³ ê° ë¦¬ë·°</h3>
        {% for review in product.reviews.all %}
            <div class="review">
                <div class="rating">{{ review.rating }}â˜…</div>
                <p>{{ review.comment }}</p>
            </div>
        {% endfor %}
    </div>
    {% endcache %}
    
    <!-- ì‚¬ìš©ìë³„ ë°ì´í„° (ìºì‹±í•˜ì§€ ì•ŠìŒ) -->
    <div class="user-actions">
        {% if user.is_authenticated %}
            <button class="add-to-cart">ì¥ë°”êµ¬ë‹ˆ ë‹´ê¸°</button>
            <button class="add-to-wishlist">ì°œí•˜ê¸°</button>
        {% endif %}
    </div>
    
    <!-- ê´€ë ¨ ìƒí’ˆ (ê¸´ ìºì‹±) -->
    {% cache 1800 related_products product.category.id %}
    <div class="related-products">
        <h3>ê´€ë ¨ ìƒí’ˆ</h3>
        {% for related in related_products %}
            <div class="product-item">
                <a href="{{ related.get_absolute_url }}">{{ related.name }}</a>
            </div>
        {% endfor %}
    </div>
    {% endcache %}
</div>
```
{% endraw %}

### 5. ìºì‹œ ë¬´íš¨í™” ì „ëµ

```python
from django.db.models.signals import post_save, post_delete, m2m_changed
from django.dispatch import receiver
import logging

logger = logging.getLogger(__name__)

class CacheInvalidationManager:
    """ìºì‹œ ë¬´íš¨í™” ê´€ë¦¬ì"""
    
    @staticmethod
    def invalidate_product_cache(product_id):
        """ìƒí’ˆ ê´€ë ¨ ìºì‹œ ë¬´íš¨í™”"""
        patterns = [
            f"*product_detail*{product_id}*",
            f"*product_list*",
            f"*api_response*",
            f"*product_basic*{product_id}*",
            f"*product_reviews*{product_id}*"
        ]
        
        for pattern in patterns:
            cache_manager.invalidate_pattern(pattern)
        
        logger.info(f"Invalidated cache for product {product_id}")
    
    @staticmethod
    def invalidate_category_cache(category_id):
        """ì¹´í…Œê³ ë¦¬ ê´€ë ¨ ìºì‹œ ë¬´íš¨í™”"""
        patterns = [
            f"*category*{category_id}*",
            f"*related_products*{category_id}*",
            f"*product_list*"
        ]
        
        for pattern in patterns:
            cache_manager.invalidate_pattern(pattern)
        
        logger.info(f"Invalidated cache for category {category_id}")

# ì‹œê·¸ë„ ê¸°ë°˜ ìºì‹œ ë¬´íš¨í™”
@receiver(post_save, sender=Product)
def invalidate_product_cache_on_save(sender, instance, created, **kwargs):
    """ìƒí’ˆ ì €ì¥ ì‹œ ìºì‹œ ë¬´íš¨í™”"""
    CacheInvalidationManager.invalidate_product_cache(instance.id)
    
    if instance.category:
        CacheInvalidationManager.invalidate_category_cache(instance.category.id)

@receiver(post_delete, sender=Product)
def invalidate_product_cache_on_delete(sender, instance, **kwargs):
    """ìƒí’ˆ ì‚­ì œ ì‹œ ìºì‹œ ë¬´íš¨í™”"""
    CacheInvalidationManager.invalidate_product_cache(instance.id)
    
    if instance.category:
        CacheInvalidationManager.invalidate_category_cache(instance.category.id)

# ë°°ì¹˜ ìºì‹œ ê°±ì‹ 
class CacheWarmupManager:
    """ìºì‹œ ì˜ˆì—´ ê´€ë¦¬ì"""
    
    @staticmethod
    def warmup_popular_products():
        """ì¸ê¸° ìƒí’ˆ ìºì‹œ ì˜ˆì—´"""
        popular_products = Product.objects.filter(
            status='active'
        ).order_by('-view_count')[:100]
        
        for product in popular_products:
            # ë¯¸ë¦¬ ìºì‹œì— ë¡œë“œ
            cache_key = cache_manager.generate_cache_key('product_detail', product.id)
            cache_manager.set_cached_data(
                cache_key, 
                product, 
                strategy='hot_data'
            )
        
        logger.info(f"Warmed up cache for {len(popular_products)} popular products")
    
    @staticmethod
    def warmup_categories():
        """ì¹´í…Œê³ ë¦¬ ìºì‹œ ì˜ˆì—´"""
        categories = Category.objects.all()
        
        for category in categories:
            # ì¹´í…Œê³ ë¦¬ë³„ ìƒí’ˆ ë¯¸ë¦¬ ë¡œë“œ
            cache_key = cache_manager.generate_cache_key('category_products', category.id)
            products = list(category.products.filter(status='active')[:20])
            cache_manager.set_cached_data(
                cache_key,
                products,
                strategy='warm_data'
            )
        
        logger.info(f"Warmed up cache for {len(categories)} categories")

# Celery íƒœìŠ¤í¬ë¡œ ì˜ˆì—´ ì‹¤í–‰
from celery import shared_task

@shared_task
def warmup_cache_periodic():
    """ì£¼ê¸°ì  ìºì‹œ ì˜ˆì—´"""
    CacheWarmupManager.warmup_popular_products()
    CacheWarmupManager.warmup_categories()
    return "Cache warmup completed"
```

## âš¡ ë¹„ë™ê¸° ì²˜ë¦¬ì™€ í ì‹œìŠ¤í…œ

ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ì—ì„œëŠ” ë¬´ê±°ìš´ ì‘ì—…ì„ ë°±ê·¸ë¼ìš´ë“œë¡œ ì²˜ë¦¬í•˜ì—¬ ì‘ë‹µ ì‹œê°„ì„ ìµœì†Œí™”í•´ì•¼ í•©ë‹ˆë‹¤. Celeryì™€ Redisë¥¼ í™œìš©í•œ ë¹„ë™ê¸° ì²˜ë¦¬ ì „ëµì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

### 1. Celery ìµœì í™” ì„¤ì •

```python
# celery_app.py
import os
from celery import Celery
from django.conf import settings

# Django ì„¤ì • ëª¨ë“ˆ ì§€ì •
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'myproject.settings.production')

app = Celery('myproject')

# Django ì„¤ì •ì—ì„œ Celery ì„¤ì • ë¡œë“œ
app.config_from_object('django.conf:settings', namespace='CELERY')

# ê³ ì„±ëŠ¥ ì„¤ì •
app.conf.update(
    # ë¸Œë¡œì»¤ ì„¤ì •
    broker_url='redis://redis-cluster:6379/0',
    result_backend='redis://redis-cluster:6379/0',
    
    # ì„±ëŠ¥ ìµœì í™”
    task_serializer='msgpack',
    result_serializer='msgpack',
    accept_content=['msgpack', 'json'],
    result_expires=3600,  # 1ì‹œê°„ í›„ ê²°ê³¼ ë§Œë£Œ
    
    # ì›Œì»¤ ìµœì í™”
    worker_prefetch_multiplier=4,  # ë™ì‹œ ì²˜ë¦¬ íƒœìŠ¤í¬ ìˆ˜
    worker_max_tasks_per_child=1000,  # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€
    worker_disable_rate_limits=True,  # ì†ë„ ì œí•œ ë¹„í™œì„±í™”
    
    # ë¼ìš°íŒ… ì„¤ì •
    task_routes={
        'myproject.tasks.send_email': {'queue': 'email'},
        'myproject.tasks.process_image': {'queue': 'media'},
        'myproject.tasks.generate_report': {'queue': 'reports'},
        'myproject.tasks.cleanup_data': {'queue': 'maintenance'},
    },
    
    # íë³„ ìš°ì„ ìˆœìœ„ ì„¤ì •
    task_default_queue='default',
    task_queue_max_priority=10,
    task_default_priority=5,
    
    # ëª¨ë‹ˆí„°ë§ ì„¤ì •
    worker_send_task_events=True,
    task_send_sent_event=True,
)

# ìë™ìœ¼ë¡œ íƒœìŠ¤í¬ ë°œê²¬
app.autodiscover_tasks()

# ì‹œì‘ ì‹œ ì„¤ì • ì¶œë ¥
@app.on_after_configure.connect
def setup_periodic_tasks(sender, **kwargs):
    """ì£¼ê¸°ì  íƒœìŠ¤í¬ ì„¤ì •"""
    
    # ìºì‹œ ì˜ˆì—´ (ë§¤ 10ë¶„)
    sender.add_periodic_task(
        600.0,
        warmup_cache_periodic.s(),
        name='cache_warmup'
    )
    
    # ì„±ëŠ¥ í†µê³„ ìˆ˜ì§‘ (ë§¤ 5ë¶„)
    sender.add_periodic_task(
        300.0,
        collect_performance_stats.s(),
        name='performance_stats'
    )
    
    # ë°ì´í„° ì •ë¦¬ (ë§¤ì¼ ìƒˆë²½ 2ì‹œ)
    sender.add_periodic_task(
        crontab(hour=2, minute=0),
        cleanup_old_data.s(),
        name='daily_cleanup'
    )
```

### 2. íƒœìŠ¤í¬ ìš°ì„ ìˆœìœ„ì™€ ë¶„ì‚° ì²˜ë¦¬

```python
# tasks.py
from celery import shared_task, group, chain, chord
from celery.exceptions import Retry
from django.core.mail import send_mail
from django.core.cache import cache
import logging
import time

logger = logging.getLogger(__name__)

# ìš°ì„ ìˆœìœ„ë³„ íƒœìŠ¤í¬ ì •ì˜
@shared_task(bind=True, priority=9, max_retries=3)  # ë†’ì€ ìš°ì„ ìˆœìœ„
def send_critical_notification(self, user_id, message):
    """ì¤‘ìš”í•œ ì•Œë¦¼ ë°œì†¡ (ì¦‰ì‹œ ì²˜ë¦¬)"""
    try:
        user = User.objects.get(id=user_id)
        
        # ì´ë©”ì¼ ë°œì†¡
        send_mail(
            subject='ì¤‘ìš” ì•Œë¦¼',
            message=message,
            from_email=settings.DEFAULT_FROM_EMAIL,
            recipient_list=[user.email],
            fail_silently=False
        )
        
        # SMS ë°œì†¡ (ì™¸ë¶€ API)
        send_sms_notification(user.phone, message)
        
        logger.info(f"Critical notification sent to user {user_id}")
        return f"Notification sent to {user.email}"
        
    except Exception as exc:
        logger.error(f"Failed to send critical notification: {exc}")
        
        # ì¬ì‹œë„ ì „ëµ
        if self.request.retries < self.max_retries:
            # ì§€ìˆ˜ ë°±ì˜¤í”„
            countdown = 2 ** self.request.retries
            raise self.retry(countdown=countdown, exc=exc)
        else:
            # ìµœì¢… ì‹¤íŒ¨ ì²˜ë¦¬
            handle_notification_failure(user_id, message, str(exc))
            raise exc

@shared_task(priority=5)  # ì¼ë°˜ ìš°ì„ ìˆœìœ„
def process_user_action(user_id, action_type, action_data):
    """ì‚¬ìš©ì ì•¡ì…˜ ì²˜ë¦¬"""
    try:
        user = User.objects.get(id=user_id)
        
        if action_type == 'purchase':
            process_purchase(user, action_data)
        elif action_type == 'review':
            process_review(user, action_data)
        elif action_type == 'wishlist':
            process_wishlist(user, action_data)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        update_user_statistics.delay(user_id)
        
        logger.info(f"Processed {action_type} for user {user_id}")
        
    except Exception as exc:
        logger.error(f"Failed to process user action: {exc}")
        raise exc

@shared_task(priority=2)  # ë‚®ì€ ìš°ì„ ìˆœìœ„
def generate_analytics_report(report_type, date_range):
    """ë¶„ì„ ë³´ê³ ì„œ ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)"""
    try:
        start_time = time.time()
        
        if report_type == 'sales':
            data = generate_sales_report(date_range)
        elif report_type == 'user_behavior':
            data = generate_user_behavior_report(date_range)
        elif report_type == 'performance':
            data = generate_performance_report(date_range)
        
        # ë³´ê³ ì„œ ì €ì¥
        report = Report.objects.create(
            type=report_type,
            data=data,
            generation_time=time.time() - start_time
        )
        
        # ê´€ë ¨ìë“¤ì—ê²Œ ì•Œë¦¼
        notify_report_completion.delay(report.id)
        
        logger.info(f"Generated {report_type} report in {report.generation_time:.2f}s")
        return report.id
        
    except Exception as exc:
        logger.error(f"Failed to generate report: {exc}")
        raise exc

# ë³‘ë ¬ ì²˜ë¦¬ íŒ¨í„´
@shared_task
def process_bulk_data(data_chunks):
    """ëŒ€ëŸ‰ ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬"""
    
    # ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
    job = group(
        process_data_chunk.s(chunk) for chunk in data_chunks
    )
    
    result = job.apply_async()
    
    # ëª¨ë“  ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ ëŒ€ê¸°
    processed_data = result.get()
    
    # ê²°ê³¼ ë³‘í•©
    merged_result = merge_processed_data(processed_data)
    
    return merged_result

@shared_task
def process_data_chunk(chunk):
    """ë°ì´í„° ì²­í¬ ì²˜ë¦¬"""
    processed_items = []
    
    for item in chunk:
        try:
            processed_item = complex_data_processing(item)
            processed_items.append(processed_item)
        except Exception as e:
            logger.error(f"Failed to process item {item}: {e}")
    
    return processed_items

# íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ íŒ¨í„´
@shared_task
def image_processing_pipeline(image_id):
    """ì´ë¯¸ì§€ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    # ì²´ì¸ ë°©ì‹ìœ¼ë¡œ ìˆœì°¨ ì²˜ë¦¬
    pipeline = chain(
        validate_image.s(image_id),
        resize_image.s(),
        apply_watermark.s(),
        generate_thumbnails.s(),
        upload_to_cdn.s(),
        update_database.s()
    )
    
    return pipeline.apply_async()

@shared_task
def validate_image(image_id):
    """ì´ë¯¸ì§€ ê²€ì¦"""
    image = Image.objects.get(id=image_id)
    
    if not is_valid_image(image.file):
        raise ValueError("Invalid image format")
    
    return image_id

@shared_task
def resize_image(image_id):
    """ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ"""
    image = Image.objects.get(id=image_id)
    
    resized_path = resize_image_file(image.file.path)
    image.resized_file = resized_path
    image.save()
    
    return image_id

# ì¡°ê±´ë¶€ ì²˜ë¦¬ íŒ¨í„´
@shared_task
def smart_notification_dispatch(notification_data):
    """ìŠ¤ë§ˆíŠ¸ ì•Œë¦¼ ë°œì†¡"""
    
    user_id = notification_data['user_id']
    message = notification_data['message']
    urgency = notification_data.get('urgency', 'normal')
    
    # ì‚¬ìš©ì ì„ í˜¸ë„ í™•ì¸
    user_preferences = get_user_notification_preferences(user_id)
    
    # ë°œì†¡ ë°©ë²• ê²°ì •
    notification_methods = []
    
    if urgency == 'critical':
        notification_methods = ['email', 'sms', 'push']
    elif urgency == 'high':
        notification_methods = ['email', 'push']
    else:
        notification_methods = ['email']
    
    # ì„ í˜¸ë„ì— ë”°ë¼ í•„í„°ë§
    filtered_methods = [
        method for method in notification_methods
        if user_preferences.get(method, True)
    ]
    
    # ë³‘ë ¬ë¡œ ë°œì†¡
    job = group(
        send_notification_by_method.s(user_id, message, method)
        for method in filtered_methods
    )
    
    return job.apply_async()

@shared_task
def send_notification_by_method(user_id, message, method):
    """íŠ¹ì • ë°©ë²•ìœ¼ë¡œ ì•Œë¦¼ ë°œì†¡"""
    try:
        if method == 'email':
            return send_email_notification(user_id, message)
        elif method == 'sms':
            return send_sms_notification(user_id, message)
        elif method == 'push':
            return send_push_notification(user_id, message)
    except Exception as e:
        logger.error(f"Failed to send {method} notification: {e}")
        raise e
```

### 3. í ëª¨ë‹ˆí„°ë§ê³¼ ê´€ë¦¬

```python
# monitoring.py
from celery import current_app
from django.core.management.base import BaseCommand
import redis
import json
import time

class CeleryMonitor:
    """Celery ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.app = current_app
        self.redis_client = redis.Redis.from_url(settings.CELERY_BROKER_URL)
    
    def get_queue_lengths(self):
        """íë³„ ëŒ€ê¸° ì¤‘ì¸ íƒœìŠ¤í¬ ìˆ˜"""
        queues = ['default', 'email', 'media', 'reports', 'maintenance']
        queue_lengths = {}
        
        for queue in queues:
            length = self.redis_client.llen(f"celery_{queue}")
            queue_lengths[queue] = length
        
        return queue_lengths
    
    def get_active_workers(self):
        """í™œì„± ì›Œì»¤ ì •ë³´"""
        inspect = self.app.control.inspect()
        
        try:
            active = inspect.active() or {}
            stats = inspect.stats() or {}
            
            worker_info = {}
            for worker_name, worker_stats in stats.items():
                worker_info[worker_name] = {
                    'active_tasks': len(active.get(worker_name, [])),
                    'total_tasks': worker_stats.get('total', {}),
                    'pool_processes': worker_stats.get('pool', {}).get('processes'),
                    'rusage': worker_stats.get('rusage', {})
                }
            
            return worker_info
            
        except Exception as e:
            logger.error(f"Failed to get worker info: {e}")
            return {}
    
    def get_failed_tasks(self, limit=10):
        """ì‹¤íŒ¨í•œ íƒœìŠ¤í¬ ëª©ë¡"""
        failed_tasks = []
        
        # Redisì—ì„œ ì‹¤íŒ¨í•œ íƒœìŠ¤í¬ ì¡°íšŒ
        failed_keys = self.redis_client.keys("celery-task-meta-*")
        
        for key in failed_keys[:limit]:
            try:
                task_data = self.redis_client.get(key)
                if task_data:
                    task_info = json.loads(task_data)
                    if task_info.get('status') == 'FAILURE':
                        failed_tasks.append({
                            'task_id': key.decode().split('-')[-1],
                            'result': task_info.get('result'),
                            'traceback': task_info.get('traceback')
                        })
            except Exception as e:
                continue
        
        return failed_tasks
    
    def get_performance_metrics(self):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
        queue_lengths = self.get_queue_lengths()
        worker_info = self.get_active_workers()
        failed_tasks = self.get_failed_tasks()
        
        # ì´ ëŒ€ê¸° íƒœìŠ¤í¬ ìˆ˜
        total_pending = sum(queue_lengths.values())
        
        # ì´ í™œì„± ì›Œì»¤ ìˆ˜
        total_workers = len(worker_info)
        
        # ì´ í™œì„± íƒœìŠ¤í¬ ìˆ˜
        total_active_tasks = sum(
            info['active_tasks'] for info in worker_info.values()
        )
        
        return {
            'queue_lengths': queue_lengths,
            'total_pending_tasks': total_pending,
            'total_workers': total_workers,
            'total_active_tasks': total_active_tasks,
            'failed_tasks_count': len(failed_tasks),
            'worker_details': worker_info,
            'timestamp': time.time()
        }

# ìë™ ìŠ¤ì¼€ì¼ë§
class CeleryAutoScaler:
    """Celery ì›Œì»¤ ìë™ ìŠ¤ì¼€ì¼ë§"""
    
    def __init__(self):
        self.monitor = CeleryMonitor()
        self.scaling_rules = {
            'scale_up_threshold': 100,    # ëŒ€ê¸° íƒœìŠ¤í¬ 100ê°œ ì´ìƒ ì‹œ ìŠ¤ì¼€ì¼ ì—…
            'scale_down_threshold': 10,   # ëŒ€ê¸° íƒœìŠ¤í¬ 10ê°œ ì´í•˜ ì‹œ ìŠ¤ì¼€ì¼ ë‹¤ìš´
            'max_workers': 20,            # ìµœëŒ€ ì›Œì»¤ ìˆ˜
            'min_workers': 3,             # ìµœì†Œ ì›Œì»¤ ìˆ˜
            'scale_up_count': 2,          # í•œ ë²ˆì— ì¶”ê°€í•  ì›Œì»¤ ìˆ˜
            'scale_down_count': 1         # í•œ ë²ˆì— ì œê±°í•  ì›Œì»¤ ìˆ˜
        }
    
    def should_scale_up(self, metrics):
        """ìŠ¤ì¼€ì¼ ì—… í•„ìš” ì—¬ë¶€ íŒë‹¨"""
        return (
            metrics['total_pending_tasks'] > self.scaling_rules['scale_up_threshold'] and
            metrics['total_workers'] < self.scaling_rules['max_workers']
        )
    
    def should_scale_down(self, metrics):
        """ìŠ¤ì¼€ì¼ ë‹¤ìš´ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
        return (
            metrics['total_pending_tasks'] < self.scaling_rules['scale_down_threshold'] and
            metrics['total_workers'] > self.scaling_rules['min_workers']
        )
    
    def execute_scaling(self):
        """ìŠ¤ì¼€ì¼ë§ ì‹¤í–‰"""
        metrics = self.monitor.get_performance_metrics()
        
        if self.should_scale_up(metrics):
            self.scale_up()
        elif self.should_scale_down(metrics):
            self.scale_down()
    
    def scale_up(self):
        """ì›Œì»¤ ìˆ˜ ì¦ê°€"""
        # Kubernetes, Docker Swarm ë“±ê³¼ ì—°ë™
        logger.info("Scaling up Celery workers")
        # kubectl scale deployment celery-worker --replicas=+2
    
    def scale_down(self):
        """ì›Œì»¤ ìˆ˜ ê°ì†Œ"""
        logger.info("Scaling down Celery workers")
        # kubectl scale deployment celery-worker --replicas=-1

# ê´€ë¦¬ ëª…ë ¹ì–´
class Command(BaseCommand):
    """Celery ëª¨ë‹ˆí„°ë§ ëª…ë ¹ì–´"""
    
    def add_arguments(self, parser):
        parser.add_argument('--action', choices=['status', 'autoscale'], default='status')
        parser.add_argument('--interval', type=int, default=30)
    
    def handle(self, *args, **options):
        monitor = CeleryMonitor()
        autoscaler = CeleryAutoScaler()
        
        if options['action'] == 'status':
            self.show_status(monitor)
        elif options['action'] == 'autoscale':
            self.run_autoscaler(autoscaler, options['interval'])
    
    def show_status(self, monitor):
        """ìƒíƒœ í‘œì‹œ"""
        metrics = monitor.get_performance_metrics()
        
        self.stdout.write("=== Celery Status ===")
        self.stdout.write(f"Total pending tasks: {metrics['total_pending_tasks']}")
        self.stdout.write(f"Active workers: {metrics['total_workers']}")
        self.stdout.write(f"Active tasks: {metrics['total_active_tasks']}")
        self.stdout.write(f"Failed tasks: {metrics['failed_tasks_count']}")
        
        self.stdout.write("\n=== Queue Lengths ===")
        for queue, length in metrics['queue_lengths'].items():
            self.stdout.write(f"{queue}: {length}")
    
    def run_autoscaler(self, autoscaler, interval):
        """ìë™ ìŠ¤ì¼€ì¼ëŸ¬ ì‹¤í–‰"""
        self.stdout.write(f"Starting autoscaler with {interval}s interval")
        
        while True:
            try:
                autoscaler.execute_scaling()
                time.sleep(interval)
            except KeyboardInterrupt:
                self.stdout.write("Autoscaler stopped")
                break
            except Exception as e:
                self.stderr.write(f"Autoscaler error: {e}")
                time.sleep(interval)
```

---

## 5. ëª¨ë‹ˆí„°ë§ ë° ì„±ëŠ¥ ì¸¡ì •

ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ì„ ì²˜ë¦¬í•˜ëŠ” Django ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œëŠ” ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ê³¼ ì„±ëŠ¥ ì¸¡ì •ì´ í•„ìˆ˜ì…ë‹ˆë‹¤. ë¬¸ì œê°€ ë°œìƒí•˜ê¸° ì „ì— ë¯¸ë¦¬ ê°ì§€í•˜ê³  ëŒ€ì‘í•  ìˆ˜ ìˆëŠ” ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•´ì•¼ í•©ë‹ˆë‹¤.

### 1. Django APM(Application Performance Monitoring)

#### Django Debug Toolbarì™€ Silkë¥¼ í™œìš©í•œ í”„ë¡œíŒŒì¼ë§

```python
# settings/development.py
INSTALLED_APPS = [
    # ... ë‹¤ë¥¸ ì•±ë“¤
    'debug_toolbar',
    'silk',
]

MIDDLEWARE = [
    'debug_toolbar.middleware.DebugToolbarMiddleware',
    'silk.middleware.SilkyMiddleware',
    # ... ë‹¤ë¥¸ ë¯¸ë“¤ì›¨ì–´ë“¤
]

# Debug Toolbar ì„¤ì •
INTERNAL_IPS = [
    '127.0.0.1',
    'localhost',
]

# Silk ì„¤ì •
SILKY_PYTHON_PROFILER = True
SILKY_PYTHON_PROFILER_BINARY = True
SILKY_AUTHENTICATION = True
SILKY_AUTHORISATION = True
SILKY_META = True
```

#### ì»¤ìŠ¤í…€ ì„±ëŠ¥ ë¯¸ë“¤ì›¨ì–´

```python
# middleware/performance.py
import time
import logging
from django.utils.deprecation import MiddlewareMixin
from django.db import connection
from django.core.cache import cache

logger = logging.getLogger('performance')

class PerformanceMonitoringMiddleware(MiddlewareMixin):
    """ìš”ì²­ë³„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë¯¸ë“¤ì›¨ì–´"""
    
    def process_request(self, request):
        request._start_time = time.time()
        request._db_queries_start = len(connection.queries)
        
    def process_response(self, request, response):
        # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        if hasattr(request, '_start_time'):
            duration = time.time() - request._start_time
            
            # DB ì¿¼ë¦¬ ìˆ˜ ê³„ì‚°
            db_queries_count = len(connection.queries) - getattr(request, '_db_queries_start', 0)
            
            # ì‘ë‹µ í¬ê¸° ê³„ì‚°
            content_length = len(response.content) if hasattr(response, 'content') else 0
            
            # ë¡œê¹…
            log_data = {
                'method': request.method,
                'path': request.path,
                'duration': duration * 1000,  # ms ë‹¨ìœ„
                'status_code': response.status_code,
                'db_queries': db_queries_count,
                'content_length': content_length,
                'user_agent': request.META.get('HTTP_USER_AGENT', ''),
                'ip': self.get_client_ip(request),
            }
            
            # ì„±ëŠ¥ ì„ê³„ê°’ ê²€ì‚¬
            if duration > 1.0:  # 1ì´ˆ ì´ìƒ
                logger.warning(f"Slow request: {log_data}")
            elif db_queries_count > 10:  # 10ê°œ ì´ìƒ ì¿¼ë¦¬
                logger.warning(f"N+1 query detected: {log_data}")
            else:
                logger.info(f"Request processed: {log_data}")
            
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            self.collect_metrics(log_data)
            
        return response
    
    def get_client_ip(self, request):
        """í´ë¼ì´ì–¸íŠ¸ IP ì£¼ì†Œ ì¶”ì¶œ"""
        x_forwarded_for = request.META.get('HTTP_X_FORWARDED_FOR')
        if x_forwarded_for:
            ip = x_forwarded_for.split(',')[0]
        else:
            ip = request.META.get('REMOTE_ADDR')
        return ip
    
    def collect_metrics(self, log_data):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì €ì¥"""
        # Redisë¥¼ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        cache_key = f"metrics:{int(time.time() // 60)}"  # ë¶„ ë‹¨ìœ„
        
        try:
            current_metrics = cache.get(cache_key, {})
            current_metrics.setdefault('request_count', 0)
            current_metrics.setdefault('total_duration', 0)
            current_metrics.setdefault('slow_requests', 0)
            current_metrics.setdefault('status_codes', {})
            
            current_metrics['request_count'] += 1
            current_metrics['total_duration'] += log_data['duration']
            
            if log_data['duration'] > 1000:  # 1ì´ˆ ì´ìƒ
                current_metrics['slow_requests'] += 1
            
            status_code = str(log_data['status_code'])
            current_metrics['status_codes'].setdefault(status_code, 0)
            current_metrics['status_codes'][status_code] += 1
            
            cache.set(cache_key, current_metrics, 300)  # 5ë¶„ ìºì‹œ
            
        except Exception as e:
            logger.error(f"Failed to collect metrics: {e}")
```

### 2. ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë‹ˆí„°ë§

#### PostgreSQL ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
# monitoring/database.py
import psycopg2
from django.db import connection
from django.conf import settings
import logging

logger = logging.getLogger('db_monitor')

class DatabaseMonitor:
    """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.connection = connection
    
    def get_slow_queries(self, threshold_ms=1000):
        """ëŠë¦° ì¿¼ë¦¬ ì¡°íšŒ"""
        with self.connection.cursor() as cursor:
            cursor.execute("""
                SELECT 
                    query,
                    calls,
                    total_time,
                    mean_time,
                    rows,
                    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
                FROM pg_stat_statements 
                WHERE mean_time > %s
                ORDER BY total_time DESC
                LIMIT 20;
            """, [threshold_ms])
            
            columns = [desc[0] for desc in cursor.description]
            results = [dict(zip(columns, row)) for row in cursor.fetchall()]
            
            for query in results:
                logger.warning(f"Slow query detected: {query}")
            
            return results
    
    def get_connection_stats(self):
        """ì—°ê²° í†µê³„ ì¡°íšŒ"""
        with self.connection.cursor() as cursor:
            cursor.execute("""
                SELECT 
                    datname,
                    numbackends,
                    xact_commit,
                    xact_rollback,
                    blks_read,
                    blks_hit,
                    temp_files,
                    temp_bytes,
                    deadlocks
                FROM pg_stat_database 
                WHERE datname = current_database();
            """)
            
            columns = [desc[0] for desc in cursor.description]
            return dict(zip(columns, cursor.fetchone()))
    
    def get_lock_info(self):
        """ì ê¸ˆ ì •ë³´ ì¡°íšŒ"""
        with self.connection.cursor() as cursor:
            cursor.execute("""
                SELECT 
                    bl.pid AS blocked_pid,
                    bl.usename AS blocked_user,
                    bl.query AS blocked_query,
                    kl.pid AS blocking_pid,
                    kl.usename AS blocking_user,
                    kl.query AS blocking_query
                FROM pg_catalog.pg_locks bl
                JOIN pg_catalog.pg_stat_activity bl_act ON bl.pid = bl_act.pid
                JOIN pg_catalog.pg_locks kl ON bl.transactionid = kl.transactionid
                JOIN pg_catalog.pg_stat_activity kl_act ON kl.pid = kl_act.pid
                WHERE bl.granted = false AND kl.granted = true
                AND bl.pid != kl.pid;
            """)
            
            columns = [desc[0] for desc in cursor.description]
            return [dict(zip(columns, row)) for row in cursor.fetchall()]
    
    def analyze_query_performance(self):
        """ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„"""
        stats = {
            'slow_queries': self.get_slow_queries(),
            'connection_stats': self.get_connection_stats(),
            'locks': self.get_lock_info(),
        }
        
        # ì„±ëŠ¥ ì„ê³„ê°’ ê²€ì‚¬
        conn_stats = stats['connection_stats']
        if conn_stats['numbackends'] > 100:
            logger.warning(f"High connection count: {conn_stats['numbackends']}")
        
        if conn_stats['deadlocks'] > 0:
            logger.error(f"Deadlocks detected: {conn_stats['deadlocks']}")
        
        return stats
```

### 3. Redis ëª¨ë‹ˆí„°ë§

```python
# monitoring/redis_monitor.py
import redis
import json
import time
from django.core.cache import cache
from django.conf import settings

class RedisMonitor:
    """Redis ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.redis_client = redis.Redis.from_url(settings.CACHES['default']['LOCATION'])
    
    def get_info(self):
        """Redis ì •ë³´ ì¡°íšŒ"""
        info = self.redis_client.info()
        
        memory_usage = {
            'used_memory': info['used_memory'],
            'used_memory_human': info['used_memory_human'],
            'used_memory_peak': info['used_memory_peak'],
            'used_memory_peak_human': info['used_memory_peak_human'],
            'memory_fragmentation_ratio': info.get('mem_fragmentation_ratio', 0),
        }
        
        performance = {
            'connected_clients': info['connected_clients'],
            'total_commands_processed': info['total_commands_processed'],
            'instantaneous_ops_per_sec': info['instantaneous_ops_per_sec'],
            'keyspace_hits': info['keyspace_hits'],
            'keyspace_misses': info['keyspace_misses'],
            'expired_keys': info['expired_keys'],
            'evicted_keys': info['evicted_keys'],
        }
        
        # íˆíŠ¸ìœ¨ ê³„ì‚°
        if (info['keyspace_hits'] + info['keyspace_misses']) > 0:
            hit_rate = info['keyspace_hits'] / (info['keyspace_hits'] + info['keyspace_misses'])
            performance['hit_rate'] = hit_rate
        
        return {
            'memory': memory_usage,
            'performance': performance,
            'uptime': info['uptime_in_seconds'],
        }
    
    def get_slow_log(self, count=10):
        """ëŠë¦° ëª…ë ¹ ë¡œê·¸ ì¡°íšŒ"""
        slow_commands = self.redis_client.slowlog_get(count)
        
        formatted_commands = []
        for cmd in slow_commands:
            formatted_commands.append({
                'id': cmd['id'],
                'start_time': cmd['start_time'],
                'duration': cmd['duration'],  # ë§ˆì´í¬ë¡œì´ˆ
                'command': ' '.join([arg.decode() if isinstance(arg, bytes) else str(arg) 
                                   for arg in cmd['command']]),
            })
        
        return formatted_commands
    
    def monitor_performance(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼"""
        info = self.get_info()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
        memory_usage_percent = (info['memory']['used_memory'] / 
                              (1024 * 1024 * 1024))  # GB ë‹¨ìœ„
        
        if memory_usage_percent > 0.8:  # 80% ì´ìƒ
            logging.warning(f"High Redis memory usage: {memory_usage_percent:.2f}GB")
        
        # íˆíŠ¸ìœ¨ ì²´í¬
        hit_rate = info['performance'].get('hit_rate', 0)
        if hit_rate < 0.8:  # 80% ë¯¸ë§Œ
            logging.warning(f"Low Redis hit rate: {hit_rate:.2%}")
        
        # ì—°ê²° ìˆ˜ ì²´í¬
        connected_clients = info['performance']['connected_clients']
        if connected_clients > 1000:
            logging.warning(f"High Redis connection count: {connected_clients}")
        
        return info
```

### 4. ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```python
# monitoring/system_metrics.py
import psutil
import platform
import json
from datetime import datetime
import logging

logger = logging.getLogger('system_metrics')

class SystemMetricsCollector:
    """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°"""
    
    def collect_cpu_metrics(self):
        """CPU ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        cpu_percent = psutil.cpu_percent(interval=1, percpu=True)
        cpu_count = psutil.cpu_count()
        load_avg = psutil.getloadavg() if hasattr(psutil, 'getloadavg') else (0, 0, 0)
        
        return {
            'cpu_percent_total': psutil.cpu_percent(),
            'cpu_percent_per_core': cpu_percent,
            'cpu_count': cpu_count,
            'load_average': {
                '1min': load_avg[0],
                '5min': load_avg[1],
                '15min': load_avg[2],
            }
        }
    
    def collect_memory_metrics(self):
        """ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        memory = psutil.virtual_memory()
        swap = psutil.swap_memory()
        
        return {
            'memory': {
                'total': memory.total,
                'available': memory.available,
                'used': memory.used,
                'percent': memory.percent,
                'free': memory.free,
                'cached': getattr(memory, 'cached', 0),
                'buffers': getattr(memory, 'buffers', 0),
            },
            'swap': {
                'total': swap.total,
                'used': swap.used,
                'free': swap.free,
                'percent': swap.percent,
            }
        }
    
    def collect_disk_metrics(self):
        """ë””ìŠ¤í¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        disk_usage = psutil.disk_usage('/')
        disk_io = psutil.disk_io_counters()
        
        partitions = []
        for partition in psutil.disk_partitions():
            try:
                usage = psutil.disk_usage(partition.mountpoint)
                partitions.append({
                    'device': partition.device,
                    'mountpoint': partition.mountpoint,
                    'fstype': partition.fstype,
                    'total': usage.total,
                    'used': usage.used,
                    'free': usage.free,
                    'percent': (usage.used / usage.total) * 100,
                })
            except PermissionError:
                continue
        
        return {
            'root_disk': {
                'total': disk_usage.total,
                'used': disk_usage.used,
                'free': disk_usage.free,
                'percent': (disk_usage.used / disk_usage.total) * 100,
            },
            'partitions': partitions,
            'io_counters': {
                'read_bytes': disk_io.read_bytes if disk_io else 0,
                'write_bytes': disk_io.write_bytes if disk_io else 0,
                'read_count': disk_io.read_count if disk_io else 0,
                'write_count': disk_io.write_count if disk_io else 0,
            } if disk_io else {}
        }
    
    def collect_network_metrics(self):
        """ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        net_io = psutil.net_io_counters()
        connections = len(psutil.net_connections())
        
        return {
            'io_counters': {
                'bytes_sent': net_io.bytes_sent,
                'bytes_recv': net_io.bytes_recv,
                'packets_sent': net_io.packets_sent,
                'packets_recv': net_io.packets_recv,
                'errin': net_io.errin,
                'errout': net_io.errout,
                'dropin': net_io.dropin,
                'dropout': net_io.dropout,
            },
            'connections_count': connections,
        }
    
    def collect_process_metrics(self):
        """í”„ë¡œì„¸ìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        processes = []
        
        for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent']):
            try:
                proc_info = proc.info
                if proc_info['name'] in ['python', 'gunicorn', 'uwsgi', 'celery']:
                    processes.append({
                        'pid': proc_info['pid'],
                        'name': proc_info['name'],
                        'cpu_percent': proc_info['cpu_percent'],
                        'memory_percent': proc_info['memory_percent'],
                    })
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
        
        return processes
    
    def collect_all_metrics(self):
        """ëª¨ë“  ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'hostname': platform.node(),
            'cpu': self.collect_cpu_metrics(),
            'memory': self.collect_memory_metrics(),
            'disk': self.collect_disk_metrics(),
            'network': self.collect_network_metrics(),
            'processes': self.collect_process_metrics(),
        }
        
        # ì„ê³„ê°’ ê²€ì‚¬ ë° ì•Œë¦¼
        self.check_thresholds(metrics)
        
        return metrics
    
    def check_thresholds(self, metrics):
        """ì„ê³„ê°’ ê²€ì‚¬"""
        # CPU ì‚¬ìš©ë¥  ê²€ì‚¬
        if metrics['cpu']['cpu_percent_total'] > 80:
            logger.warning(f"High CPU usage: {metrics['cpu']['cpu_percent_total']:.1f}%")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê²€ì‚¬
        if metrics['memory']['memory']['percent'] > 80:
            logger.warning(f"High memory usage: {metrics['memory']['memory']['percent']:.1f}%")
        
        # ë””ìŠ¤í¬ ì‚¬ìš©ë¥  ê²€ì‚¬
        if metrics['disk']['root_disk']['percent'] > 80:
            logger.warning(f"High disk usage: {metrics['disk']['root_disk']['percent']:.1f}%")
        
        # ë¡œë“œ ì• ë²„ë¦¬ì§€ ê²€ì‚¬
        cpu_count = metrics['cpu']['cpu_count']
        load_1min = metrics['cpu']['load_average']['1min']
        if load_1min > cpu_count * 0.8:
            logger.warning(f"High load average: {load_1min:.2f} (CPU count: {cpu_count})")
```

### 5. ì•Œë¦¼ ì‹œìŠ¤í…œ

```python
# monitoring/alerts.py
import smtplib
import json
import requests
from email.mime.text import MimeText
from email.mime.multipart import MimeMultipart
from django.conf import settings
import logging

logger = logging.getLogger('alerts')

class AlertManager:
    """ì•Œë¦¼ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.email_config = getattr(settings, 'ALERT_EMAIL_CONFIG', {})
        self.slack_webhook = getattr(settings, 'SLACK_WEBHOOK_URL', None)
        self.discord_webhook = getattr(settings, 'DISCORD_WEBHOOK_URL', None)
    
    def send_email_alert(self, subject, message, recipients=None):
        """ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡"""
        if not self.email_config or not recipients:
            return False
        
        try:
            msg = MimeMultipart()
            msg['From'] = self.email_config['from_email']
            msg['To'] = ', '.join(recipients)
            msg['Subject'] = f"[Django Alert] {subject}"
            
            msg.attach(MimeText(message, 'plain'))
            
            server = smtplib.SMTP(self.email_config['smtp_host'], 
                                 self.email_config['smtp_port'])
            server.starttls()
            server.login(self.email_config['username'], 
                        self.email_config['password'])
            
            text = msg.as_string()
            server.sendmail(self.email_config['from_email'], recipients, text)
            server.quit()
            
            logger.info(f"Email alert sent: {subject}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to send email alert: {e}")
            return False
    
    def send_slack_alert(self, message, channel=None):
        """Slack ì•Œë¦¼ ë°œì†¡"""
        if not self.slack_webhook:
            return False
        
        try:
            payload = {
                'text': f"ğŸš¨ Django Alert: {message}",
                'username': 'Django Monitor',
                'icon_emoji': ':warning:',
            }
            
            if channel:
                payload['channel'] = channel
            
            response = requests.post(self.slack_webhook, 
                                   data=json.dumps(payload),
                                   headers={'Content-Type': 'application/json'})
            
            if response.status_code == 200:
                logger.info(f"Slack alert sent: {message}")
                return True
            else:
                logger.error(f"Failed to send Slack alert: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"Failed to send Slack alert: {e}")
            return False
    
    def send_discord_alert(self, message):
        """Discord ì•Œë¦¼ ë°œì†¡"""
        if not self.discord_webhook:
            return False
        
        try:
            payload = {
                'content': f"ğŸš¨ **Django Alert**\n{message}",
                'username': 'Django Monitor',
            }
            
            response = requests.post(self.discord_webhook, 
                                   data=json.dumps(payload),
                                   headers={'Content-Type': 'application/json'})
            
            if response.status_code == 204:
                logger.info(f"Discord alert sent: {message}")
                return True
            else:
                logger.error(f"Failed to send Discord alert: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"Failed to send Discord alert: {e}")
            return False
    
    def send_alert(self, level, subject, message, channels=None):
        """í†µí•© ì•Œë¦¼ ë°œì†¡"""
        if not channels:
            channels = ['email', 'slack']
        
        success_count = 0
        
        if 'email' in channels and level in ['critical', 'error']:
            recipients = getattr(settings, 'ALERT_EMAIL_RECIPIENTS', [])
            if self.send_email_alert(subject, message, recipients):
                success_count += 1
        
        if 'slack' in channels:
            if self.send_slack_alert(f"{subject}\n{message}"):
                success_count += 1
        
        if 'discord' in channels:
            if self.send_discord_alert(f"**{subject}**\n{message}"):
                success_count += 1
        
        return success_count > 0

# monitoring/tasks.py (Celery íƒœìŠ¤í¬)
from celery import shared_task
from .database import DatabaseMonitor
from .redis_monitor import RedisMonitor
from .system_metrics import SystemMetricsCollector
from .alerts import AlertManager

@shared_task
def collect_system_metrics():
    """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ íƒœìŠ¤í¬"""
    collector = SystemMetricsCollector()
    metrics = collector.collect_all_metrics()
    
    # ë©”íŠ¸ë¦­ì„ Redisì— ì €ì¥
    from django.core.cache import cache
    cache_key = f"system_metrics:{int(time.time() // 60)}"
    cache.set(cache_key, metrics, 300)  # 5ë¶„ ìºì‹œ
    
    return metrics

@shared_task
def monitor_database_performance():
    """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬"""
    monitor = DatabaseMonitor()
    stats = monitor.analyze_query_performance()
    
    alert_manager = AlertManager()
    
    # ëŠë¦° ì¿¼ë¦¬ ì•Œë¦¼
    if stats['slow_queries']:
        message = f"Detected {len(stats['slow_queries'])} slow queries"
        alert_manager.send_alert('warning', 'Slow Queries Detected', message)
    
    # ë°ë“œë½ ì•Œë¦¼
    if stats['locks']:
        message = f"Detected {len(stats['locks'])} database locks"
        alert_manager.send_alert('critical', 'Database Locks Detected', message)
    
    return stats

@shared_task
def monitor_redis_performance():
    """Redis ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬"""
    monitor = RedisMonitor()
    info = monitor.monitor_performance()
    
    return info
```

### 6. ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

```python
# views/monitoring.py
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.contrib.admin.views.decorators import staff_member_required
from django.core.cache import cache
import json

@staff_member_required
def monitoring_dashboard(request):
    """ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ API"""
    if request.method == 'GET':
        return render(request, 'monitoring/dashboard.html')
    
    # AJAX ìš”ì²­ ì²˜ë¦¬
    metric_type = request.GET.get('type', 'all')
    
    if metric_type == 'system':
        return JsonResponse(get_system_metrics())
    elif metric_type == 'database':
        return JsonResponse(get_database_metrics())
    elif metric_type == 'redis':
        return JsonResponse(get_redis_metrics())
    elif metric_type == 'application':
        return JsonResponse(get_application_metrics())
    else:
        return JsonResponse({
            'system': get_system_metrics(),
            'database': get_database_metrics(),
            'redis': get_redis_metrics(),
            'application': get_application_metrics(),
        })

def get_system_metrics():
    """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    collector = SystemMetricsCollector()
    return collector.collect_all_metrics()

def get_database_metrics():
    """ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    monitor = DatabaseMonitor()
    return monitor.analyze_query_performance()

def get_redis_metrics():
    """Redis ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    monitor = RedisMonitor()
    return monitor.get_info()

def get_application_metrics():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    # ìµœê·¼ 10ë¶„ê°„ì˜ ë©”íŠ¸ë¦­ ì¡°íšŒ
    current_time = int(time.time() // 60)
    metrics = []
    
    for i in range(10):
        cache_key = f"metrics:{current_time - i}"
        metric = cache.get(cache_key)
        if metric:
            metric['timestamp'] = current_time - i
            metrics.append(metric)
    
    return {
        'recent_metrics': metrics,
        'total_requests': sum(m.get('request_count', 0) for m in metrics),
        'avg_response_time': sum(m.get('total_duration', 0) for m in metrics) / max(sum(m.get('request_count', 0) for m in metrics), 1),
        'slow_requests': sum(m.get('slow_requests', 0) for m in metrics),
    }
```

### 7. ì„¤ì • ì˜ˆì‹œ

```python
# settings/monitoring.py

LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'verbose': {
            'format': '{levelname} {asctime} {module} {process:d} {thread:d} {message}',
            'style': '{',
        },
        'json': {
            'format': '{"level": "%(levelname)s", "time": "%(asctime)s", "module": "%(module)s", "message": "%(message)s"}',
        },
    },
    'handlers': {
        'file': {
            'level': 'INFO',
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': '/var/log/django/django.log',
            'maxBytes': 50 * 1024 * 1024,  # 50MB
            'backupCount': 5,
            'formatter': 'verbose',
        },
        'performance': {
            'level': 'INFO',
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': '/var/log/django/performance.log',
            'maxBytes': 100 * 1024 * 1024,  # 100MB
            'backupCount': 10,
            'formatter': 'json',
        },
    },
    'loggers': {
        'django': {
            'handlers': ['file'],
            'level': 'INFO',
            'propagate': True,
        },
        'performance': {
            'handlers': ['performance'],
            'level': 'INFO',
            'propagate': False,
        },
        'db_monitor': {
            'handlers': ['file'],
            'level': 'WARNING',
            'propagate': False,
        },
    },
}

# ì•Œë¦¼ ì„¤ì •
ALERT_EMAIL_CONFIG = {
    'smtp_host': 'smtp.gmail.com',
    'smtp_port': 587,
    'username': 'your-email@gmail.com',
    'password': 'your-app-password',
    'from_email': 'alerts@yourcompany.com',
}

ALERT_EMAIL_RECIPIENTS = [
    'admin@yourcompany.com',
    'devops@yourcompany.com',
]

SLACK_WEBHOOK_URL = 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
DISCORD_WEBHOOK_URL = 'https://discord.com/api/webhooks/YOUR/DISCORD/WEBHOOK'

# Celery ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ ìŠ¤ì¼€ì¤„
CELERY_BEAT_SCHEDULE = {
    'collect-system-metrics': {
        'task': 'monitoring.tasks.collect_system_metrics',
        'schedule': 60.0,  # 1ë¶„ë§ˆë‹¤
    },
    'monitor-database': {
        'task': 'monitoring.tasks.monitor_database_performance',
        'schedule': 300.0,  # 5ë¶„ë§ˆë‹¤
    },
    'monitor-redis': {
        'task': 'monitoring.tasks.monitor_redis_performance',
        'schedule': 180.0,  # 3ë¶„ë§ˆë‹¤
    },
}
```

---

## 6. ì‹¤ì „ ì‚¬ë¡€ ë° ëª¨ë²” ì‚¬ë¡€

ì´ ì„¹ì…˜ì—ì„œëŠ” ì‹¤ì œ ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ì„ ì²˜ë¦¬í•˜ëŠ” Django ì„œë¹„ìŠ¤ì˜ ì‚¬ë¡€ì™€ ê²€ì¦ëœ ëª¨ë²” ì‚¬ë¡€ë“¤ì„ ì†Œê°œí•©ë‹ˆë‹¤.

### 1. ì‹¤ì œ ì‚¬ë¡€: ëŒ€í˜• ì´ì»¤ë¨¸ìŠ¤ í”Œë«í¼

#### ì‚¬ë¡€ ê°œìš”
- **ì¼ì¼ í™œì„± ì‚¬ìš©ì**: 100ë§Œëª…
- **ì¼ì¼ ì£¼ë¬¸ ê±´ìˆ˜**: 5ë§Œê±´
- **í”¼í¬ ì‹œê°„ ë™ì‹œ ì ‘ì†ì**: 10ë§Œëª…
- **ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸°**: 5TB+

#### ì•„í‚¤í…ì²˜ êµ¬ì„±

```python
# ì‹¤ì œ ì ìš©ëœ ì„¤ì • ì˜ˆì‹œ
# settings/production.py

# 1. ASGI ì„œë²„ êµ¬ì„±
ASGI_APPLICATION = 'myapp.asgi.application'

# Daphne ì„¤ì • (docker-compose.ymlì—ì„œ)
"""
services:
  web:
    image: myapp:latest
    command: daphne -b 0.0.0.0 -p 8000 --access-log - myapp.asgi:application
    deploy:
      replicas: 12
      resources:
        limits:
          cpus: '2'
          memory: 4G
    environment:
      - DJANGO_SETTINGS_MODULE=myapp.settings.production
"""

# 2. ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'ecommerce_main',
        'USER': 'django_user',
        'PASSWORD': os.environ['DB_PASSWORD'],
        'HOST': 'postgresql-master.internal',
        'PORT': '5432',
        'OPTIONS': {
            'MAX_CONNS': 20,
            'CONN_MAX_AGE': 300,
        },
        'TEST': {
            'NAME': 'test_ecommerce',
        }
    },
    'replica': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'ecommerce_main',
        'USER': 'django_readonly',
        'PASSWORD': os.environ['DB_READONLY_PASSWORD'],
        'HOST': 'postgresql-replica.internal',
        'PORT': '5432',
        'OPTIONS': {
            'MAX_CONNS': 15,
            'CONN_MAX_AGE': 600,
        }
    },
    'analytics': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'ecommerce_analytics',
        'USER': 'analytics_user',
        'PASSWORD': os.environ['ANALYTICS_DB_PASSWORD'],
        'HOST': 'postgresql-analytics.internal',
        'PORT': '5432',
        'OPTIONS': {
            'MAX_CONNS': 10,
        }
    }
}

# 3. ìºì‹œ í´ëŸ¬ìŠ¤í„° ì„¤ì •
CACHES = {
    'default': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': [
            'redis://redis-cluster-1.internal:6379/1',
            'redis://redis-cluster-2.internal:6379/1',
            'redis://redis-cluster-3.internal:6379/1',
        ],
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.ShardClient',
            'CONNECTION_POOL_KWARGS': {
                'max_connections': 50,
                'retry_on_timeout': True,
            }
        }
    },
    'sessions': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': 'redis://redis-sessions.internal:6379/2',
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
            'CONNECTION_POOL_KWARGS': {
                'max_connections': 100,
            }
        }
    }
}

# 4. Celery ì„¤ì •
CELERY_BROKER_URL = 'redis://redis-celery.internal:6379/0'
CELERY_RESULT_BACKEND = 'redis://redis-celery.internal:6379/0'

CELERY_TASK_ROUTES = {
    'orders.tasks.process_payment': {'queue': 'high_priority'},
    'orders.tasks.send_confirmation_email': {'queue': 'email'},
    'analytics.tasks.update_metrics': {'queue': 'analytics'},
    'inventory.tasks.update_stock': {'queue': 'inventory'},
}

CELERY_WORKER_CONCURRENCY = 8
CELERY_WORKER_PREFETCH_MULTIPLIER = 2
CELERY_TASK_ACKS_LATE = True
CELERY_WORKER_MAX_TASKS_PER_CHILD = 1000
```

#### í•µì‹¬ ìµœì í™” ì „ëµ

```python
# 1. ìŠ¤ë§ˆíŠ¸ ìºì‹± ì „ëµ
# models/product.py
class Product(models.Model):
    name = models.CharField(max_length=255)
    price = models.DecimalField(max_digits=10, decimal_places=2)
    stock = models.PositiveIntegerField()
    category = models.ForeignKey(Category, on_delete=models.CASCADE)
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    @cached_property
    def cache_key(self):
        return f"product:{self.id}:{self.updated_at.timestamp()}"
    
    def get_cached_data(self):
        """ìºì‹œëœ ìƒí’ˆ ë°ì´í„° ì¡°íšŒ"""
        cached_data = cache.get(self.cache_key)
        if cached_data is None:
            cached_data = {
                'id': self.id,
                'name': self.name,
                'price': str(self.price),
                'stock': self.stock,
                'category_name': self.category.name,
                'image_urls': list(self.images.values_list('url', flat=True)),
                'average_rating': self.reviews.aggregate(
                    avg_rating=models.Avg('rating')
                )['avg_rating'] or 0,
                'review_count': self.reviews.count(),
            }
            # ìƒí’ˆ ë°ì´í„°ëŠ” 6ì‹œê°„ ìºì‹œ
            cache.set(self.cache_key, cached_data, 21600)
        return cached_data
    
    def invalidate_cache(self):
        """ìºì‹œ ë¬´íš¨í™”"""
        cache.delete(self.cache_key)
        # ê´€ë ¨ ì¹´í…Œê³ ë¦¬ ìºì‹œë„ ë¬´íš¨í™”
        cache.delete(f"category_products:{self.category.id}")

# 2. ì£¼ë¬¸ ì²˜ë¦¬ ìµœì í™”
# services/order_service.py
class OrderService:
    """ì£¼ë¬¸ ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    @transaction.atomic
    def create_order(self, user, cart_items):
        """ì£¼ë¬¸ ìƒì„±"""
        # 1. ì¬ê³  í™•ì¸ ë° ì˜ˆì•½
        self._reserve_inventory(cart_items)
        
        # 2. ì£¼ë¬¸ ìƒì„±
        order = Order.objects.create(
            user=user,
            status='PENDING',
            total_amount=self._calculate_total(cart_items)
        )
        
        # 3. ì£¼ë¬¸ í•­ëª© ìƒì„±
        order_items = []
        for item in cart_items:
            order_items.append(OrderItem(
                order=order,
                product_id=item['product_id'],
                quantity=item['quantity'],
                price=item['price']
            ))
        OrderItem.objects.bulk_create(order_items)
        
        # 4. ë¹„ë™ê¸° í›„ì²˜ë¦¬ ì‘ì—…
        from orders.tasks import process_order_async
        process_order_async.delay(order.id)
        
        return order
    
    def _reserve_inventory(self, cart_items):
        """ì¬ê³  ì˜ˆì•½ (ë‚™ê´€ì  ì ê¸ˆ ì‚¬ìš©)"""
        for item in cart_items:
            product = Product.objects.select_for_update().get(
                id=item['product_id']
            )
            
            if product.stock < item['quantity']:
                raise ValueError(f"Insufficient stock for {product.name}")
            
            # ì¬ê³  ì°¨ê°
            product.stock -= item['quantity']
            product.save(update_fields=['stock'])
            
            # ìºì‹œ ë¬´íš¨í™”
            product.invalidate_cache()

# 3. ê²€ìƒ‰ ìµœì í™”
# services/search_service.py
class SearchService:
    """ê²€ìƒ‰ ì„œë¹„ìŠ¤"""
    
    def search_products(self, query, filters=None, page=1, per_page=20):
        """ìƒí’ˆ ê²€ìƒ‰"""
        cache_key = self._get_search_cache_key(query, filters, page, per_page)
        
        cached_result = cache.get(cache_key)
        if cached_result:
            return cached_result
        
        # Elasticsearch ë˜ëŠ” PostgreSQL í’€í…ìŠ¤íŠ¸ ê²€ìƒ‰
        if hasattr(settings, 'ELASTICSEARCH_DSL'):
            results = self._elasticsearch_search(query, filters, page, per_page)
        else:
            results = self._postgresql_search(query, filters, page, per_page)
        
        # 15ë¶„ ìºì‹œ
        cache.set(cache_key, results, 900)
        return results
    
    def _postgresql_search(self, query, filters, page, per_page):
        """PostgreSQL í’€í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        from django.contrib.postgres.search import SearchVector, SearchQuery, SearchRank
        
        search_vector = SearchVector('name', weight='A') + SearchVector('description', weight='B')
        search_query = SearchQuery(query)
        
        queryset = Product.objects.annotate(
            search=search_vector,
            rank=SearchRank(search_vector, search_query)
        ).filter(search=search_query)
        
        # í•„í„° ì ìš©
        if filters:
            if filters.get('category_id'):
                queryset = queryset.filter(category_id=filters['category_id'])
            if filters.get('price_min'):
                queryset = queryset.filter(price__gte=filters['price_min'])
            if filters.get('price_max'):
                queryset = queryset.filter(price__lte=filters['price_max'])
        
        # ì •ë ¬ ë° í˜ì´ì§•
        queryset = queryset.order_by('-rank', '-created_at')
        
        offset = (page - 1) * per_page
        products = list(queryset[offset:offset + per_page])
        total_count = queryset.count()
        
        return {
            'products': [product.get_cached_data() for product in products],
            'total_count': total_count,
            'page': page,
            'per_page': per_page,
            'total_pages': (total_count + per_page - 1) // per_page,
        }
```

### 2. ì„±ëŠ¥ ê°œì„  ì‚¬ë¡€

#### Before/After ë¹„êµ

```python
# Before: ë¹„íš¨ìœ¨ì ì¸ ì½”ë“œ
def get_user_orders_bad(user_id):
    """ë¹„íš¨ìœ¨ì ì¸ ì£¼ë¬¸ ì¡°íšŒ"""
    user = User.objects.get(id=user_id)  # N+1 ë¬¸ì œ
    orders = []
    
    for order in user.orders.all():  # ì¶”ê°€ ì¿¼ë¦¬
        order_data = {
            'id': order.id,
            'created_at': order.created_at,
            'total_amount': order.total_amount,
            'items': []
        }
        
        for item in order.items.all():  # N+1 ë¬¸ì œ
            product = item.product  # ì¶”ê°€ ì¿¼ë¦¬
            order_data['items'].append({
                'product_name': product.name,  # ì¶”ê°€ ì¿¼ë¦¬
                'quantity': item.quantity,
                'price': item.price,
            })
        
        orders.append(order_data)
    
    return orders

# After: ìµœì í™”ëœ ì½”ë“œ
def get_user_orders_optimized(user_id):
    """ìµœì í™”ëœ ì£¼ë¬¸ ì¡°íšŒ"""
    # ìºì‹œ í™•ì¸
    cache_key = f"user_orders:{user_id}"
    cached_orders = cache.get(cache_key)
    if cached_orders:
        return cached_orders
    
    # ìµœì í™”ëœ ì¿¼ë¦¬ (í•œ ë²ˆì— ëª¨ë“  ë°ì´í„° ì¡°íšŒ)
    orders = Order.objects.filter(user_id=user_id).select_related('user').prefetch_related(
        'items__product'
    ).order_by('-created_at')
    
    # ë°ì´í„° ë³€í™˜
    orders_data = []
    for order in orders:
        order_data = {
            'id': order.id,
            'created_at': order.created_at.isoformat(),
            'total_amount': str(order.total_amount),
            'items': [
                {
                    'product_name': item.product.name,
                    'quantity': item.quantity,
                    'price': str(item.price),
                }
                for item in order.items.all()
            ]
        }
        orders_data.append(order_data)
    
    # ìºì‹œ ì €ì¥ (1ì‹œê°„)
    cache.set(cache_key, orders_data, 3600)
    return orders_data

# ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:
# Before: í‰ê·  2.5ì´ˆ, 150+ ì¿¼ë¦¬
# After: í‰ê·  0.1ì´ˆ, 2 ì¿¼ë¦¬
```

### 3. ëª¨ë²” ì‚¬ë¡€ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### ê°œë°œ ë‹¨ê³„

```python
# ì½”ë“œ ë¦¬ë·° ì²´í¬ë¦¬ìŠ¤íŠ¸
PERFORMANCE_CHECKLIST = {
    'database': [
        'select_related() ë˜ëŠ” prefetch_related() ì‚¬ìš© í™•ì¸',
        'bulk_create(), bulk_update() í™œìš©',
        'only(), defer() í•„ìš”í•œ í•„ë“œë§Œ ì¡°íšŒ',
        'exists() vs count() ì ì ˆí•œ ì‚¬ìš©',
        'iterator() ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬',
        'F() í‘œí˜„ì‹ìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ë ˆë²¨ ì—°ì‚°',
        'ì¸ë±ìŠ¤ ì¶”ê°€ ê²€í† ',
        'explain() ì¿¼ë¦¬ ì‹¤í–‰ ê³„íš í™•ì¸'
    ],
    'caching': [
        'view ë ˆë²¨ ìºì‹± ì ìš©',
        'template fragment ìºì‹±',
        'low-level ìºì‹œ API í™œìš©',
        'ìºì‹œ TTL ì„¤ì • ê²€í† ',
        'ìºì‹œ ë¬´íš¨í™” ì „ëµ êµ¬í˜„',
        'cache_page ë°ì½”ë ˆì´í„° í™œìš©',
        'Vary í—¤ë” ì„¤ì •'
    ],
    'async': [
        'ë¬´ê±°ìš´ ì‘ì—… Celery íƒœìŠ¤í¬ë¡œ ë¶„ë¦¬',
        'ì´ë©”ì¼/SMS ë¹„ë™ê¸° ì²˜ë¦¬',
        'ì´ë¯¸ì§€ ì²˜ë¦¬ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…',
        'API í˜¸ì¶œ ë¹„ë™ê¸° ì²˜ë¦¬',
        'ë°ì´í„° ë¶„ì„ ë°°ì¹˜ ì‘ì—…'
    ],
    'security': [
        'SQL injection ë°©ì§€',
        'CSRF í† í° ê²€ì¦',
        'XSS ë°©ì§€',
        'ì¸ì¦/ì¸ê°€ ë¡œì§ ê²€ì¦',
        'ë¯¼ê°í•œ ë°ì´í„° ì•”í˜¸í™”',
        'rate limiting ì ìš©'
    ]
}
```

#### ë°°í¬ ë‹¨ê³„

```bash
# ë°°í¬ ì „ ì„±ëŠ¥ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
#!/bin/bash

echo "=== Django Performance Check ==="

# 1. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
echo "Checking database connections..."
python manage.py dbshell -c "SELECT COUNT(*) FROM pg_stat_activity;"

# 2. ìºì‹œ ì—°ê²° í™•ì¸
echo "Checking cache connections..."
python manage.py shell -c "
from django.core.cache import cache
cache.set('test', 'ok', 10)
print('Cache test:', cache.get('test'))
"

# 3. Celery ì›Œì»¤ ìƒíƒœ í™•ì¸
echo "Checking Celery workers..."
celery -A myapp inspect active

# 4. ì •ì  íŒŒì¼ ìˆ˜ì§‘
echo "Collecting static files..."
python manage.py collectstatic --noinput

# 5. ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ í™•ì¸
echo "Checking migrations..."
python manage.py showmigrations --plan

# 6. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸
echo "Checking system resources..."
free -h
df -h
```

#### ìš´ì˜ ë‹¨ê³„

```python
# ìš´ì˜ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
# management/commands/health_check.py
from django.core.management.base import BaseCommand
from django.db import connection
from django.core.cache import cache
import redis
import time

class Command(BaseCommand):
    help = 'Comprehensive health check'
    
    def handle(self, *args, **options):
        health_status = {
            'database': self.check_database(),
            'cache': self.check_cache(),
            'celery': self.check_celery(),
            'disk_space': self.check_disk_space(),
            'memory': self.check_memory(),
        }
        
        overall_status = all(health_status.values())
        
        self.stdout.write(f"Overall Health: {'âœ… HEALTHY' if overall_status else 'âŒ UNHEALTHY'}")
        
        for component, status in health_status.items():
            icon = 'âœ…' if status else 'âŒ'
            self.stdout.write(f"{icon} {component.upper()}: {'OK' if status else 'FAIL'}")
        
        if not overall_status:
            exit(1)
    
    def check_database(self):
        try:
            with connection.cursor() as cursor:
                cursor.execute("SELECT 1")
                return True
        except Exception as e:
            self.stderr.write(f"Database check failed: {e}")
            return False
    
    def check_cache(self):
        try:
            cache.set('health_check', 'ok', 10)
            return cache.get('health_check') == 'ok'
        except Exception as e:
            self.stderr.write(f"Cache check failed: {e}")
            return False
    
    def check_celery(self):
        try:
            from celery import current_app
            inspect = current_app.control.inspect()
            active_workers = inspect.active()
            return bool(active_workers)
        except Exception as e:
            self.stderr.write(f"Celery check failed: {e}")
            return False
    
    def check_disk_space(self):
        import shutil
        try:
            total, used, free = shutil.disk_usage('/')
            free_percent = free / total * 100
            return free_percent > 10  # 10% ì´ìƒ ì—¬ìœ  ê³µê°„
        except Exception as e:
            self.stderr.write(f"Disk check failed: {e}")
            return False
    
    def check_memory(self):
        try:
            import psutil
            memory = psutil.virtual_memory()
            return memory.percent < 90  # 90% ë¯¸ë§Œ ì‚¬ìš©ë¥ 
        except Exception as e:
            self.stderr.write(f"Memory check failed: {e}")
            return False
```

### 4. íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

#### ì¼ë°˜ì ì¸ ì„±ëŠ¥ ë¬¸ì œì™€ í•´ê²°ì±…

```python
# 1. N+1 ì¿¼ë¦¬ ë¬¸ì œ í•´ê²°
# ë¬¸ì œ: ë¦¬ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ ê°ì²´ì— ì ‘ê·¼í•  ë•Œ ì¶”ê°€ ì¿¼ë¦¬ ë°œìƒ
def fix_n_plus_one():
    # âŒ ì˜ëª»ëœ ë°©ë²•
    posts = Post.objects.all()
    for post in posts:
        print(post.author.username)  # ê° postë§ˆë‹¤ author ì¿¼ë¦¬ ì‹¤í–‰
    
    # âœ… ì˜¬ë°”ë¥¸ ë°©ë²•
    posts = Post.objects.select_related('author')
    for post in posts:
        print(post.author.username)  # í•œ ë²ˆì˜ JOIN ì¿¼ë¦¬ë¡œ í•´ê²°

# 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
def optimize_memory_usage():
    # âŒ í° QuerySetì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ
    all_users = list(User.objects.all())
    
    # âœ… iterator() ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
    for user in User.objects.iterator(chunk_size=1000):
        process_user(user)

# 3. ìºì‹œ ë¯¸ìŠ¤ ìµœì†Œí™”
class OptimizedProductView:
    def get_product_data(self, product_id):
        # ë‹¤ë‹¨ê³„ ìºì‹œ ì „ëµ
        
        # L1: ì¸ë©”ëª¨ë¦¬ ìºì‹œ (ê°€ì¥ ë¹ ë¦„)
        cache_key = f"product:{product_id}"
        data = self.local_cache.get(cache_key)
        if data:
            return data
        
        # L2: Redis ìºì‹œ
        data = cache.get(cache_key)
        if data:
            self.local_cache.set(cache_key, data, 300)  # 5ë¶„ ë¡œì»¬ ìºì‹œ
            return data
        
        # L3: ë°ì´í„°ë² ì´ìŠ¤
        product = Product.objects.select_related('category').get(id=product_id)
        data = product.get_cached_data()
        
        # ìºì‹œì— ì €ì¥
        cache.set(cache_key, data, 3600)  # 1ì‹œê°„ Redis ìºì‹œ
        self.local_cache.set(cache_key, data, 300)  # 5ë¶„ ë¡œì»¬ ìºì‹œ
        
        return data
```

### 5. ì„±ëŠ¥ ì¸¡ì • ë° ë²¤ì¹˜ë§ˆí‚¹

```python
# performance/benchmarks.py
import time
import statistics
from django.test import TestCase, TransactionTestCase
from django.test.utils import override_settings
import concurrent.futures

class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ë„êµ¬"""
    
    def __init__(self, iterations=100):
        self.iterations = iterations
        self.results = []
    
    def benchmark(self, func, *args, **kwargs):
        """í•¨ìˆ˜ ì„±ëŠ¥ ì¸¡ì •"""
        times = []
        
        for _ in range(self.iterations):
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                success = True
            except Exception as e:
                result = None
                success = False
                print(f"Error in benchmark: {e}")
            
            end_time = time.time()
            times.append(end_time - start_time)
        
        stats = {
            'iterations': self.iterations,
            'mean': statistics.mean(times),
            'median': statistics.median(times),
            'min': min(times),
            'max': max(times),
            'std_dev': statistics.stdev(times) if len(times) > 1 else 0,
        }
        
        return stats
    
    def stress_test(self, func, concurrent_users=10, duration=60):
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        results = []
        
        def worker():
            worker_results = []
            while time.time() - start_time < duration:
                try:
                    start = time.time()
                    func()
                    end = time.time()
                    worker_results.append(end - start)
                except Exception as e:
                    worker_results.append(None)
            return worker_results
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = [executor.submit(worker) for _ in range(concurrent_users)]
            
            for future in concurrent.futures.as_completed(futures):
                results.extend(future.result())
        
        successful_requests = [r for r in results if r is not None]
        failed_requests = len([r for r in results if r is None])
        
        return {
            'total_requests': len(results),
            'successful_requests': len(successful_requests),
            'failed_requests': failed_requests,
            'success_rate': len(successful_requests) / len(results) * 100,
            'avg_response_time': statistics.mean(successful_requests) if successful_requests else 0,
            'requests_per_second': len(successful_requests) / duration,
        }

# ì‚¬ìš© ì˜ˆì‹œ
benchmark = PerformanceBenchmark()

# API ì—”ë“œí¬ì¸íŠ¸ ì„±ëŠ¥ ì¸¡ì •
def test_api_performance():
    from django.test import Client
    client = Client()
    
    def api_call():
        response = client.get('/api/products/')
        return response.status_code == 200
    
    stats = benchmark.benchmark(api_call)
    print(f"API Performance: {stats['mean']:.3f}s average")

# ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì •
def test_query_performance():
    def query_test():
        return list(Product.objects.select_related('category')[:100])
    
    stats = benchmark.benchmark(query_test)
    print(f"Query Performance: {stats['mean']:.3f}s average")
```

## ê²°ë¡ 

Djangoì—ì„œ ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ì„ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ í•µì‹¬ ì›ì¹™ë“¤ì„ ê¸°ì–µí•´ì•¼ í•©ë‹ˆë‹¤:

### í•µì‹¬ ì›ì¹™

1. **ì¸¡ì • ì—†ì´ëŠ” ìµœì í™” ì—†ë‹¤**: í•­ìƒ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ë³‘ëª©ì ì„ íŒŒì•…í•œ í›„ ìµœì í™”ë¥¼ ì§„í–‰í•˜ì„¸ìš”.

2. **ì ì§„ì  ê°œì„ **: í•œ ë²ˆì— ëª¨ë“  ê²ƒì„ ë°”ê¾¸ë ¤ í•˜ì§€ ë§ê³ , ê°€ì¥ í° impactë¥¼ ê°€ì§„ ë¶€ë¶„ë¶€í„° ì°¨ë¡€ëŒ€ë¡œ ê°œì„ í•˜ì„¸ìš”.

3. **ìºì‹±ì€ í•„ìˆ˜**: ì ì ˆí•œ ìºì‹± ì „ëµì€ ì„±ëŠ¥ í–¥ìƒì˜ ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•ì…ë‹ˆë‹¤.

4. **ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”**: N+1 ì¿¼ë¦¬ ë¬¸ì œ í•´ê²°ê³¼ ì ì ˆí•œ ì¸ë±ì‹±ì€ ê¸°ë³¸ ì¤‘ì˜ ê¸°ë³¸ì…ë‹ˆë‹¤.

5. **ë¹„ë™ê¸° ì²˜ë¦¬**: ë¬´ê±°ìš´ ì‘ì—…ì€ ë°˜ë“œì‹œ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬í•˜ì„¸ìš”.

6. **ëª¨ë‹ˆí„°ë§ê³¼ ì•Œë¦¼**: ë¬¸ì œê°€ ë°œìƒí•˜ê¸° ì „ì— ë¯¸ë¦¬ ê°ì§€í•  ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ì„¸ìš”.

### ë§ˆì§€ë§‰ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### ğŸ” ê°œë°œ ë‹¨ê³„
- [ ] Django Debug Toolbarë¡œ ì¿¼ë¦¬ ìµœì í™” í™•ì¸
- [ ] select_related/prefetch_related ì ìš©
- [ ] ìºì‹± ì „ëµ ìˆ˜ë¦½
- [ ] ë¹„ë™ê¸° ì‘ì—… ë¶„ë¦¬
- [ ] ì½”ë“œ ë¦¬ë·°ì—ì„œ ì„±ëŠ¥ ì²´í¬

#### ğŸš€ ë°°í¬ ë‹¨ê³„  
- [ ] ì •ì  íŒŒì¼ CDN ì„¤ì •
- [ ] Gzip ì••ì¶• í™œì„±í™”
- [ ] ASGI ì„œë²„ ì„¤ì •
- [ ] ë¡œë“œ ë°¸ëŸ°ì„œ êµ¬ì„±
- [ ] SSL ìµœì í™”

#### ğŸ“Š ìš´ì˜ ë‹¨ê³„
- [ ] ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ìš´ì˜
- [ ] ì •ê¸°ì ì¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
- [ ] ìš©ëŸ‰ ê³„íš ìˆ˜ë¦½
- [ ] ì¥ì•  ëŒ€ì‘ ë§¤ë‰´ì–¼ ì‘ì„±
- [ ] ë°±ì—… ë° ë³µêµ¬ ì „ëµ ê²€ì¦

DjangoëŠ” ì ì ˆí•œ ìµœì í™”ë¥¼ í†µí•´ ì¶©ë¶„íˆ ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œ ì œì‹œí•œ ë°©ë²•ë“¤ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì ìš©í•˜ì—¬ ì—¬ëŸ¬ë¶„ì˜ ì„œë¹„ìŠ¤ê°€ ë” ë§ì€ ì‚¬ìš©ìë“¤ì—ê²Œ ì•ˆì •ì ìœ¼ë¡œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ ìˆê¸°ë¥¼ ë°”ëë‹ˆë‹¤.

ì„±ëŠ¥ ìµœì í™”ëŠ” ì§€ì†ì ì¸ ê³¼ì •ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ëŠ˜ì–´ë‚˜ê³  ì„œë¹„ìŠ¤ê°€ ë³µì¡í•´ì§ˆìˆ˜ë¡ ìƒˆë¡œìš´ ë³‘ëª©ì ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, í•­ìƒ ëª¨ë‹ˆí„°ë§í•˜ê³  ê°œì„ í•´ ë‚˜ê°€ëŠ” ìì„¸ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.

ë‹¤ìŒ ì„¹ì…˜ì—ì„œëŠ” **ì‹¤ì „ ì‚¬ë¡€ ë° ê²°ë¡ **ì„ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.