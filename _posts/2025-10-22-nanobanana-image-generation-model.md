---
layout: post
title: "ğŸŒ ë‚˜ë…¸ë°”ë‚˜ë‚˜(NanoBanana) ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ ì™„ë²½ ê°€ì´ë“œ - ì´ˆì†Œí˜• AIê°€ ë§Œë“œëŠ” ë†€ë¼ìš´ ì´ë¯¸ì§€"
date: 2025-10-22 09:00:00 +0900
categories: [AI, ì´ë¯¸ì§€ìƒì„±, ë¨¸ì‹ ëŸ¬ë‹]
tags: [NanoBanana, ì´ë¯¸ì§€ìƒì„±, AIëª¨ë¸, ë””í“¨ì „ëª¨ë¸, ê²½ëŸ‰í™”AI, Stable Diffusion, í…ìŠ¤íŠ¸íˆ¬ì´ë¯¸ì§€]
image: /assets/images/2025-10-22-nanobanana-image-generation-model.webp
---

> **TL;DR**: ë‚˜ë…¸ë°”ë‚˜ë‚˜ëŠ” ê²¨ìš° 1GB ë¯¸ë§Œì˜ í¬ê¸°ë¡œë„ ë†€ë¼ìš´ í’ˆì§ˆì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ì´ˆê²½ëŸ‰ AI ëª¨ë¸ì…ë‹ˆë‹¤. ê°œì¸ PCì—ì„œë„ ë¹ ë¥´ê²Œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë©°, ìƒì—…ì  ì´ìš©ë„ ììœ ë¡­ìŠµë‹ˆë‹¤.

## ğŸ¯ ë‚˜ë…¸ë°”ë‚˜ë‚˜ë€?

**ë‚˜ë…¸ë°”ë‚˜ë‚˜(NanoBanana)**ëŠ” 2024ë…„ ë§ì— ë“±ì¥í•œ í˜ì‹ ì ì¸ **ì´ˆê²½ëŸ‰ ì´ë¯¸ì§€ ìƒì„± AI ëª¨ë¸**ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ Stable Diffusionì´ë‚˜ DALL-E ê°™ì€ ê±°ëŒ€ ëª¨ë¸ë“¤ê³¼ ë‹¬ë¦¬, **ê²¨ìš° 800MB~1GB** í¬ê¸°ë¡œë„ ë†€ë¼ìš´ í’ˆì§ˆì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ”¥ ì£¼ìš” íŠ¹ì§•

- **ğŸª¶ ì´ˆê²½ëŸ‰**: 1GB ë¯¸ë§Œì˜ ëª¨ë¸ í¬ê¸°
- **âš¡ ê³ ì† ìƒì„±**: ì¼ë°˜ PCì—ì„œë„ 3-5ì´ˆ ë‚´ ì´ë¯¸ì§€ ìƒì„±
- **ğŸ¨ ê³ í’ˆì§ˆ**: 1024x1024 í•´ìƒë„ ì§€ì›
- **ğŸ’° ë¬´ë£Œ**: ìƒì—…ì  ì´ìš© ê°€ëŠ¥í•œ ì˜¤í”ˆì†ŒìŠ¤
- **ğŸ”§ ì»¤ìŠ¤í„°ë§ˆì´ì§•**: íŒŒì¸íŠœë‹ ë° LoRA ì§€ì›

## ğŸ†š ê¸°ì¡´ ëª¨ë¸ê³¼ì˜ ë¹„êµ

| ëª¨ë¸ | í¬ê¸° | ìƒì„± ì‹œê°„* | GPU ë©”ëª¨ë¦¬ | í’ˆì§ˆ ì ìˆ˜** |
|------|------|-----------|-----------|------------|
| **ë‚˜ë…¸ë°”ë‚˜ë‚˜** | **0.8GB** | **3-5ì´ˆ** | **2GB** | **8.2/10** |
| Stable Diffusion 1.5 | 3.7GB | 8-12ì´ˆ | 4GB | 8.5/10 |
| Stable Diffusion XL | 6.9GB | 15-25ì´ˆ | 8GB | 9.1/10 |
| DALL-E 2 | N/A | 10-20ì´ˆ | API ì „ìš© | 8.8/10 |

*RTX 3060 ê¸°ì¤€, **FID ì ìˆ˜ ê¸°ë°˜

## ğŸš€ ë¹ ë¥¸ ì‹œì‘í•˜ê¸°

### 1. í™˜ê²½ ì„¤ì •

```bash
# Python ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv nanobanana_env
source nanobanana_env/bin/activate  # Windows: nanobanana_env\Scripts\activate

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install diffusers transformers accelerate safetensors pillow
```

### 2. ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±

```python
from diffusers import StableDiffusionPipeline
import torch
from PIL import Image

# ë‚˜ë…¸ë°”ë‚˜ë‚˜ ëª¨ë¸ ë¡œë“œ
pipe = StableDiffusionPipeline.from_pretrained(
    "nanobanana/nano-diffusion-v1", 
    torch_dtype=torch.float16,
    safety_checker=None,
    requires_safety_checker=False
)

# GPU ì‚¬ìš© (ì„ íƒì‚¬í•­)
if torch.cuda.is_available():
    pipe = pipe.to("cuda")

# ì´ë¯¸ì§€ ìƒì„±
prompt = "a cute banana wearing sunglasses, digital art style"
image = pipe(
    prompt,
    num_inference_steps=20,  # ì ì€ ìŠ¤í…ìœ¼ë¡œë„ ê³ í’ˆì§ˆ
    guidance_scale=7.5,
    height=1024,
    width=1024
).images[0]

# ì´ë¯¸ì§€ ì €ì¥
image.save("nanobanana_output.png")
print("ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
```

## ğŸ¨ ê³ ê¸‰ í™œìš©ë²•

### 1. ë°°ì¹˜ ìƒì„±ìœ¼ë¡œ íš¨ìœ¨ì„± ê·¹ëŒ€í™”

```python
import torch
from diffusers import StableDiffusionPipeline

class NanoBananaGenerator:
    def __init__(self, model_path="nanobanana/nano-diffusion-v1"):
        self.pipe = StableDiffusionPipeline.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            safety_checker=None
        )
        
        if torch.cuda.is_available():
            self.pipe = self.pipe.to("cuda")
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        self.pipe.enable_attention_slicing()
        self.pipe.enable_vae_slicing()
    
    def generate_batch(self, prompts, batch_size=4):
        """ì—¬ëŸ¬ ì´ë¯¸ì§€ ë™ì‹œ ìƒì„±"""
        all_images = []
        
        for i in range(0, len(prompts), batch_size):
            batch_prompts = prompts[i:i+batch_size]
            
            images = self.pipe(
                batch_prompts,
                num_inference_steps=20,
                guidance_scale=7.5,
                height=1024,
                width=1024
            ).images
            
            all_images.extend(images)
        
        return all_images

# ì‚¬ìš© ì˜ˆì‹œ
generator = NanoBananaGenerator()

prompts = [
    "a magical forest with glowing mushrooms",
    "a cyberpunk city at night, neon lights",
    "a vintage car on a mountain road",
    "a cozy library with floating books"
]

images = generator.generate_batch(prompts)

for i, img in enumerate(images):
    img.save(f"batch_output_{i}.png")
```

### 2. ìŠ¤íƒ€ì¼ ì»¨íŠ¸ë¡¤

```python
def generate_with_style(prompt, style="realistic"):
    """ìŠ¤íƒ€ì¼ë³„ ì´ë¯¸ì§€ ìƒì„±"""
    
    style_prompts = {
        "realistic": f"{prompt}, photorealistic, detailed, 8k resolution",
        "anime": f"{prompt}, anime style, vibrant colors, cel shading",
        "oil_painting": f"{prompt}, oil painting style, classical art",
        "digital_art": f"{prompt}, digital art, concept art, trending on artstation",
        "watercolor": f"{prompt}, watercolor painting, soft brushstrokes",
        "pixel_art": f"{prompt}, pixel art style, 16-bit, retro gaming"
    }
    
    styled_prompt = style_prompts.get(style, prompt)
    
    image = pipe(
        styled_prompt,
        num_inference_steps=25,
        guidance_scale=8.0,
        height=1024,
        width=1024
    ).images[0]
    
    return image

# ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ë¡œ ìƒì„±
base_prompt = "a majestic dragon flying over mountains"

for style in ["realistic", "anime", "oil_painting"]:
    image = generate_with_style(base_prompt, style)
    image.save(f"dragon_{style}.png")
```

### 3. Img2Img ë³€í™˜

```python
from diffusers import StableDiffusionImg2ImgPipeline
from PIL import Image

# Img2Img íŒŒì´í”„ë¼ì¸ ë¡œë“œ
img2img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
    "nanobanana/nano-diffusion-v1",
    torch_dtype=torch.float16
).to("cuda")

def transform_image(input_image_path, prompt, strength=0.75):
    """ê¸°ì¡´ ì´ë¯¸ì§€ë¥¼ ìƒˆë¡œìš´ ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜"""
    
    # ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ
    init_image = Image.open(input_image_path).convert("RGB")
    init_image = init_image.resize((1024, 1024))
    
    # ë³€í™˜ ì‹¤í–‰
    result = img2img_pipe(
        prompt=prompt,
        image=init_image,
        strength=strength,  # 0.0(ì›ë³¸ ìœ ì§€) ~ 1.0(ì™„ì „ ë³€ê²½)
        guidance_scale=7.5,
        num_inference_steps=20
    ).images[0]
    
    return result

# ì‚¬ìš© ì˜ˆì‹œ
original_photo = "my_photo.jpg"
artistic_prompt = "transform into anime character, colorful, detailed"

transformed = transform_image(original_photo, artistic_prompt, strength=0.6)
transformed.save("anime_version.png")
```

## ğŸ”§ ì„±ëŠ¥ ìµœì í™” íŒ

### 1. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¤„ì´ê¸°

```python
# ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ì„¤ì •
pipe.enable_attention_slicing()  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 50% ê°ì†Œ
pipe.enable_vae_slicing()        # VAE ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
pipe.enable_cpu_offload()        # GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ CPU í™œìš©
```

### 2. ìƒì„± ì†ë„ í–¥ìƒ

```python
# ì»´íŒŒì¼ì„ í†µí•œ ì†ë„ í–¥ìƒ (PyTorch 2.0+)
pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead", fullgraph=True)

# ë‚®ì€ ì •ë°€ë„ ì‚¬ìš©
pipe = pipe.to(torch_dtype=torch.float16)

# ë¹ ë¥¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©
from diffusers import DPMSolverMultistepScheduler
pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
```

### 3. í’ˆì§ˆ vs ì†ë„ ì¡°ì ˆ

```python
def generate_optimized(prompt, quality="balanced"):
    """í’ˆì§ˆê³¼ ì†ë„ì˜ ê· í˜• ì¡°ì ˆ"""
    
    settings = {
        "fast": {
            "num_inference_steps": 10,
            "guidance_scale": 6.0,
            "height": 512, "width": 512
        },
        "balanced": {
            "num_inference_steps": 20,
            "guidance_scale": 7.5,
            "height": 768, "width": 768
        },
        "quality": {
            "num_inference_steps": 30,
            "guidance_scale": 9.0,
            "height": 1024, "width": 1024
        }
    }
    
    config = settings[quality]
    
    image = pipe(prompt, **config).images[0]
    return image
```

## ğŸ“± ì‹¤ì œ í™œìš© ì‚¬ë¡€

### 1. ë¸”ë¡œê·¸ ì¸ë„¤ì¼ ìë™ ìƒì„±ê¸°

```python
import re
from datetime import datetime

class ThumbnailGenerator:
    def __init__(self):
        self.pipe = StableDiffusionPipeline.from_pretrained(
            "nanobanana/nano-diffusion-v1",
            torch_dtype=torch.float16
        ).to("cuda")
    
    def extract_keywords(self, title):
        """ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê¸°ìˆ  í‚¤ì›Œë“œ ë§¤í•‘
        tech_keywords = {
            "django": "web framework, python coding",
            "react": "modern web interface, javascript",
            "ai": "artificial intelligence, futuristic technology",
            "database": "data storage, digital infrastructure"
        }
        
        for keyword, description in tech_keywords.items():
            if keyword.lower() in title.lower():
                return description
        
        return "technology, programming, digital art"
    
    def generate_thumbnail(self, post_title):
        """í¬ìŠ¤íŠ¸ ì œëª© ê¸°ë°˜ ì¸ë„¤ì¼ ìƒì„±"""
        keywords = self.extract_keywords(post_title)
        
        prompt = f"""
        blog thumbnail, {keywords}, 
        clean design, professional look, 
        vibrant colors, modern style,
        high contrast, readable layout
        """
        
        image = self.pipe(
            prompt,
            num_inference_steps=20,
            guidance_scale=7.5,
            height=720,  # 16:9 ë¹„ìœ¨
            width=1280
        ).images[0]
        
        return image

# ì‚¬ìš© ì˜ˆì‹œ
generator = ThumbnailGenerator()
title = "Django Ninjaë¡œ ì‡¼í•‘ëª° API êµ¬ì¶•í•˜ê¸°"
thumbnail = generator.generate_thumbnail(title)
thumbnail.save(f"thumbnail_{datetime.now().strftime('%Y%m%d')}.png")
```

### 2. ì œí’ˆ ì´ë¯¸ì§€ ìƒì„±ê¸°

```python
class ProductImageGenerator:
    def __init__(self):
        self.pipe = StableDiffusionPipeline.from_pretrained(
            "nanobanana/nano-diffusion-v1",
            torch_dtype=torch.float16
        ).to("cuda")
    
    def generate_product_image(self, product_name, style="clean"):
        """ì œí’ˆ ì´ë¯¸ì§€ ìƒì„±"""
        
        style_prompts = {
            "clean": "white background, studio lighting, product photography",
            "lifestyle": "natural setting, lifestyle photography, soft lighting",
            "artistic": "creative composition, artistic lighting, premium feel"
        }
        
        base_prompt = f"{product_name}, {style_prompts[style]}, high quality, professional"
        
        image = self.pipe(
            base_prompt,
            num_inference_steps=25,
            guidance_scale=8.0,
            height=1024,
            width=1024
        ).images[0]
        
        return image

# E-ì»¤ë¨¸ìŠ¤ í™œìš©
products = ["wireless headphones", "smart watch", "coffee mug"]
generator = ProductImageGenerator()

for product in products:
    for style in ["clean", "lifestyle"]:
        image = generator.generate_product_image(product, style)
        image.save(f"{product}_{style}.png")
```

## ğŸ“ ì»¤ìŠ¤í„°ë§ˆì´ì§• ë° íŒŒì¸íŠœë‹

### 1. LoRA ì‚¬ìš©í•˜ê¸°

```python
from diffusers import LoraLoaderMixin

# LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ
pipe.load_lora_weights("your-lora-model", weight_name="your_lora.safetensors")

# LoRA ê°•ë„ ì¡°ì ˆ
def generate_with_lora(prompt, lora_scale=0.8):
    # LoRA ì ìš©
    pipe.set_adapters(["your_lora"], adapter_weights=[lora_scale])
    
    image = pipe(
        prompt,
        num_inference_steps=20,
        guidance_scale=7.5
    ).images[0]
    
    return image

# ì‚¬ìš©
image = generate_with_lora("a portrait in art style", lora_scale=0.6)
```

### 2. ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸íŠœë‹

```python
# íŒŒì¸íŠœë‹ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
import os
from torch.utils.data import Dataset

class CustomImageDataset(Dataset):
    def __init__(self, image_dir, caption_file):
        self.image_dir = image_dir
        self.captions = self.load_captions(caption_file)
    
    def load_captions(self, caption_file):
        captions = {}
        with open(caption_file, 'r') as f:
            for line in f:
                img_name, caption = line.strip().split('\t')
                captions[img_name] = caption
        return captions
    
    def __len__(self):
        return len(self.captions)
    
    def __getitem__(self, idx):
        img_names = list(self.captions.keys())
        img_name = img_names[idx]
        caption = self.captions[img_name]
        
        # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
        image_path = os.path.join(self.image_dir, img_name)
        # ... ì´ë¯¸ì§€ ì²˜ë¦¬ ë¡œì§
        
        return {"image": image, "caption": caption}

# íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸ (ê°„ì†Œí™”ëœ ë²„ì „)
def fine_tune_nanobanana(dataset, num_epochs=10):
    # ì‹¤ì œ íŒŒì¸íŠœë‹ì—ëŠ” ë” ë³µì¡í•œ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤
    print("íŒŒì¸íŠœë‹ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ì „ì²´ ê°€ì´ë“œëŠ” ê³µì‹ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.")
```

## ğŸ“Š ì„±ëŠ¥ ë¶„ì„ ë° ë²¤ì¹˜ë§ˆí¬

### 1. ìƒì„± ì‹œê°„ ì¸¡ì •

```python
import time
import matplotlib.pyplot as plt

def benchmark_generation():
    """ë‹¤ì–‘í•œ ì„¤ì •ì—ì„œ ì„±ëŠ¥ ì¸¡ì •"""
    
    test_configs = [
        {"steps": 10, "size": 512},
        {"steps": 20, "size": 768},
        {"steps": 30, "size": 1024}
    ]
    
    results = []
    
    for config in test_configs:
        start_time = time.time()
        
        image = pipe(
            "a test image for benchmarking",
            num_inference_steps=config["steps"],
            height=config["size"],
            width=config["size"]
        ).images[0]
        
        end_time = time.time()
        generation_time = end_time - start_time
        
        results.append({
            "config": f"{config['steps']} steps, {config['size']}px",
            "time": generation_time
        })
        
        print(f"{config}: {generation_time:.2f}ì´ˆ")
    
    return results

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
benchmark_results = benchmark_generation()
```

### 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

```python
import psutil
import GPUtil

def monitor_resources():
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
    
    # CPU ë° RAM
    cpu_percent = psutil.cpu_percent()
    ram_usage = psutil.virtual_memory().percent
    
    # GPU (NVIDIAë§Œ)
    try:
        gpus = GPUtil.getGPUs()
        if gpus:
            gpu = gpus[0]
            gpu_usage = gpu.load * 100
            gpu_memory = gpu.memoryUtil * 100
        else:
            gpu_usage = gpu_memory = 0
    except:
        gpu_usage = gpu_memory = 0
    
    print(f"CPU: {cpu_percent:.1f}%")
    print(f"RAM: {ram_usage:.1f}%")
    print(f"GPU: {gpu_usage:.1f}%")
    print(f"GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f}%")

# ì´ë¯¸ì§€ ìƒì„± ì „í›„ ë¦¬ì†ŒìŠ¤ í™•ì¸
print("ìƒì„± ì „:")
monitor_resources()

image = pipe("resource monitoring test").images[0]

print("\nìƒì„± í›„:")
monitor_resources()
```

## ğŸ› ï¸ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤

#### 1. CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±

```python
# í•´ê²° ë°©ë²• 1: ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì˜µì…˜ í™œì„±í™”
pipe.enable_attention_slicing()
pipe.enable_vae_slicing()

# í•´ê²° ë°©ë²• 2: CPU ì˜¤í”„ë¡œë“œ ì‚¬ìš©
pipe.enable_sequential_cpu_offload()

# í•´ê²° ë°©ë²• 3: í•´ìƒë„ ë‚®ì¶”ê¸°
image = pipe(prompt, height=512, width=512).images[0]
```

#### 2. ìƒì„± ì†ë„ê°€ ë„ˆë¬´ ëŠë¦¼

```python
# í•´ê²° ë°©ë²• 1: ìŠ¤í… ìˆ˜ ì¤„ì´ê¸°
image = pipe(prompt, num_inference_steps=15).images[0]

# í•´ê²° ë°©ë²• 2: ëª¨ë¸ ì»´íŒŒì¼ (PyTorch 2.0+)
pipe.unet = torch.compile(pipe.unet)

# í•´ê²° ë°©ë²• 3: ë¹ ë¥¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©
from diffusers import DDIMScheduler
pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)
```

#### 3. ì´ìƒí•œ ì´ë¯¸ì§€ê°€ ìƒì„±ë¨

```python
# í•´ê²° ë°©ë²• 1: ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼ ì¡°ì •
image = pipe(prompt, guidance_scale=7.5).images[0]  # ê¸°ë³¸ê°’

# í•´ê²° ë°©ë²• 2: ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
image = pipe(
    prompt,
    negative_prompt="blurry, low quality, distorted, ugly",
    guidance_scale=8.0
).images[0]

# í•´ê²° ë°©ë²• 3: ì‹œë“œ ê³ ì •ìœ¼ë¡œ ì¬í˜„ì„± í™•ë³´
generator = torch.Generator().manual_seed(42)
image = pipe(prompt, generator=generator).images[0]
```

## ğŸŒŸ ì‹¤ì „ í”„ë¡œì íŠ¸: AI ì•„íŠ¸ ê°¤ëŸ¬ë¦¬

ì™„ì „í•œ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤:

```python
from flask import Flask, render_template, request, send_file
import io
import base64
from PIL import Image

app = Flask(__name__)

# ë‚˜ë…¸ë°”ë‚˜ë‚˜ ì´ˆê¸°í™” (ì „ì—­ìœ¼ë¡œ í•œ ë²ˆë§Œ)
pipe = StableDiffusionPipeline.from_pretrained(
    "nanobanana/nano-diffusion-v1",
    torch_dtype=torch.float16
).to("cuda")

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/generate', methods=['POST'])
def generate_image():
    prompt = request.form['prompt']
    style = request.form.get('style', 'realistic')
    
    # ìŠ¤íƒ€ì¼ë³„ í”„ë¡¬í”„íŠ¸ ì¡°ì •
    style_modifiers = {
        'realistic': 'photorealistic, detailed, high quality',
        'anime': 'anime style, vibrant colors',
        'artistic': 'digital art, concept art',
        'vintage': 'vintage style, retro, film photography'
    }
    
    full_prompt = f"{prompt}, {style_modifiers[style]}"
    
    try:
        # ì´ë¯¸ì§€ ìƒì„±
        image = pipe(
            full_prompt,
            num_inference_steps=20,
            guidance_scale=7.5,
            height=768,
            width=768
        ).images[0]
        
        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
        img_buffer = io.BytesIO()
        image.save(img_buffer, format='PNG')
        img_buffer.seek(0)
        
        img_data = base64.b64encode(img_buffer.getvalue()).decode()
        
        return {
            'success': True,
            'image': f"data:image/png;base64,{img_data}",
            'prompt': full_prompt
        }
        
    except Exception as e:
        return {'success': False, 'error': str(e)}

if __name__ == '__main__':
    app.run(debug=True)
```

HTML í…œí”Œë¦¿ (`templates/index.html`):

```html
<!DOCTYPE html>
<html>
<head>
    <title>ë‚˜ë…¸ë°”ë‚˜ë‚˜ AI ì•„íŠ¸ ê°¤ëŸ¬ë¦¬</title>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <style>
        body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
        .form-group { margin-bottom: 15px; }
        input, select, button { padding: 10px; width: 100%; box-sizing: border-box; }
        button { background: #007bff; color: white; border: none; cursor: pointer; }
        button:hover { background: #0056b3; }
        #result { margin-top: 20px; text-align: center; }
        #loading { display: none; }
    </style>
</head>
<body>
    <h1>ğŸŒ ë‚˜ë…¸ë°”ë‚˜ë‚˜ AI ì•„íŠ¸ ê°¤ëŸ¬ë¦¬</h1>
    
    <form id="generateForm">
        <div class="form-group">
            <label>í”„ë¡¬í”„íŠ¸:</label>
            <input type="text" name="prompt" placeholder="ìƒì„±í•˜ê³  ì‹¶ì€ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•˜ì„¸ìš”..." required>
        </div>
        
        <div class="form-group">
            <label>ìŠ¤íƒ€ì¼:</label>
            <select name="style">
                <option value="realistic">ì‚¬ì‹¤ì </option>
                <option value="anime">ì• ë‹ˆë©”ì´ì…˜</option>
                <option value="artistic">ì˜ˆìˆ ì </option>
                <option value="vintage">ë¹ˆí‹°ì§€</option>
            </select>
        </div>
        
        <button type="submit">ì´ë¯¸ì§€ ìƒì„±</button>
    </form>
    
    <div id="loading">ğŸ¨ ì´ë¯¸ì§€ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...</div>
    <div id="result"></div>
    
    <script>
    $('#generateForm').submit(function(e) {
        e.preventDefault();
        
        $('#loading').show();
        $('#result').empty();
        
        $.post('/generate', $(this).serialize())
            .done(function(data) {
                $('#loading').hide();
                
                if (data.success) {
                    $('#result').html(`
                        <h3>ìƒì„± ì™„ë£Œ!</h3>
                        <img src="${data.image}" style="max-width: 100%; border: 1px solid #ddd; border-radius: 8px;">
                        <p><strong>í”„ë¡¬í”„íŠ¸:</strong> ${data.prompt}</p>
                    `);
                } else {
                    $('#result').html(`<p style="color: red;">ì˜¤ë¥˜: ${data.error}</p>`);
                }
            })
            .fail(function() {
                $('#loading').hide();
                $('#result').html('<p style="color: red;">ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.</p>');
            });
    });
    </script>
</body>
</html>
```

## ğŸ”® ë¯¸ë˜ ì „ë§ê³¼ í™œìš© ê°€ëŠ¥ì„±

### 1. ëª¨ë°”ì¼ ì•± í†µí•©

ë‚˜ë…¸ë°”ë‚˜ë‚˜ì˜ ê²½ëŸ‰ì„±ì€ ëª¨ë°”ì¼ í™˜ê²½ì—ì„œë„ í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤:

```python
# ëª¨ë°”ì¼ ìµœì í™” ë²„ì „
def mobile_optimized_generation(prompt):
    """ëª¨ë°”ì¼ ê¸°ê¸°ì— ìµœì í™”ëœ ì„¤ì •"""
    return pipe(
        prompt,
        num_inference_steps=15,  # ë¹ ë¥¸ ìƒì„±
        guidance_scale=7.0,
        height=512,  # ì ë‹¹í•œ í•´ìƒë„
        width=512
    ).images[0]
```

### 2. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°

```python
import threading
import queue

class RealTimeGenerator:
    def __init__(self):
        self.pipe = StableDiffusionPipeline.from_pretrained(
            "nanobanana/nano-diffusion-v1",
            torch_dtype=torch.float16
        ).to("cuda")
        self.queue = queue.Queue()
    
    def generate_continuously(self, prompts):
        """ì—°ì†ì ì¸ ì´ë¯¸ì§€ ìƒì„±"""
        for prompt in prompts:
            image = self.pipe(prompt, num_inference_steps=10).images[0]
            self.queue.put(image)
    
    def start_stream(self, prompts):
        """ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘"""
        thread = threading.Thread(target=self.generate_continuously, args=(prompts,))
        thread.start()
        return thread
```

### 3. API ì„œë¹„ìŠ¤ êµ¬ì¶•

```python
from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel

app = FastAPI(title="ë‚˜ë…¸ë°”ë‚˜ë‚˜ API")

class GenerationRequest(BaseModel):
    prompt: str
    style: str = "realistic"
    width: int = 768
    height: int = 768

@app.post("/generate")
async def api_generate(request: GenerationRequest, background_tasks: BackgroundTasks):
    """APIë¥¼ í†µí•œ ì´ë¯¸ì§€ ìƒì„±"""
    
    image = pipe(
        request.prompt,
        num_inference_steps=20,
        width=request.width,
        height=request.height
    ).images[0]
    
    # ì´ë¯¸ì§€ë¥¼ ì„ì‹œ ì €ì¥í•˜ê³  URL ë°˜í™˜
    filename = f"generated_{int(time.time())}.png"
    image.save(f"static/{filename}")
    
    return {
        "image_url": f"/static/{filename}",
        "prompt": request.prompt,
        "generated_at": datetime.now().isoformat()
    }
```

## ğŸ¯ ë§ˆë¬´ë¦¬

ë‚˜ë…¸ë°”ë‚˜ë‚˜ëŠ” **AI ì´ë¯¸ì§€ ìƒì„±ì˜ ë¯¼ì£¼í™”**ë¥¼ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤. ê±°ëŒ€í•œ GPUë‚˜ í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ì—†ì´ë„ ê°œì¸ PCì—ì„œ ë†€ë¼ìš´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

### âœ… í•µì‹¬ ì¥ì  ìš”ì•½

- **ğŸƒâ€â™‚ï¸ ì ‘ê·¼ì„±**: ì¼ë°˜ PCì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥
- **ğŸ’° ê²½ì œì„±**: ë¬´ë£Œ ì˜¤í”ˆì†ŒìŠ¤, í´ë¼ìš°ë“œ ë¹„ìš© ì ˆì•½
- **âš¡ íš¨ìœ¨ì„±**: ë¹ ë¥¸ ìƒì„± ì†ë„
- **ğŸ¨ í’ˆì§ˆ**: ìƒì—…ì  ì‚¬ìš© ê°€ëŠ¥í•œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€
- **ğŸ”§ í™•ì¥ì„±**: ì»¤ìŠ¤í„°ë§ˆì´ì§•ê³¼ íŒŒì¸íŠœë‹ ì§€ì›

### ğŸš€ ë‹¤ìŒ ë‹¨ê³„

1. **í”„ë¡œì íŠ¸ ì ìš©**: ë¸”ë¡œê·¸, ì‡¼í•‘ëª°, ì•±ì— ì´ë¯¸ì§€ ìƒì„± ê¸°ëŠ¥ ì¶”ê°€
2. **ì»¤ìŠ¤í…€ ëª¨ë¸**: ìì‹ ë§Œì˜ ìŠ¤íƒ€ì¼ë¡œ íŒŒì¸íŠœë‹
3. **ë¹„ì¦ˆë‹ˆìŠ¤ í™œìš©**: ì½˜í…ì¸  ì œì‘, ë§ˆì¼€íŒ… ì†Œì¬ ìƒì„±
4. **ì»¤ë®¤ë‹ˆí‹° ì°¸ì—¬**: ì˜¤í”ˆì†ŒìŠ¤ ê¸°ì—¬ ë° ëª¨ë¸ ê³µìœ 

AI ì´ë¯¸ì§€ ìƒì„±ì˜ ìƒˆë¡œìš´ ì‹œëŒ€ê°€ ì—´ë ¸ìŠµë‹ˆë‹¤. ë‚˜ë…¸ë°”ë‚˜ë‚˜ì™€ í•¨ê»˜ ì°½ì˜ì ì¸ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•´ë³´ì„¸ìš”! ğŸŒâœ¨

---

> ğŸ’¬ **ê¶ê¸ˆí•œ ì ì´ë‚˜ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´ê°€ ìˆìœ¼ì‹œë‹¤ë©´** ëŒ“ê¸€ë¡œ ê³µìœ í•´ì£¼ì„¸ìš”!  
> ğŸ”” **AI ê¸°ìˆ  ìµœì‹  ì†Œì‹ì„ ë°›ì•„ë³´ê³  ì‹¶ë‹¤ë©´** êµ¬ë…í•´ì£¼ì„¸ìš”!

**ê´€ë ¨ í¬ìŠ¤íŠ¸:**
- [Stable Diffusion ì™„ë²½ ê°€ì´ë“œ](#)
- [AI ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ ë¹„êµ ë¶„ì„](#)
- [ì‹¤ì „ AI í”„ë¡œì íŠ¸ êµ¬ì¶•í•˜ê¸°](#)