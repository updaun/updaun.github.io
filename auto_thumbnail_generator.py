#!/usr/bin/env python3
"""
í¬ìŠ¤íŠ¸ í‚¤ì›Œë“œ ê¸°ë°˜ ìë™ ì¸ë„¤ì¼ ìƒì„±ê¸°

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì˜ ì¹´í…Œê³ ë¦¬, íƒœê·¸, ì œëª©ì„ ë¶„ì„í•˜ì—¬
ê´€ë ¨ëœ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³ , í•´ë‹¹ í‚¤ì›Œë“œë¡œ Unsplashì—ì„œ ì´ë¯¸ì§€ë¥¼ ê²€ìƒ‰í•˜ì—¬
ìë™ìœ¼ë¡œ ì¸ë„¤ì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

import os
import re
import requests
import yaml
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
import io
import json
import time
from datetime import datetime
from typing import List, Dict, Optional, Tuple
import hashlib


class AutoThumbnailGenerator:
    """í¬ìŠ¤íŠ¸ í‚¤ì›Œë“œ ê¸°ë°˜ ìë™ ì¸ë„¤ì¼ ìƒì„±ê¸°"""
    
    def __init__(self, workspace_path: str):
        self.workspace_path = Path(workspace_path)
        self.posts_dir = self.workspace_path / "_posts"
        self.images_dir = self.workspace_path / "assets" / "img" / "posts"
        self.cache_dir = self.workspace_path / ".thumbnail_cache"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.images_dir.mkdir(parents=True, exist_ok=True)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ìºì‹œ íŒŒì¼ ê²½ë¡œ
        self.cache_file = self.cache_dir / "keyword_cache.json"
        self.image_cache_file = self.cache_dir / "image_cache.json"
        
        # í‚¤ì›Œë“œ ë§¤í•‘ ë¡œë“œ
        self.keyword_mapping = self._load_keyword_mapping()
        
        # ì´ë¯¸ì§€ ìºì‹œ ë¡œë“œ
        self.image_cache = self._load_image_cache()
        
        # ìƒ‰ìƒ íŒ”ë ˆíŠ¸
        self.color_schemes = {
            'aws': {
                'primary': '#232F3E',
                'secondary': '#FF9900', 
                'accent': '#4A90E2',
                'text': '#FFFFFF',
                'gradient': ['#232F3E', '#1A252F']
            },
            'python': {
                'primary': '#1E3A8A',
                'secondary': '#FFD43B',
                'accent': '#306998', 
                'text': '#FFFFFF',
                'gradient': ['#1E3A8A', '#3B82F6']
            },
            'django': {
                'primary': '#0C4B33',
                'secondary': '#44B78B',
                'accent': '#092A1C',
                'text': '#FFFFFF', 
                'gradient': ['#0C4B33', '#44B78B']
            },
            'ai': {
                'primary': '#0C0A09',
                'secondary': '#A855F7',
                'accent': '#06B6D4',
                'text': '#FFFFFF',
                'gradient': ['#0C0A09', '#1F1B24']
            },
            'firebase': {
                'primary': '#1A1A1A',
                'secondary': '#FFCB2B', 
                'accent': '#FF6B35',
                'text': '#FFFFFF',
                'gradient': ['#1A1A1A', '#2D2D2D']
            },
            'opencv': {
                'primary': '#0F172A',
                'secondary': '#22C55E',
                'accent': '#3B82F6',
                'text': '#FFFFFF',
                'gradient': ['#0F172A', '#1E293B']
            },
            'default': {
                'primary': '#1F2937',
                'secondary': '#3B82F6',
                'accent': '#10B981',
                'text': '#FFFFFF',
                'gradient': ['#1F2937', '#374151']
            }
        }

    def _load_keyword_mapping(self) -> Dict:
        """í‚¤ì›Œë“œ ë§¤í•‘ ë¡œë“œ ë˜ëŠ” ìƒì„±"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return self._create_default_keyword_mapping()

    def _load_image_cache(self) -> Dict:
        """ì´ë¯¸ì§€ ìºì‹œ ë¡œë“œ"""
        if self.image_cache_file.exists():
            try:
                with open(self.image_cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ ì´ë¯¸ì§€ ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return {}

    def _save_caches(self):
        """ìºì‹œ íŒŒì¼ë“¤ ì €ì¥"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.keyword_mapping, f, ensure_ascii=False, indent=2)
            
            with open(self.image_cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.image_cache, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _create_default_keyword_mapping(self) -> Dict:
        """ê¸°ë³¸ í‚¤ì›Œë“œ ë§¤í•‘ ìƒì„±"""
        return {
            # AWS ê´€ë ¨
            'aws': ['cloud computing', 'amazon web services', 'server infrastructure', 'cloud architecture'],
            'ec2': ['virtual machines', 'cloud servers', 'compute instances', 'server hosting'],
            's3': ['cloud storage', 'data backup', 'file storage', 'digital archives'],
            'lambda': ['serverless', 'cloud functions', 'microservices', 'automation'],
            'rds': ['database', 'cloud database', 'data management', 'sql servers'],
            'vpc': ['network security', 'cloud networking', 'private cloud', 'network infrastructure'],
            
            # Python/Django ê´€ë ¨  
            'python': ['programming', 'coding', 'software development', 'computer programming'],
            'django': ['web development', 'backend programming', 'web framework', 'server development'],
            'flask': ['web development', 'microframework', 'api development', 'python web'],
            'fastapi': ['api development', 'modern web', 'async programming', 'high performance'],
            'ninja': ['api framework', 'fast development', 'modern programming', 'web api'],
            
            # AI/ML ê´€ë ¨
            'ai': ['artificial intelligence', 'machine learning', 'neural networks', 'deep learning'],
            'yolo': ['computer vision', 'object detection', 'image recognition', 'ai vision'],
            'opencv': ['computer vision', 'image processing', 'video analysis', 'visual computing'],
            'tensorflow': ['machine learning', 'deep learning', 'neural networks', 'ai development'],
            'pytorch': ['deep learning', 'machine learning', 'neural networks', 'ai research'],
            
            # ì›¹ ê°œë°œ
            'javascript': ['web development', 'frontend programming', 'interactive web', 'coding'],
            'react': ['frontend development', 'user interface', 'web components', 'modern web'],
            'vue': ['frontend framework', 'user interface', 'web development', 'javascript'],
            'nextjs': ['full stack', 'react framework', 'modern web', 'web development'],
            
            # ë°ì´í„°ë² ì´ìŠ¤
            'mongodb': ['database', 'nosql', 'document database', 'data storage'],
            'postgresql': ['database', 'sql', 'relational database', 'data management'],
            'mysql': ['database', 'sql server', 'data management', 'web database'],
            'redis': ['caching', 'in-memory database', 'performance', 'data structure'],
            
            # DevOps/ë„êµ¬
            'docker': ['containerization', 'devops', 'deployment', 'software containers'],
            'kubernetes': ['container orchestration', 'devops', 'cloud native', 'microservices'],
            'git': ['version control', 'collaboration', 'code management', 'development tools'],
            'cicd': ['automation', 'devops', 'continuous integration', 'deployment pipeline'],
            
            # ê¸°íƒ€ ê¸°ìˆ 
            'api': ['software integration', 'web services', 'data exchange', 'programming interfaces'],
            'testing': ['software testing', 'quality assurance', 'code validation', 'debugging'],
            'performance': ['optimization', 'speed improvement', 'efficiency', 'system performance'],
            'security': ['cybersecurity', 'data protection', 'secure coding', 'system security'],
            'async': ['concurrent programming', 'parallel processing', 'performance optimization', 'modern programming'],
            'tdd': ['test driven development', 'software testing', 'code quality', 'agile development'],
            
            # ì¼ë°˜ì ì¸ í”„ë¡œê·¸ë˜ë° ê°œë…
            'architecture': ['software architecture', 'system design', 'technical blueprint', 'engineering'],
            'optimization': ['performance tuning', 'efficiency improvement', 'speed optimization', 'resource management'],
            'guide': ['tutorial', 'learning', 'education', 'instruction manual'],
            'analysis': ['data analysis', 'research', 'investigation', 'examination'],
            
            # í•œê¸€ í‚¤ì›Œë“œ ë§¤í•‘
            'ê°€ì´ë“œ': ['tutorial', 'learning', 'education', 'instruction manual'],
            'ë¶„ì„': ['data analysis', 'research', 'investigation', 'examination'],
            'ìµœì í™”': ['performance tuning', 'efficiency improvement', 'speed optimization'],
            'ì•„í‚¤í…ì²˜': ['software architecture', 'system design', 'technical blueprint'],
            'ì„±ëŠ¥': ['performance', 'optimization', 'speed', 'efficiency'],
            'ë³´ì•ˆ': ['cybersecurity', 'data protection', 'secure coding', 'system security'],
            'ë°ì´í„°ë² ì´ìŠ¤': ['database', 'data management', 'sql', 'storage'],
            'ì›¹ê°œë°œ': ['web development', 'frontend', 'backend', 'full stack'],
            'ë¨¸ì‹ ëŸ¬ë‹': ['machine learning', 'artificial intelligence', 'neural networks'],
            'ë”¥ëŸ¬ë‹': ['deep learning', 'neural networks', 'ai', 'machine learning'],
            'ì»´í“¨í„°ë¹„ì „': ['computer vision', 'image processing', 'object detection'],
            'í´ë¼ìš°ë“œ': ['cloud computing', 'aws', 'server infrastructure'],
            'ì„œë²„ë¦¬ìŠ¤': ['serverless', 'cloud functions', 'microservices'],
            'ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤': ['microservices', 'distributed systems', 'api'],
            'ì»¨í…Œì´ë„ˆ': ['containerization', 'docker', 'kubernetes'],
            'ë°ë¸Œì˜µìŠ¤': ['devops', 'automation', 'deployment', 'ci/cd'],
            'í…ŒìŠ¤íŠ¸': ['software testing', 'quality assurance', 'tdd'],
            'ë¦¬íŒ©í† ë§': ['code refactoring', 'code improvement', 'clean code'],
            'ì•Œê³ ë¦¬ì¦˜': ['algorithms', 'data structures', 'computer science'],
            'ìë£Œêµ¬ì¡°': ['data structures', 'algorithms', 'programming'],
            'í”„ë ˆì„ì›Œí¬': ['framework', 'development tools', 'programming'],
            'ë¼ì´ë¸ŒëŸ¬ë¦¬': ['library', 'programming tools', 'software development'],
            'ë°±ì—”ë“œ': ['backend development', 'server programming', 'api development'],
            'í”„ë¡ íŠ¸ì—”ë“œ': ['frontend development', 'user interface', 'web design'],
            'í’€ìŠ¤íƒ': ['full stack development', 'web development', 'programming'],
            'ëª¨ë°”ì¼': ['mobile development', 'app development', 'mobile apps'],
            'ê²Œì„ê°œë°œ': ['game development', 'game programming', 'interactive media'],
            'ë¸”ë¡ì²´ì¸': ['blockchain', 'cryptocurrency', 'distributed ledger'],
            'ë¹…ë°ì´í„°': ['big data', 'data analytics', 'data science'],
            'ì¸ê³µì§€ëŠ¥': ['artificial intelligence', 'machine learning', 'neural networks']
        }

    def extract_post_metadata(self, post_path: Path) -> Dict:
        """í¬ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        try:
            with open(post_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # YAML front matter ì¶”ì¶œ
            yaml_match = re.search(r'^---\s*\n(.*?)\n---', content, re.DOTALL | re.MULTILINE)
            if not yaml_match:
                return {}
            
            # YAML íŒŒì‹±
            yaml_content = yaml_match.group(1)
            metadata = yaml.safe_load(yaml_content)
            
            # ë³¸ë¬¸ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ì œí•œì ìœ¼ë¡œ)
            body_content = content[yaml_match.end():]
            body_keywords = self._extract_keywords_from_content(body_content)
            
            metadata['body_keywords'] = body_keywords
            return metadata
            
        except Exception as e:
            print(f"âŒ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ ({post_path}): {e}")
            return {}

    def _extract_keywords_from_content(self, content: str, max_keywords: int = 10) -> List[str]:
        """ë³¸ë¬¸ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê¸°ìˆ  ìš©ì–´ íŒ¨í„´
        tech_patterns = [
            r'\b(AWS|EC2|S3|Lambda|RDS|VPC|CloudFormation|API Gateway)\b',
            r'\b(Python|Django|Flask|FastAPI|JavaScript|React|Vue|Node\.js)\b',
            r'\b(MongoDB|PostgreSQL|MySQL|Redis|Elasticsearch)\b',
            r'\b(Docker|Kubernetes|Git|CI/CD|DevOps)\b',
            r'\b(AI|ML|YOLO|OpenCV|TensorFlow|PyTorch)\b',
            r'\b(API|REST|GraphQL|gRPC|WebSocket)\b'
        ]
        
        keywords = []
        for pattern in tech_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            keywords.extend([match.lower() for match in matches])
        
        # ì¤‘ë³µ ì œê±° ë° ë¹ˆë„ìˆœ ì •ë ¬
        keyword_counts = {}
        for keyword in keywords:
            keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1
        
        sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)
        return [kw[0] for kw in sorted_keywords[:max_keywords]]

    def generate_search_keywords(self, metadata: Dict) -> List[str]:
        """ë©”íƒ€ë°ì´í„°ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±"""
        keywords = set()
        
        # ì¹´í…Œê³ ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        categories = metadata.get('categories', [])
        if isinstance(categories, str):
            categories = [categories]
        
        for category in categories:
            category_lower = category.lower()
            keywords.add(category_lower)
            
            # ë§¤í•‘ëœ í‚¤ì›Œë“œ ì¶”ê°€
            if category_lower in self.keyword_mapping:
                keywords.update(self.keyword_mapping[category_lower])
        
        # íƒœê·¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        tags = metadata.get('tags', [])
        if isinstance(tags, str):
            tags = [tags]
            
        for tag in tags:
            tag_lower = tag.lower().replace('-', ' ')
            keywords.add(tag_lower)
            
            # ë§¤í•‘ëœ í‚¤ì›Œë“œ ì¶”ê°€
            tag_key = tag_lower.replace(' ', '').replace('-', '')
            if tag_key in self.keyword_mapping:
                keywords.update(self.keyword_mapping[tag_key])
        
        # ì œëª©ì—ì„œ ì£¼ìš” ê¸°ìˆ  ìš©ì–´ ì¶”ì¶œ
        title = metadata.get('title', '')
        title_keywords = self._extract_keywords_from_content(title)
        for kw in title_keywords:
            keywords.add(kw)
            if kw in self.keyword_mapping:
                keywords.update(self.keyword_mapping[kw])
        
        # ë³¸ë¬¸ í‚¤ì›Œë“œ ì¶”ê°€
        body_keywords = metadata.get('body_keywords', [])
        for kw in body_keywords:
            keywords.add(kw)
            if kw in self.keyword_mapping:
                keywords.update(self.keyword_mapping[kw])
        
        # í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„ ì •ë ¬
        keyword_list = list(keywords)
        
        # ê¸°ìˆ  ê´€ë ¨ í‚¤ì›Œë“œ ìš°ì„ 
        tech_keywords = [kw for kw in keyword_list if any(tech in kw.lower() for tech in 
                        ['programming', 'development', 'software', 'computer', 'technology', 
                         'coding', 'api', 'database', 'cloud', 'server', 'web'])]
        
        other_keywords = [kw for kw in keyword_list if kw not in tech_keywords]
        
        return tech_keywords[:5] + other_keywords[:3]  # ìµœëŒ€ 8ê°œ í‚¤ì›Œë“œ

    def search_unsplash_images(self, keywords: List[str], count: int = 5) -> List[Dict]:
        """Unsplashì—ì„œ ì´ë¯¸ì§€ ê²€ìƒ‰ (ë¬´ë£Œ API ì‚¬ìš©)"""
        images = []
        
        for keyword in keywords[:3]:  # ì²˜ìŒ 3ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
            cache_key = f"unsplash_{hashlib.md5(keyword.encode()).hexdigest()}"
            
            # ìºì‹œ í™•ì¸
            if cache_key in self.image_cache:
                cached_data = self.image_cache[cache_key]
                if time.time() - cached_data['timestamp'] < 86400:  # 24ì‹œê°„ ìºì‹œ
                    images.extend(cached_data['images'][:2])
                    continue
            
            try:
                # Unsplash ë¬´ë£Œ API ì—”ë“œí¬ì¸íŠ¸ (rate limited)
                url = f"https://source.unsplash.com/1200x630/?{keyword.replace(' ', ',')}"
                
                # ê°„ë‹¨í•œ ë©”íƒ€ë°ì´í„° ìƒì„±
                image_info = {
                    'url': url,
                    'keyword': keyword,
                    'width': 1200,
                    'height': 630,
                    'description': f"Image for {keyword}"
                }
                
                images.append(image_info)
                
                # ìºì‹œ ì €ì¥
                self.image_cache[cache_key] = {
                    'images': [image_info],
                    'timestamp': time.time()
                }
                
                # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ëŒ€ê¸°
                time.sleep(0.5)
                
            except Exception as e:
                print(f"âš ï¸ Unsplash ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {e}")
                continue
            
            if len(images) >= count:
                break
        
        return images[:count]

    def download_and_process_image(self, image_info: Dict, output_path: Path, 
                                 metadata: Dict) -> bool:
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(image_info['url'], headers=headers, timeout=30)
            response.raise_for_status()
            
            # ì´ë¯¸ì§€ ì—´ê¸°
            img = Image.open(io.BytesIO(response.content))
            
            # RGBë¡œ ë³€í™˜
            if img.mode != 'RGB':
                img = img.convert('RGB')
            
            # í¬ê¸° ì¡°ì • (1200x630)
            img = self._resize_and_crop(img, (1200, 630))
            
            # ì˜¤ë²„ë ˆì´ ì¶”ê°€
            img = self._add_overlay(img, metadata)
            
            # WebPë¡œ ì €ì¥
            img.save(output_path, 'WEBP', quality=85, optimize=True)
            print(f"âœ… ì´ë¯¸ì§€ ì²˜ë¦¬ ì™„ë£Œ: {output_path}")
            return True
            
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ/ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False

    def _resize_and_crop(self, img: Image.Image, size: Tuple[int, int]) -> Image.Image:
        """ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • ë° í¬ë¡­"""
        img_ratio = img.width / img.height
        target_ratio = size[0] / size[1]
        
        if img_ratio > target_ratio:
            # ì´ë¯¸ì§€ê°€ ë” ë„“ìŒ
            new_height = size[1]
            new_width = int(new_height * img_ratio)
            img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
            left = (new_width - size[0]) // 2
            img = img.crop((left, 0, left + size[0], size[1]))
        else:
            # ì´ë¯¸ì§€ê°€ ë” ë†’ìŒ
            new_width = size[0]
            new_height = int(new_width / img_ratio)
            img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
            top = (new_height - size[1]) // 2
            img = img.crop((0, top, size[0], top + size[1]))
        
        return img

    def _add_overlay(self, img: Image.Image, metadata: Dict) -> Image.Image:
        """ì´ë¯¸ì§€ì— í…ìŠ¤íŠ¸ ì˜¤ë²„ë ˆì´ ì¶”ê°€"""
        overlay = Image.new('RGBA', img.size, (0, 0, 0, 0))
        draw = ImageDraw.Draw(overlay)
        
        # ìƒ‰ìƒ ìŠ¤í‚¤ë§ˆ ì„ íƒ
        scheme = self._select_color_scheme(metadata)
        
        # ì œëª© ì¶”ì¶œ
        title = metadata.get('title', 'ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸')
        if len(title) > 60:
            title = title[:57] + "..."
        
        # í•˜ë‹¨ ê·¸ë¼ë°ì´ì…˜ ë°°ê²½
        gradient_height = 200
        for i in range(gradient_height):
            alpha = int(150 * (i / gradient_height))
            y = img.height - gradient_height + i
            color = tuple(list(self._hex_to_rgb(scheme['primary'])) + [alpha])
            draw.line([(0, y), (img.width, y)], fill=color)
        
        # í•œê¸€ ì§€ì› í°íŠ¸ ë¡œë“œ ì‹œë„
        try:
            # macOS í•œê¸€ í°íŠ¸ë“¤
            title_font = ImageFont.truetype("/System/Library/Fonts/AppleSDGothicNeo.ttc", 42)
            category_font = ImageFont.truetype("/System/Library/Fonts/AppleSDGothicNeo.ttc", 24)
        except:
            try:
                # ëŒ€ì²´ í•œê¸€ í°íŠ¸
                title_font = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", 42)
                category_font = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", 24)
            except:
                try:
                    # NanumGothic (ë§ì´ ì‚¬ìš©ë˜ëŠ” í•œê¸€ í°íŠ¸)
                    title_font = ImageFont.truetype("/System/Library/Fonts/NanumGothic.ttc", 42)
                    category_font = ImageFont.truetype("/System/Library/Fonts/NanumGothic.ttc", 24)
                except:
                    try:
                        # Windows í•œê¸€ í°íŠ¸
                        title_font = ImageFont.truetype("malgun.ttf", 42)
                        category_font = ImageFont.truetype("malgun.ttf", 24)
                    except:
                        # ê¸°ë³¸ í°íŠ¸ (í•œê¸€ ì§€ì› ì œí•œì )
                        title_font = ImageFont.load_default()
                        category_font = ImageFont.load_default()
        
        # ì œëª© í…ìŠ¤íŠ¸ (ì—¬ëŸ¬ ì¤„ ì²˜ë¦¬)
        lines = self._wrap_text(title, title_font, img.width - 100)
        
        total_text_height = len(lines) * 50
        start_y = img.height - 160 - (len(lines) - 1) * 25  # ì¤„ ìˆ˜ì— ë”°ë¼ ì‹œì‘ ìœ„ì¹˜ ì¡°ì •
        
        for i, line in enumerate(lines):
            try:
                bbox = draw.textbbox((0, 0), line, font=title_font)
                text_width = bbox[2] - bbox[0]
            except:
                text_width = len(line) * 20  # ê·¼ì‚¬ì¹˜
            
            x = 50
            y = start_y + i * 50
            
            # ë” ë¶€ë“œëŸ¬ìš´ í…ìŠ¤íŠ¸ ê·¸ë¦¼ì (ì—¬ëŸ¬ ë ˆì´ì–´)
            for offset in [(3, 3), (2, 2), (1, 1)]:
                shadow_alpha = 100 - (offset[0] * 20)
                draw.text((x + offset[0], y + offset[1]), line, font=title_font, 
                         fill=(0, 0, 0, shadow_alpha))
            
            # ë©”ì¸ í…ìŠ¤íŠ¸ (ë” ì„ ëª…í•˜ê²Œ)
            draw.text((x, y), line, font=title_font, fill=(255, 255, 255, 255))
        
        # ì¹´í…Œê³ ë¦¬/íƒœê·¸ í‘œì‹œ
        categories = metadata.get('categories', [])
        if categories:
            if isinstance(categories, list):
                category_text = ' â€¢ '.join(categories[:2])
            else:
                category_text = categories
                
            draw.text((50, img.height - 40), category_text, 
                     font=category_font, fill=tuple(list(self._hex_to_rgb(scheme['secondary'])) + [255]))
        
        # ì˜¤ë²„ë ˆì´ í•©ì„±
        img = Image.alpha_composite(img.convert('RGBA'), overlay)
        return img.convert('RGB')

    def _wrap_text(self, text: str, font, max_width: int) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì—¬ëŸ¬ ì¤„ë¡œ ë˜í•‘ (í•œê¸€ ì§€ì›)"""
        # í•œê¸€ê³¼ ì˜ì–´ê°€ ì„ì¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        words = []
        current_word = ""
        
        for char in text:
            if char.isspace():
                if current_word:
                    words.append(current_word)
                    current_word = ""
                words.append(char)
            elif self._is_korean(char):
                # í•œê¸€ ë¬¸ìì˜ ê²½ìš° ì ì ˆí•œ ìœ„ì¹˜ì—ì„œ ì¤„ë°”ê¿ˆ ê°€ëŠ¥
                current_word += char
                if len(current_word) >= 15:  # í•œê¸€ 15ì ì •ë„ì—ì„œ ì¤„ë°”ê¿ˆ ê°€ëŠ¥
                    words.append(current_word)
                    current_word = ""
            else:
                current_word += char
        
        if current_word:
            words.append(current_word)
        
        # ê³µë°± ì œê±°
        words = [w for w in words if not w.isspace()]
        
        lines = []
        current_line = []
        
        for word in words:
            test_line = ''.join(current_line + [word])
            try:
                bbox = font.getbbox(test_line)
                text_width = bbox[2] - bbox[0]
            except:
                # í°íŠ¸ê°€ getbboxë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš°
                text_width = len(test_line) * 20  # ê·¼ì‚¬ì¹˜
            
            if text_width <= max_width:
                current_line.append(word)
            else:
                if current_line:
                    lines.append(''.join(current_line))
                    current_line = [word]
                else:
                    # ë‹¨ì–´ê°€ ë„ˆë¬´ ê¸´ ê²½ìš° ê°•ì œë¡œ ì˜ë¼ë‚´ê¸°
                    if len(word) > 20:
                        lines.append(word[:20] + "...")
                    else:
                        lines.append(word)
        
        if current_line:
            lines.append(''.join(current_line))
        
        return lines[:2]  # ìµœëŒ€ 2ì¤„
    
    def _is_korean(self, char: str) -> bool:
        """í•œê¸€ ë¬¸ìì¸ì§€ í™•ì¸"""
        return '\uac00' <= char <= '\ud7af' or '\u3131' <= char <= '\u318e'

    def _select_color_scheme(self, metadata: Dict) -> Dict:
        """ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒ‰ìƒ ìŠ¤í‚¤ë§ˆ ì„ íƒ"""
        categories = metadata.get('categories', [])
        tags = metadata.get('tags', [])
        
        all_terms = []
        if isinstance(categories, list):
            all_terms.extend([cat.lower() for cat in categories])
        elif isinstance(categories, str):
            all_terms.append(categories.lower())
            
        if isinstance(tags, list):
            all_terms.extend([tag.lower() for tag in tags])
        elif isinstance(tags, str):
            all_terms.append(tags.lower())
        
        # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ìŠ¤í‚¤ë§ˆ ì„ íƒ
        for term in all_terms:
            if 'aws' in term:
                return self.color_schemes['aws']
            elif any(python_term in term for python_term in ['python', 'django', 'flask', 'fastapi']):
                if 'django' in term:
                    return self.color_schemes['django']
                return self.color_schemes['python']
            elif any(ai_term in term for ai_term in ['ai', 'yolo', 'opencv', 'tensorflow', 'pytorch']):
                return self.color_schemes['ai']
            elif 'firebase' in term:
                return self.color_schemes['firebase']
            elif 'opencv' in term:
                return self.color_schemes['opencv']
        
        return self.color_schemes['default']

    def _hex_to_rgb(self, hex_color: str) -> Tuple[int, int, int]:
        """HEX ìƒ‰ìƒì„ RGBë¡œ ë³€í™˜"""
        hex_color = hex_color.lstrip('#')
        return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))

    def create_fallback_image(self, metadata: Dict, output_path: Path) -> bool:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í´ë°± ì´ë¯¸ì§€ ìƒì„±"""
        try:
            size = (1200, 630)
            scheme = self._select_color_scheme(metadata)
            
            # ê·¸ë¼ë°ì´ì…˜ ë°°ê²½ ìƒì„±
            img = Image.new('RGB', size, self._hex_to_rgb(scheme['primary']))
            
            # ê·¸ë¼ë°ì´ì…˜ íš¨ê³¼
            for i in range(size[1]):
                alpha = i / size[1]
                primary_rgb = self._hex_to_rgb(scheme['primary'])
                secondary_rgb = self._hex_to_rgb(scheme['gradient'][1])
                
                blended = tuple(int(primary_rgb[j] * (1 - alpha) + secondary_rgb[j] * alpha) for j in range(3))
                
                draw = ImageDraw.Draw(img)
                draw.line([(0, i), (size[0], i)], fill=blended)
            
            # ì¥ì‹ì  ìš”ì†Œ
            draw = ImageDraw.Draw(img)
            
            # ìƒë‹¨ ì¥ì‹ ë°”
            draw.rectangle([0, 0, size[0], 8], fill=self._hex_to_rgb(scheme['secondary']))
            
            # í•˜ë‹¨ ì¥ì‹ ë°”
            draw.rectangle([0, size[1]-8, size[0], size[1]], fill=self._hex_to_rgb(scheme['accent']))
            
            # ê¸°í•˜í•™ì  íŒ¨í„´
            for i in range(0, size[0], 100):
                draw.line([(i, 0), (i + 50, 50)], fill=self._hex_to_rgb(scheme['accent']), width=1)
            
            # ì˜¤ë²„ë ˆì´ ì¶”ê°€
            img = self._add_overlay(img, metadata)
            
            # ì €ì¥
            img.save(output_path, 'WEBP', quality=85, optimize=True)
            print(f"âœ… í´ë°± ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: {output_path}")
            return True
            
        except Exception as e:
            print(f"âŒ í´ë°± ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return False

    def generate_thumbnail_for_post(self, post_file: str) -> bool:
        """íŠ¹ì • í¬ìŠ¤íŠ¸ì˜ ì¸ë„¤ì¼ ìƒì„±"""
        post_path = self.posts_dir / post_file
        
        if not post_path.exists():
            print(f"âŒ í¬ìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {post_path}")
            return False
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata = self.extract_post_metadata(post_path)
        if not metadata:
            print(f"âŒ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {post_file}")
            return False
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        image_name = post_file.replace('.md', '.webp')
        output_path = self.images_dir / image_name
        
        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš° ê±´ë„ˆë›°ê¸° (ë®ì–´ì“°ê¸° ì›í•˜ë©´ ì‚­ì œ í›„ ì‹¤í–‰)
        if output_path.exists():
            print(f"â„¹ï¸ ì¸ë„¤ì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {output_path}")
            return True
        
        print(f"ğŸ¨ ì¸ë„¤ì¼ ìƒì„± ì¤‘: {metadata.get('title', post_file)}")
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±
        keywords = self.generate_search_keywords(metadata)
        print(f"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {keywords}")
        
        # ì´ë¯¸ì§€ ê²€ìƒ‰
        images = self.search_unsplash_images(keywords)
        
        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬ ì‹œë„
        success = False
        for i, image_info in enumerate(images):
            print(f"ğŸ”„ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œë„ {i+1}/{len(images)}: {image_info['keyword']}")
            if self.download_and_process_image(image_info, output_path, metadata):
                success = True
                break
        
        # ëª¨ë“  ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œ í´ë°± ì´ë¯¸ì§€ ìƒì„±
        if not success:
            print("âš ï¸ ëª¨ë“  ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, í´ë°± ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
            success = self.create_fallback_image(metadata, output_path)
        
        if success:
            print(f"âœ… ì¸ë„¤ì¼ ìƒì„± ì™„ë£Œ: {output_path}")
        else:
            print(f"âŒ ì¸ë„¤ì¼ ìƒì„± ì‹¤íŒ¨: {post_file}")
        
        # ìºì‹œ ì €ì¥
        self._save_caches()
        
        return success

    def generate_thumbnails_for_recent_posts(self, days: int = 30) -> List[str]:
        """ìµœê·¼ í¬ìŠ¤íŠ¸ë“¤ì˜ ì¸ë„¤ì¼ ìƒì„±"""
        from datetime import datetime, timedelta
        
        cutoff_date = datetime.now() - timedelta(days=days)
        recent_posts = []
        
        # ìµœê·¼ í¬ìŠ¤íŠ¸ ì°¾ê¸°
        for post_file in self.posts_dir.glob("*.md"):
            try:
                # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ (YYYY-MM-DD í˜•ì‹)
                date_match = re.search(r'(\d{4}-\d{2}-\d{2})', post_file.name)
                if date_match:
                    post_date = datetime.strptime(date_match.group(1), '%Y-%m-%d')
                    if post_date >= cutoff_date:
                        recent_posts.append(post_file.name)
            except Exception:
                continue
        
        # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
        recent_posts.sort(reverse=True)
        
        print(f"ğŸ“… ìµœê·¼ {days}ì¼ ê°„ì˜ í¬ìŠ¤íŠ¸ {len(recent_posts)}ê°œ ë°œê²¬")
        
        success_count = 0
        for post_file in recent_posts:
            if self.generate_thumbnail_for_post(post_file):
                success_count += 1
        
        print(f"âœ… ì´ {success_count}/{len(recent_posts)}ê°œ ì¸ë„¤ì¼ ìƒì„± ì™„ë£Œ")
        
        return recent_posts

    def generate_thumbnail_for_current_post(self) -> bool:
        """í˜„ì¬ í¸ì§‘ ì¤‘ì¸ í¬ìŠ¤íŠ¸ì˜ ì¸ë„¤ì¼ ìƒì„±"""
        # ê°€ì¥ ìµœê·¼ ìˆ˜ì •ëœ í¬ìŠ¤íŠ¸ íŒŒì¼ ì°¾ê¸°
        latest_post = None
        latest_time = 0
        
        for post_file in self.posts_dir.glob("*.md"):
            mtime = post_file.stat().st_mtime
            if mtime > latest_time:
                latest_time = mtime
                latest_post = post_file
        
        if latest_post:
            print(f"ğŸ“ ê°€ì¥ ìµœê·¼ ìˆ˜ì •ëœ í¬ìŠ¤íŠ¸: {latest_post.name}")
            return self.generate_thumbnail_for_post(latest_post.name)
        else:
            print("âŒ í¬ìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='í¬ìŠ¤íŠ¸ í‚¤ì›Œë“œ ê¸°ë°˜ ìë™ ì¸ë„¤ì¼ ìƒì„±ê¸°')
    parser.add_argument('--post', '-p', help='íŠ¹ì • í¬ìŠ¤íŠ¸ íŒŒì¼ëª… (ì˜ˆ: 2025-08-14-example.md)')
    parser.add_argument('--recent', '-r', type=int, default=7, help='ìµœê·¼ Nì¼ê°„ì˜ í¬ìŠ¤íŠ¸ ì²˜ë¦¬ (ê¸°ë³¸ê°’: 7)')
    parser.add_argument('--current', '-c', action='store_true', help='í˜„ì¬ í¸ì§‘ ì¤‘ì¸ í¬ìŠ¤íŠ¸ ì²˜ë¦¬')
    parser.add_argument('--workspace', '-w', default='.', help='ì‘ì—… ê³µê°„ ê²½ë¡œ (ê¸°ë³¸ê°’: í˜„ì¬ ë””ë ‰í† ë¦¬)')
    
    args = parser.parse_args()
    
    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ë¥¼ ê¸°ë³¸ ì‘ì—…ê³µê°„ìœ¼ë¡œ ì„¤ì •
    if args.workspace == '.':
        args.workspace = os.path.dirname(os.path.abspath(__file__))
    
    generator = AutoThumbnailGenerator(args.workspace)
    
    if args.post:
        # íŠ¹ì • í¬ìŠ¤íŠ¸ ì²˜ë¦¬
        generator.generate_thumbnail_for_post(args.post)
    elif args.current:
        # í˜„ì¬ í¸ì§‘ ì¤‘ì¸ í¬ìŠ¤íŠ¸ ì²˜ë¦¬
        generator.generate_thumbnail_for_current_post()
    else:
        # ìµœê·¼ í¬ìŠ¤íŠ¸ë“¤ ì²˜ë¦¬
        generator.generate_thumbnails_for_recent_posts(args.recent)


if __name__ == "__main__":
    main()
